<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Session 3 Introduction to Bayesian inference | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research</title>
  <meta name="description" content="Course notes for HAD5314H Winter 2022" />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Session 3 Introduction to Bayesian inference | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Course notes for HAD5314H Winter 2022" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Session 3 Introduction to Bayesian inference | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research" />
  
  <meta name="twitter:description" content="Course notes for HAD5314H Winter 2022" />
  

<meta name="author" content="Kuan Liu   Institute of Health Policy, Management and Evaluation   University of Toronto" />


<meta name="date" content="2022-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="prob.html"/>
<link rel="next" href="Prior.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.22/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">HAD5314H - Winter 2022 </a></li>

<li class="divider"></li>
<li><a href="index.html#university-of-toronto-statement-of-acknowledgment-of-traditional-land">University of Toronto Statement of Acknowledgment of Traditional Land<span></span></a></li>
<li><a href="course-syllabus.html#course-syllabus">Course Syllabus<span></span></a>
<ul>
<li><a href="course-syllabus.html#course-info">Course Info<span></span></a></li>
<li><a href="course-syllabus.html#course-description">Course Description<span></span></a></li>
<li><a href="course-syllabus.html#course-textbook-and-structure">Course Textbook and Structure<span></span></a></li>
<li><a href="course-syllabus.html#calendar-and-outline">Calendar and Outline<span></span></a></li>
<li><a href="course-syllabus.html#accessibility-and-accommodations">Accessibility and Accommodations<span></span></a></li>
<li><a href="course-syllabus.html#academic-integrity">Academic Integrity<span></span></a></li>
<li><a href="course-syllabus.html#key-resources-and-supports-for-dslph-graduate-students">Key Resources and Supports for DSLPH Graduate Students<span></span></a></li>
<li><a href="course-syllabus.html#license">License<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="into.html"><a href="into.html"><i class="fa fa-check"></i><b>1</b> Introduction to the course<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="into.html"><a href="into.html#about-me"><i class="fa fa-check"></i><b>1.1</b> About me<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="into.html"><a href="into.html#syllabus"><i class="fa fa-check"></i><b>1.2</b> Syllabus<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="into.html"><a href="into.html#some-history"><i class="fa fa-check"></i><b>1.3</b> Some history<span></span></a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="into.html"><a href="into.html#bayesian-history"><i class="fa fa-check"></i><b>1.3.1</b> Bayesian history<span></span></a></li>
<li class="chapter" data-level="1.3.2" data-path="into.html"><a href="into.html#history-of-this-course"><i class="fa fa-check"></i><b>1.3.2</b> History of this course<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="into.html"><a href="into.html#thinking-like-a-bayesian-using-the-concept-of-probability"><i class="fa fa-check"></i><b>1.4</b> Thinking like a Bayesian using the concept of probability<span></span></a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="into.html"><a href="into.html#probability-is-not-unitary"><i class="fa fa-check"></i><b>1.4.1</b> Probability is not unitary<span></span></a></li>
<li class="chapter" data-level="1.4.2" data-path="into.html"><a href="into.html#bayes-rule"><i class="fa fa-check"></i><b>1.4.2</b> Bayes’ Rule<span></span></a></li>
<li class="chapter" data-level="1.4.3" data-path="into.html"><a href="into.html#the-scientific-method-in-steps"><i class="fa fa-check"></i><b>1.4.3</b> The Scientific Method in steps<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="lab1-getting-started-with-r-rstudio.html#lab1-getting-started-with-r-rstudio">Lab1 Getting started with R &amp; RStudio<span></span></a>
<ul>
<li class="chapter" data-level="1.5" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html"><i class="fa fa-check"></i><b>1.5</b> R and RStudio Installation<span></span></a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#windows-operating-system"><i class="fa fa-check"></i><b>1.5.1</b> Windows operating system<span></span></a></li>
<li class="chapter" data-level="1.5.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#macos-operating-system"><i class="fa fa-check"></i><b>1.5.2</b> macOS operating system<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#r-packages"><i class="fa fa-check"></i><b>1.6</b> R Packages<span></span></a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#bayesian-analysis-in-r-using-brms-package"><i class="fa fa-check"></i><b>1.6.1</b> Bayesian Analysis in R using brms package<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#working-in-rstudio"><i class="fa fa-check"></i><b>1.7</b> Working in RStudio<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#rstudio-layout"><i class="fa fa-check"></i><b>1.7.1</b> RStudio layout<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#customization"><i class="fa fa-check"></i><b>1.7.2</b> Customization<span></span></a></li>
<li class="chapter" data-level="1.7.3" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#working-directory"><i class="fa fa-check"></i><b>1.7.3</b> Working directory<span></span></a></li>
<li class="chapter" data-level="1.7.4" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#getting-help-with-r"><i class="fa fa-check"></i><b>1.7.4</b> Getting help with R<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#basic-r-a-crash-introduction"><i class="fa fa-check"></i><b>1.8</b> Basic R (a crash introduction)<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#arithmetic"><i class="fa fa-check"></i><b>1.8.1</b> Arithmetic<span></span></a></li>
<li class="chapter" data-level="1.8.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#vectors"><i class="fa fa-check"></i><b>1.8.2</b> Vectors<span></span></a></li>
<li class="chapter" data-level="1.8.3" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#data-frame---the-titanic-dataset"><i class="fa fa-check"></i><b>1.8.3</b> Data frame - The Titanic dataset<span></span></a></li>
<li class="chapter" data-level="1.8.4" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#simple-plots"><i class="fa fa-check"></i><b>1.8.4</b> Simple plots<span></span></a></li>
<li><a href="lab1-getting-started-with-r-rstudio.html#r-session-information">R Session information<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prob.html"><a href="prob.html"><i class="fa fa-check"></i><b>2</b> Probability, random variables and distributions<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="prob.html"><a href="prob.html#probability"><i class="fa fa-check"></i><b>2.1</b> Probability<span></span></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prob.html"><a href="prob.html#venn-diagrams"><i class="fa fa-check"></i><b>2.1.1</b> Venn Diagrams<span></span></a></li>
<li class="chapter" data-level="2.1.2" data-path="prob.html"><a href="prob.html#probability-rules"><i class="fa fa-check"></i><b>2.1.2</b> Probability Rules<span></span></a></li>
<li class="chapter" data-level="2.1.3" data-path="prob.html"><a href="prob.html#how-to-define-and-assign-probabilities-in-general"><i class="fa fa-check"></i><b>2.1.3</b> How to define and assign probabilities in general?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prob.html"><a href="prob.html#probability-distributions"><i class="fa fa-check"></i><b>2.2</b> Probability Distributions<span></span></a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prob.html"><a href="prob.html#probability-density-functions"><i class="fa fa-check"></i><b>2.2.1</b> Probability density functions<span></span></a></li>
<li class="chapter" data-level="2.2.2" data-path="prob.html"><a href="prob.html#discrete-distributions"><i class="fa fa-check"></i><b>2.2.2</b> Discrete Distributions<span></span></a></li>
<li class="chapter" data-level="2.2.3" data-path="prob.html"><a href="prob.html#continous-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Continous Distributions<span></span></a></li>
<li><a href="prob.html#r-session-information-1">R Session information<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>3</b> Introduction to Bayesian inference<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayes.html"><a href="bayes.html#classical-frequentist-approach"><i class="fa fa-check"></i><b>3.1</b> Classical frequentist approach<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="bayes.html"><a href="bayes.html#introduction-to-bayesian-approach"><i class="fa fa-check"></i><b>3.2</b> Introduction to Bayesian approach<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="bayes.html"><a href="bayes.html#review-from-session-1"><i class="fa fa-check"></i><b>3.2.1</b> Review from session 1<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="bayes.html"><a href="bayes.html#the-beta-binomial-model"><i class="fa fa-check"></i><b>3.2.2</b> The Beta-binomial model<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="bayes.html"><a href="bayes.html#the-normal-normal-model"><i class="fa fa-check"></i><b>3.2.3</b> The Normal-normal model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayes.html"><a href="bayes.html#summary-of-conjugate-priors-models"><i class="fa fa-check"></i><b>3.3</b> Summary of Conjugate priors &amp; models<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="bayes.html"><a href="bayes.html#why-bayesian-statistical-estimation-revisit"><i class="fa fa-check"></i><b>3.4</b> Why Bayesian: Statistical Estimation Revisit<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="bayes.html"><a href="bayes.html#classic-frequentist-inference"><i class="fa fa-check"></i><b>3.4.1</b> Classic Frequentist Inference<span></span></a></li>
<li class="chapter" data-level="3.4.2" data-path="bayes.html"><a href="bayes.html#bayesian-inference"><i class="fa fa-check"></i><b>3.4.2</b> Bayesian Inference<span></span></a></li>
<li><a href="bayes.html#r-session-information-2">R Session information<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Prior.html"><a href="Prior.html"><i class="fa fa-check"></i><b>4</b> Prior Distributions<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="Prior.html"><a href="Prior.html#choosing-priors"><i class="fa fa-check"></i><b>4.1</b> Choosing priors<span></span></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="Prior.html"><a href="Prior.html#eliciting-priors-from-experts"><i class="fa fa-check"></i><b>4.1.1</b> Eliciting priors from experts<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="Prior.html"><a href="Prior.html#default-clincial-priors"><i class="fa fa-check"></i><b>4.2</b> Default clincial priors<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="Prior.html"><a href="Prior.html#non-informative-or-reference-priors"><i class="fa fa-check"></i><b>4.2.1</b> “Non-informative” or “reference” priors<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="Prior.html"><a href="Prior.html#minimally-informative-prior"><i class="fa fa-check"></i><b>4.2.2</b> Minimally informative prior<span></span></a></li>
<li class="chapter" data-level="4.2.3" data-path="Prior.html"><a href="Prior.html#skeptical-prior"><i class="fa fa-check"></i><b>4.2.3</b> Skeptical Prior<span></span></a></li>
<li class="chapter" data-level="4.2.4" data-path="Prior.html"><a href="Prior.html#optimisticenthusiastic-prior"><i class="fa fa-check"></i><b>4.2.4</b> Optimistic/enthusiastic Prior<span></span></a></li>
<li class="chapter" data-level="4.2.5" data-path="Prior.html"><a href="Prior.html#evaluating-priors"><i class="fa fa-check"></i><b>4.2.5</b> Evaluating Priors?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="Prior.html"><a href="Prior.html#historical-data-meta-analysis"><i class="fa fa-check"></i><b>4.3</b> Historical data (meta-analysis)<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="Prior.html"><a href="Prior.html#hierarchical-priors-and-shrinkage-priors"><i class="fa fa-check"></i><b>4.4</b> Hierarchical priors and shrinkage priors<span></span></a>
<ul>
<li><a href="Prior.html#r-session-information-3">R Session information<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="BayesMCMC.html"><a href="BayesMCMC.html"><i class="fa fa-check"></i><b>5</b> Bayesian estimation with MCMC<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="BayesMCMC.html"><a href="BayesMCMC.html#introduction-to-markov-chain-monte-carlo-methods"><i class="fa fa-check"></i><b>5.1</b> Introduction to Markov Chain Monte Carlo Methods<span></span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="BayesMCMC.html"><a href="BayesMCMC.html#gibbs-sampling"><i class="fa fa-check"></i><b>5.1.1</b> Gibbs sampling<span></span></a></li>
<li class="chapter" data-level="5.1.2" data-path="BayesMCMC.html"><a href="BayesMCMC.html#metropolis-algorithm"><i class="fa fa-check"></i><b>5.1.2</b> Metropolis algorithm<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="BayesMCMC.html"><a href="BayesMCMC.html#mcmc-diganostics---assess-convergence"><i class="fa fa-check"></i><b>5.2</b> MCMC diganostics - assess convergence<span></span></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="BayesMCMC.html"><a href="BayesMCMC.html#acceptance-rate"><i class="fa fa-check"></i><b>5.2.1</b> Acceptance Rate<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="BayesMCMC.html"><a href="BayesMCMC.html#diagnostics-using-multiple-chains"><i class="fa fa-check"></i><b>5.2.2</b> Diagnostics Using Multiple Chains<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="BayesMCMC.html"><a href="BayesMCMC.html#example-bayesian-modelling-with-brms"><i class="fa fa-check"></i><b>5.3</b> Example Bayesian modelling with brms<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="BayesMCMC.html"><a href="BayesMCMC.html#example-bayesian-logistic-regression-model"><i class="fa fa-check"></i><b>5.3.1</b> Example Bayesian logistic regression model<span></span></a></li>
<li><a href="BayesMCMC.html#r-session-information-4">R Session information<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="BayesReg1.html"><a href="BayesReg1.html"><i class="fa fa-check"></i><b>6</b> Bayesian Regression I<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="BayesReg1.html"><a href="BayesReg1.html#normal-models-and-linear-regression"><i class="fa fa-check"></i><b>6.1</b> Normal Models and Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="BayesReg1.html"><a href="BayesReg1.html#one-sample-and-two-sample-normal-regression"><i class="fa fa-check"></i><b>6.1.1</b> One-sample and two-sample normal regression<span></span></a></li>
<li class="chapter" data-level="6.1.2" data-path="BayesReg1.html"><a href="BayesReg1.html#linear-regression"><i class="fa fa-check"></i><b>6.1.2</b> Linear regression<span></span></a></li>
<li class="chapter" data-level="6.1.3" data-path="BayesReg1.html"><a href="BayesReg1.html#model-diagnostics"><i class="fa fa-check"></i><b>6.1.3</b> Model diagnostics<span></span></a></li>
<li class="chapter" data-level="6.1.4" data-path="BayesReg1.html"><a href="BayesReg1.html#model-comparision"><i class="fa fa-check"></i><b>6.1.4</b> Model comparision<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="BayesReg1.html"><a href="BayesReg1.html#hierarchical-models"><i class="fa fa-check"></i><b>6.2</b> Hierarchical models<span></span></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="BayesReg1.html"><a href="BayesReg1.html#clustered-data-multi-level"><i class="fa fa-check"></i><b>6.2.1</b> Clustered data (Multi-level)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="BayesReg1.html"><a href="BayesReg1.html#regression-regularization"><i class="fa fa-check"></i><b>6.3</b> Regression regularization<span></span></a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="BayesReg1.html"><a href="BayesReg1.html#shrinkage-priors-in-action"><i class="fa fa-check"></i><b>6.3.1</b> Shrinkage Priors in action<span></span></a></li>
<li class="chapter" data-level="6.3.2" data-path="BayesReg1.html"><a href="BayesReg1.html#bayesian-logistic-regression-model-cope-example-revisit"><i class="fa fa-check"></i><b>6.3.2</b> Bayesian logistic regression model (COPE example revisit)<span></span></a></li>
<li><a href="BayesReg1.html#r-session-information-5">R Session information<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="BayesReg2.html"><a href="BayesReg2.html"><i class="fa fa-check"></i><b>7</b> Bayesian Regression II<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="BayesReg2.html"><a href="BayesReg2.html#models-for-binary-data"><i class="fa fa-check"></i><b>7.1</b> Models for Binary Data<span></span></a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="BayesReg2.html"><a href="BayesReg2.html#logistic-regression-with-predictor"><i class="fa fa-check"></i><b>7.1.1</b> Logistic regression with predictor<span></span></a></li>
<li class="chapter" data-level="7.1.2" data-path="BayesReg2.html"><a href="BayesReg2.html#centring-continous-variable"><i class="fa fa-check"></i><b>7.1.2</b> Centring continous variable<span></span></a></li>
<li class="chapter" data-level="7.1.3" data-path="BayesReg2.html"><a href="BayesReg2.html#hierarchical-model-revisit"><i class="fa fa-check"></i><b>7.1.3</b> hierarchical model revisit<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="BayesReg2.html"><a href="BayesReg2.html#models-for-count-data"><i class="fa fa-check"></i><b>7.2</b> Models for Count Data<span></span></a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="BayesReg2.html"><a href="BayesReg2.html#modelling-rates"><i class="fa fa-check"></i><b>7.2.1</b> Modelling Rates<span></span></a></li>
<li class="chapter" data-level="7.2.2" data-path="BayesReg2.html"><a href="BayesReg2.html#expanding-the-poisson-model"><i class="fa fa-check"></i><b>7.2.2</b> Expanding the Poisson Model<span></span></a></li>
<li class="chapter" data-level="7.2.3" data-path="BayesReg2.html"><a href="BayesReg2.html#checking-and-comparing-models"><i class="fa fa-check"></i><b>7.2.3</b> Checking and comparing models<span></span></a></li>
<li><a href="BayesReg2.html#r-session-information-6">R Session information<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="BayesMeta.html"><a href="BayesMeta.html"><i class="fa fa-check"></i><b>8</b> Bayesian Meta-Analysis<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="BayesMeta.html"><a href="BayesMeta.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="BayesMeta.html"><a href="BayesMeta.html#bayesian-inference-1"><i class="fa fa-check"></i><b>8.2</b> Bayesian inference<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="BayesMeta.html"><a href="BayesMeta.html#continuous-data"><i class="fa fa-check"></i><b>8.3</b> Continuous data<span></span></a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="BayesMeta.html"><a href="BayesMeta.html#fitting-a-ma-with-brms"><i class="fa fa-check"></i><b>8.3.1</b> Fitting a MA with brms<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="BayesMeta.html"><a href="BayesMeta.html#meta-regression"><i class="fa fa-check"></i><b>8.3.2</b> Meta-regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="BayesMeta.html"><a href="BayesMeta.html#other-types-of-data"><i class="fa fa-check"></i><b>8.4</b> Other types of data<span></span></a>
<ul>
<li><a href="BayesMeta.html#r-session-information-7">R Session information<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a> <a href="https://www.kuan-liu.com/" target="blank">Developed by Kuan Liu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayes" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Session 3</span> Introduction to Bayesian inference<a href="bayes.html#bayes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="chapterintro">
<ul>
<li>Review of frequentist inferential approaches</li>
<li>Introduce Bayesian inference</li>
<li>Learn two simple Bayesian models (Beta-binomial &amp; normal-normal)</li>
<li>Discuss practical advantages and disadvantages of Bayesian approach</li>
</ul>
</div>
<script src="hideOutput.js"></script>
<p></br></p>
<div id="classical-frequentist-approach" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Classical frequentist approach<a href="bayes.html#classical-frequentist-approach" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>The classical (frequentist) statistical approach takes many forms, but the most wide-ranging is the likelihood-based approach</p></li>
<li><p>This approach specifies a distributional form for data and considers the
parameters of the distributions to be fixed constants to be estimated.</p></li>
<li><p>The parameters are estimated by finding the values that maximize the
likelihood (hence the name)</p></li>
</ul>
<blockquote>
<p>i.e. given the observed data, and assuming they come from specific distributions, what are the parameter values for these distributions that maximize the likelihood of these data?</p>
</blockquote>
<div class="important">
<p><strong>Review of likelihood function</strong></p>
<ul>
<li>Given a statistical model with some parameters (let’s call them <span class="math inline">\(\theta\)</span>), and given a set of observed data of size <span class="math inline">\(n\)</span>, <span class="math inline">\(D = \{x_1, x_2, \ldots, x_n \}\)</span>, the likelihood function, <span class="math inline">\(L(\theta, D)\)</span> is a  function that for every value of <span class="math inline">\(\theta\)</span> is equal to the probability (mass or density) of observing <span class="math inline">\(D\)</span> given <span class="math inline">\(\theta\)</span></li>
<li>i.e. <span class="math inline">\(L(\theta, D) = L_D(\theta) = P(Data | \theta)\)</span></li>
<li>if we assume <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> are independent and identically distributed, we can express the likelihood function as</li>
</ul>
<p><span class="math display">\[ L(\theta, D) = P(x_1 \mid \theta)\times P(x_2 \mid \theta) \ldots \times P(x_n \mid \theta) = \prod_{i=1}^n P(x_i\mid \theta).\]</span></p>
<p><strong>Example - Bernoulli trials</strong> Suppose we want to estimate the risk of death <span class="math inline">\(\theta\)</span> after a surgery</p>
<ul>
<li>We assume that every patient has the same risk <span class="math inline">\(\theta\)</span></li>
<li>We collect data from 10 surgeries and we find that 3 patients died and 7 survived,</li>
<li>What is the likelihood function for <span class="math inline">\(\theta\)</span> in this example?</li>
</ul>
<p>The distribution for each patient is <span class="math inline">\(Bernoulli(\theta)\)</span>  the probability of the number of those who died out of <span class="math inline">\(n\)</span> (here <span class="math inline">\(n\)</span>=10) is <span class="math inline">\(Binomial(\theta, 10)\)</span></p>
<p>The probability mass function of the binomial is <span class="math display">\[p(x|\theta, n) = {n \choose x} \theta^x (1-\theta)^{n-x}\]</span></p>
<p>The likelihood function of the observed data (3 deaths out of 10) given <span class="math inline">\(\theta\)</span> is <span class="math display">\[ L_D(\theta) = p(x=3| \theta) = {10 \choose 3} \theta^3 (1-\theta)^{10-3} \propto \theta^3 (1-\theta)^{10-3}\]</span></p>
<p><strong>Maximum Likelihood Estimator</strong></p>
<ul>
<li>The value that maximizes the likelihood function is called the maximum likelihood estimator or MLE</li>
<li>It is the “most likely” value for <span class="math inline">\(\theta\)</span> given the observed data</li>
<li>In this example it is equal to <span class="math inline">\(\hat{\theta}_{mle} = \frac{x}{n} = \frac{3}{10}=0.3\)</span> (the observed proportion of event), which can be obtained by taking the first derivative of the loglikelihood and calculate the value of <span class="math inline">\(\theta\)</span> that yields</li>
</ul>
<p><span class="math display">\[\begin{aligned}
LogL(\theta, D) &amp;= log({10 \choose 3}) + 3\ log(\theta) + (10-3)\ log(1- \theta) \\
\frac{\partial}{\partial \theta}LogL(\theta, D) &amp; = \frac{3}{\theta} - \frac{10-3}{1-p} = 0 \\
\hat{\theta}_{mle} &amp; = \frac{3}{10}=0.3
\end{aligned}\]</span></p>
<ul>
<li>It is the most commonly method to estimate a parameter in frequentist statistics</li>
</ul>
</div>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="bayes.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co">#simulating a sequence of probability representing parameter \theta;</span></span>
<span id="cb29-2"><a href="bayes.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co">#\theta, probability of success, value between 0 and 1;</span></span>
<span id="cb29-3"><a href="bayes.html#cb29-3" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length=</span><span class="dv">1000</span>) </span>
<span id="cb29-4"><a href="bayes.html#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co">#coding Binomial likelihood given x = 3 and n = 10;</span></span>
<span id="cb29-5"><a href="bayes.html#cb29-5" aria-hidden="true" tabindex="-1"></a>L <span class="ot">&lt;-</span> <span class="fu">choose</span>(<span class="dv">10</span>,<span class="dv">3</span>)<span class="sc">*</span>theta<span class="sc">^</span><span class="dv">3</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span>(<span class="dv">10-3</span>)</span>
<span id="cb29-6"><a href="bayes.html#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co">#coding log Binomial likelihood given x = 3 and n = 10;</span></span>
<span id="cb29-7"><a href="bayes.html#cb29-7" aria-hidden="true" tabindex="-1"></a>logL <span class="ot">&lt;-</span> <span class="fu">log</span>(<span class="fu">choose</span>(<span class="dv">10</span>,<span class="dv">3</span>)) <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span><span class="fu">log</span>(theta)<span class="sc">+</span> (<span class="dv">10-3</span>)<span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>theta))</span>
<span id="cb29-8"><a href="bayes.html#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Ploting likelihood function</span></span>
<span id="cb29-9"><a href="bayes.html#cb29-9" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">theta=</span>theta, <span class="at">L=</span>L)</span>
<span id="cb29-10"><a href="bayes.html#cb29-10" aria-hidden="true" tabindex="-1"></a>p1<span class="ot">&lt;-</span><span class="fu">ggplot</span>(<span class="at">data=</span>d, <span class="fu">aes</span>(theta,L)) <span class="sc">+</span></span>
<span id="cb29-11"><a href="bayes.html#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>()<span class="sc">+</span></span>
<span id="cb29-12"><a href="bayes.html#cb29-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggtitle</span>(<span class="st">&quot;Binomial likelihood x = 3 and n=10&quot;</span>) <span class="sc">+</span> </span>
<span id="cb29-13"><a href="bayes.html#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb29-14"><a href="bayes.html#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Ploting likelihood function</span></span>
<span id="cb29-15"><a href="bayes.html#cb29-15" aria-hidden="true" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">theta=</span>theta, <span class="at">logL=</span>logL)</span>
<span id="cb29-16"><a href="bayes.html#cb29-16" aria-hidden="true" tabindex="-1"></a>p2<span class="ot">&lt;-</span><span class="fu">ggplot</span>(<span class="at">data=</span>d, <span class="fu">aes</span>(theta,logL)) <span class="sc">+</span></span>
<span id="cb29-17"><a href="bayes.html#cb29-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>()<span class="sc">+</span></span>
<span id="cb29-18"><a href="bayes.html#cb29-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggtitle</span>(<span class="st">&quot;Log Binomial likelihood x = 3 and n=10&quot;</span>) <span class="sc">+</span> </span>
<span id="cb29-19"><a href="bayes.html#cb29-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span>
<span id="cb29-20"><a href="bayes.html#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="bayes.html#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(p1, p2,  <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-39-1.png" width="768" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="bayes.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">#negative loglikelihood function of binomial;</span></span>
<span id="cb30-2"><a href="bayes.html#cb30-2" aria-hidden="true" tabindex="-1"></a>neglogL <span class="ot">&lt;-</span> <span class="cf">function</span>(theta){<span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="at">x=</span><span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>, theta, <span class="at">log =</span> <span class="cn">TRUE</span>))}</span>
<span id="cb30-3"><a href="bayes.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">#optimize:</span></span>
<span id="cb30-4"><a href="bayes.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="fu">optim</span>(<span class="at">par =</span> <span class="fl">0.5</span>, <span class="at">fn=</span>neglogL, <span class="at">method =</span> <span class="st">&quot;Brent&quot;</span>, <span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>, <span class="at">hessian =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## $par
## [1] 0.3
## 
## $value
## [1] 1.321151
## 
## $counts
## function gradient 
##       NA       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
## 
## $hessian
##          [,1]
## [1,] 47.61985</code></pre>
<div class="important">
<p><strong>Maximum Likelihood confidence interval</strong></p>
<p>MLE satisfies the following two properties called <strong>consistency</strong> and <strong>asymptotic normality</strong>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Consistency.</strong> We say that an estimate <span class="math inline">\(\hat{\theta}\)</span> is consistent if <span class="math inline">\(\hat{\theta} \rightarrow \theta_0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>, where <span class="math inline">\(\theta_0\)</span> is the true unknown parameter and <span class="math inline">\(n\)</span> is sample size.</p></li>
<li><p><strong>Asymptotic normality</strong> <span class="math inline">\(\hat{\theta}\)</span> is asymptotic normality if</p></li>
</ol>
<p><span class="math display">\[ \sqrt{n} (\hat{\theta} - \theta_0) \rightarrow^d N(0, \sigma_{\theta_0}^2) \]</span>
where <span class="math inline">\(\sigma_{\theta_0}^2)\)</span> is the asymptotic variance of the estimate <span class="math inline">\(\hat{\theta}\)</span>. Asymptotic normality says that the estimator not only converges to the unknown parameter, but it converges fast enough, at a rate <span class="math inline">\(1/\sqrt{n}\)</span>.</p>
<p>Given this properties, we can use <strong>Fisher information</strong> to estimate the variance of MLE and subsequently obtaining confidence intervals.
- MLE Asymptotic normality with Fisher information, <span class="math inline">\(I(\theta_0)\)</span></p>
<p><span class="math display">\[ \sqrt{n} (\hat{\theta}_{mle} - \theta_0) \rightarrow^d N(0, \frac{1}{I(\theta_0)}) \]</span></p>
<ul>
<li><p>Fisher information is defined using the second derivative of the loglikelihood.
<span class="math display">\[ I(\theta) = - E[\frac{\partial^2}{\partial \theta^2} logL(x_1,\ldots, x_n \mid \theta)]\]</span></p>
<ul>
<li>e.g., for binomail distribution, <span class="math inline">\(I(\theta)=\frac{n}{\theta(1-\theta)}\)</span>, thus the 95% CI for <span class="math inline">\(\hat{\theta}_{mle}\)</span> is
<span class="math display">\[ \hat{\theta}_{mle} \pm 1.96 \sqrt{\frac{\hat{\theta}_{mle}(1-\hat{\theta}_{mle})}{n}} \]</span></li>
</ul>
<p>which gives us <span class="math inline">\(0.3 \pm 1.96 \times \sqrt{\frac{0.3 \times 0.7}{10}}\)</span>, [0.016, 0.584].</p></li>
</ul>
</div>
<p>How to calculate variance of MLE in R?</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="bayes.html#cb32-1" aria-hidden="true" tabindex="-1"></a>mle<span class="ot">&lt;-</span><span class="fu">optim</span>(<span class="at">par =</span> <span class="fl">0.5</span>, <span class="at">fn=</span>neglogL, <span class="at">method =</span> <span class="st">&quot;Brent&quot;</span>, <span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>, <span class="at">hessian =</span> <span class="cn">TRUE</span>)</span>
<span id="cb32-2"><a href="bayes.html#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="bayes.html#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co"># solve(mle$hessian) # to compute the inverse of hessian which is the approximate the variance of theta;</span></span>
<span id="cb32-4"><a href="bayes.html#cb32-4" aria-hidden="true" tabindex="-1"></a>upperbound<span class="ot">&lt;-</span><span class="fl">0.3</span> <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">solve</span>(mle<span class="sc">$</span>hessian))</span>
<span id="cb32-5"><a href="bayes.html#cb32-5" aria-hidden="true" tabindex="-1"></a>lowerbound<span class="ot">&lt;-</span><span class="fl">0.3</span> <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">solve</span>(mle<span class="sc">$</span>hessian))</span>
<span id="cb32-6"><a href="bayes.html#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="bayes.html#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">&quot;95% CI for theta is:&quot;</span>,<span class="fu">round</span>(lowerbound,<span class="dv">3</span>),<span class="st">&quot;-&quot;</span>, <span class="fu">round</span>(upperbound,<span class="dv">3</span>)))</span></code></pre></div>
<pre><code>## [1] &quot;95% CI for theta is: 0.016 - 0.584&quot;</code></pre>
<div class="guidedexercise">
<p><strong>Practice MLE estimation in R (Tutorial Practice)</strong></p>
<p>Suppose we want to estimate the risk of death after a surgery and We assume that every patient has the same risk . We collect data from 100 surgeries and we find that 30 patients died and 70 survived,</p>
<ul>
<li>What is the likelihood function for in this example?</li>
<li>What is the MLE estimator given the observed data?</li>
<li>Can you construct the 95% CI confidence interval of the MLE estimator?</li>
<li>What is you conclusion comparing this estimator to the MLE obtain from the smaller dataset (10 surgeries, 3 patients died and 7 survived)?</li>
</ul>
</div>
</div>
<div id="introduction-to-bayesian-approach" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Introduction to Bayesian approach<a href="bayes.html#introduction-to-bayesian-approach" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="review-from-session-1" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Review from session 1<a href="bayes.html#review-from-session-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>In the Bayesian approach, everything that is not data is considered as a parameter</li>
<li>Uncertainty about these parameters is expressed using probability distributions
and probabilistic statements</li>
<li>A prior distribution expresses what is known or believed independently of the data</li>
<li>This prior is updated as data or new evidence is presented</li>
<li>The posterior distribution expresses the updated belief</li>
</ul>
<div class="important">
<p><strong>Recall Bayes theorem</strong></p>
<p>Let <span class="math inline">\(D = \text{patient has disease}\)</span> and <span class="math inline">\(Y = \text{patient has a positive diagnostic test}\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
P(D \mid T) &amp; = \frac{P(T \mid D)P(D)}{P(T)} \\
&amp; = \frac{P(T \mid D)P(D)}{P(T \mid D)P(D) + P(T \mid D^c)P(D^c)}
\end{aligned}\]</span></p>
<ul>
<li><span class="math inline">\(P(T\mid D)\)</span> is the <strong>likelihood</strong> of the outcome (positive test) given the unknown parameter (disease state)</li>
<li><span class="math inline">\(P(D)\)</span> is <strong>pre-test probability</strong> (prior probability) of disease</li>
<li><span class="math inline">\(P(D\mid T)\)</span> is the post-test probability of disease which can be obtained by multiplying the <strong>likelihood</strong> and the <strong>pre-test probabiltiy</strong>.</li>
<li>Here, to calculate <span class="math inline">\(P(D\mid T)\)</span> we need <span class="math inline">\(P(D)\)</span>!</li>
<li>A very sensitive test (e.g., P(TD) = 0.99) can still result in a small post-test probability if the prior probability of disease, <span class="math inline">\(P(D)\)</span>, is low!</li>
</ul>
</div>
<p><strong>The Bayesian approach to estimating parameters stems from Bayes’ theorem for continuous variables:</strong></p>
<p>Let <span class="math inline">\(\theta\)</span> be the parameter of interest and <span class="math inline">\(y\)</span> be the observed data,</p>
<p><span class="math display">\[\begin{aligned}
P(\theta \mid y) &amp; = \frac{P(y \mid \theta)P(\theta)}{P(y)} \\
&amp; = \frac{\text{likelihood of data given parameter} \times \text{prior}}{\text{marginal distribution of data free of the parameter}} \\
&amp; \propto \text{likelihood}(y \mid \theta ) \times \text{prior}(\theta)
\end{aligned}\]</span></p>
<ul>
<li><p><span class="math inline">\(P(y)\)</span> is called a normalizing factor, it’s in place to ensure that <span class="math inline">\(\int P(\theta \mid y) d\theta = 1\)</span>, that is the posterior distribution of <span class="math inline">\(\theta\)</span> is a proper probability distribution with area under the density curve equals to 1.</p></li>
<li><p>Its value is not of interest, unless we are comparing between data models.</p></li>
<li><p>The essence of Bayes theorem only concerns the terms involving the parameter, <span class="math inline">\(\theta\)</span>, hence <span class="math inline">\(P(\theta \mid y) \propto P(y\mid \theta)P(\theta)\)</span>.</p></li>
</ul>
<div class="workedexample">
<p><strong>Estimating a Proportion</strong></p>
<p>Suppose you have observed 6 patients in a Phase I RCT on a given dose of drug,
- 0 out of 6 patients have had an adverse event
- decision to escalate to a higher dose if it’s unlikely that the current dosing results in a true proportion of adverse events above 20% (i.e., given the current data, is there sufficient evidence to infer the true proportion of adverse event is less than 20%, if so we can increase the dose level)</p>
<ul>
<li>This is a classic phase I estimate, under frequentist test (Exact Binomial Test) we have</li>
</ul>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="bayes.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">n=</span><span class="dv">6</span>, <span class="at">p =</span> <span class="fl">0.2</span>,</span>
<span id="cb34-2"><a href="bayes.html#cb34-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">alternative =</span> <span class="fu">c</span>(<span class="st">&quot;less&quot;</span>),</span>
<span id="cb34-3"><a href="bayes.html#cb34-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  0 and 6
## number of successes = 0, number of trials = 6, p-value = 0.2621
## alternative hypothesis: true probability of success is less than 0.2
## 95 percent confidence interval:
##  0.0000000 0.3930378
## sample estimates:
## probability of success 
##                      0</code></pre>
<ul>
<li>The observed proportion <span class="math inline">\(\hat{\theta}=0\)</span> with 95% CI: 0 - 0.39.</li>
<li>How much evidence we have that AE rate is &lt; 20%?</li>
</ul>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="bayes.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co">#suppose we observe 0 adverse event out of 14 patients; </span></span>
<span id="cb36-2"><a href="bayes.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co">#the test results below suggest we would reject the null hypothesis;</span></span>
<span id="cb36-3"><a href="bayes.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">#at 0.05 alpha level and conclude the true AE rate is &lt; 20%;</span></span>
<span id="cb36-4"><a href="bayes.html#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="bayes.html#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">n=</span><span class="dv">14</span>, <span class="at">p =</span> <span class="fl">0.2</span>,</span>
<span id="cb36-6"><a href="bayes.html#cb36-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">alternative =</span> <span class="fu">c</span>(<span class="st">&quot;less&quot;</span>),</span>
<span id="cb36-7"><a href="bayes.html#cb36-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  0 and 14
## number of successes = 0, number of trials = 14, p-value = 0.04398
## alternative hypothesis: true probability of success is less than 0.2
## 95 percent confidence interval:
##  0.0000000 0.1926362
## sample estimates:
## probability of success 
##                      0</code></pre>
</div>
<p><strong>What would a Bayesian do?</strong></p>
<p>To make probability statements about <span class="math inline">\(\theta\)</span> after observing data <span class="math inline">\(y\)</span>, we need a probability distribution for <span class="math inline">\(\theta\)</span> given <span class="math inline">\(y\)</span> (the posterior distribution).</p>
<p>1.First, we need to specify a prior distribution for <span class="math inline">\(\theta\)</span>, <span class="math inline">\(P(\theta)\)</span>.</p>
<ul>
<li><p>Example 1: We might have no idea about <span class="math inline">\(\theta\)</span> other than that it lies in the interval [0,1] and thus specify a unif(0,1). Let <span class="math inline">\(\theta \sim U(0,1)\)</span>, the prior probability distribution (p.d.f) is
<span class="math display">\[ P(\theta) = \frac{1}{1-0} = 1.\]</span></p></li>
<li><p>Example 2: We might have some knowledge about the range of <span class="math inline">\(\theta\)</span>, say, we are believe <span class="math inline">\(0.05&lt;\theta&lt;0.5\)</span>. We can have
<span class="math display">\[ \theta \sim U(0.05, 0.5)\]</span>
<span class="math display">\[P(\theta) = \frac{1}{0.5-0.05} = 2.22.\]</span></p></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><p>We assume the <span class="math inline">\(P(y \mid \theta)\)</span> follows a binomial distribution, thus the likelihood of the observed data given <span class="math inline">\(\theta\)</span> is
<span class="math display">\[ P(y = 0 \mid \theta) = {6 \choose 0} \theta^0 (1-\theta)^6 = (1-\theta)^6\]</span></p></li>
<li><p>The posterior then becomes (given example prior 1)</p></li>
</ol>
<p><span class="math display">\[\begin{align}
P(\theta \mid y = 0) &amp;= \frac{P(y = 0 \mid \theta) \times P(\theta)}{P(y=0)} \\
&amp; = \frac{(1-\theta)^6 \times 1}{P(y=0)} \\
&amp; = \text{Constant} \times (1-\theta)^6 \\
 &amp; \propto (1-\theta)^6 
\end{align}\]</span></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="bayes.html#cb38-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">p_grid =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">1000</span>),</span>
<span id="cb38-2"><a href="bayes.html#cb38-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">y      =</span> <span class="dv">0</span>, </span>
<span id="cb38-3"><a href="bayes.html#cb38-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">n      =</span> <span class="dv">6</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb38-4"><a href="bayes.html#cb38-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">prior      =</span> <span class="fu">dunif</span>(p_grid, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb38-5"><a href="bayes.html#cb38-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">likelihood =</span> <span class="fu">dbinom</span>(y, n, p_grid)) <span class="sc">%&gt;%</span> </span>
<span id="cb38-6"><a href="bayes.html#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">posterior =</span> likelihood <span class="sc">*</span> prior )</span>
<span id="cb38-7"><a href="bayes.html#cb38-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-8"><a href="bayes.html#cb38-8" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(prior<span class="sc">:</span>posterior) <span class="sc">%&gt;%</span> </span>
<span id="cb38-9"><a href="bayes.html#cb38-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># this line allows us to dictate the order in which the panels will appear</span></span>
<span id="cb38-10"><a href="bayes.html#cb38-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">name =</span> <span class="fu">factor</span>(name, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;prior&quot;</span>, <span class="st">&quot;likelihood&quot;</span>, <span class="st">&quot;posterior&quot;</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb38-11"><a href="bayes.html#cb38-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p_grid, <span class="at">y =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb38-12"><a href="bayes.html#cb38-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_area</span>(<span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb38-13"><a href="bayes.html#cb38-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;grey&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>)) <span class="sc">+</span></span>
<span id="cb38-14"><a href="bayes.html#cb38-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span> name, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>)<span class="sc">+</span></span>
<span id="cb38-15"><a href="bayes.html#cb38-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-44"></span>
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-44-1.png" alt="Approximate posterior distribution obtained using Bayes' rule with UNIF(0,1) prior. In this example, the normalizaing term P(y=0) is not considered." width="672" />
<p class="caption">
Figure 3.1: Approximate posterior distribution obtained using Bayes’ rule with UNIF(0,1) prior. In this example, the normalizaing term P(y=0) is not considered.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-45"></span>
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-45-1.png" alt="Approximate posterior distribution obtained using Bayes' rule with UNIF(0.05,0.5) prior. In this example, the normalizaing term P(y=0) is not considered." width="672" />
<p class="caption">
Figure 3.2: Approximate posterior distribution obtained using Bayes’ rule with UNIF(0.05,0.5) prior. In this example, the normalizaing term P(y=0) is not considered.
</p>
</div>
<p><strong>Why is P(y=0) free of <span class="math inline">\(\theta\)</span>?</strong></p>
<ul>
<li>The <strong>law of total probability</strong> for discrete parameter values can be used</li>
<li>Suppose there are two possible values of parameter <span class="math inline">\(\theta\)</span>, 0.5 and 0.1.</li>
<li>Suppose we know the prior distribution of <span class="math inline">\(\theta\)</span>: <span class="math inline">\(P(\theta = 0.5) = 0.8\)</span> and
<span class="math inline">\(P(\theta = 0.1) = 0.2\)</span></li>
<li>Likelihood values are calculated given a known <span class="math inline">\(\theta\)</span>, so then don’t include the parameter <span class="math inline">\(\theta\)</span>.</li>
<li>Call these <span class="math inline">\(P_{0.5} = P(Y = 0 \mid \theta = 0.5)\)</span> and <span class="math inline">\(P_{0.1} = P(Y = 0 \mid \theta = 0.1)\)</span>,</li>
<li>Putting all components together using law of total probability, <span class="math inline">\(P(Y = 0)\)</span> does not involved the unknown <span class="math inline">\(\theta\)</span>
<span class="math display">\[P(Y = 0) = P(Y = 0 \mid \theta = 0.5)P(\theta = 0.5) + P(Y = 0 \mid \theta = 0.1)P(\theta = 0.1)\]</span></li>
</ul>
<p><span class="math display">\[ P(Y = 0) = P_{0.5} \times 0.8 + P_{0.1} \times 0.2 \]</span></p>
<ul>
<li><p>In case of a continuous parameter value, we can obtain <span class="math inline">\(P(y=0)\)</span> by integrating over the space of <span class="math inline">\(\theta\)</span> as following
<span class="math display">\[P(y=0) = \int_{\theta=0}^{\theta=1} P(y=0 \mid \theta) P(\theta) \ d \theta\]</span></p></li>
<li><p>Integrating over <span class="math inline">\(\theta\)</span> is analogous to summing over a set of discrete values of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>After integration over <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta\)</span> is no longer featured in <span class="math inline">\(P(y=0)\)</span>.</p></li>
<li><p>In this example, we have</p></li>
</ul>
<p><span class="math display">\[P(y=0) = \int_{\theta=0}^{\theta=1} P(y=0 \mid \theta) \times \frac{1}{1-0} \ d \theta \]</span>
<span class="math display">\[P(y=0) = \int_{\theta=0}^{\theta=1} P(y=0 \mid \theta) \times \frac{1}{0.5-0.05} \ d \theta \]</span></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="bayes.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Integration in R with one variable;</span></span>
<span id="cb39-2"><a href="bayes.html#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="co">#Unif(0,1);</span></span>
<span id="cb39-3"><a href="bayes.html#cb39-3" aria-hidden="true" tabindex="-1"></a>integrand <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) {(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">6</span>}</span>
<span id="cb39-4"><a href="bayes.html#cb39-4" aria-hidden="true" tabindex="-1"></a>normalizing_constant<span class="ot">&lt;-</span><span class="fu">integrate</span>(integrand, <span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>)</span>
<span id="cb39-5"><a href="bayes.html#cb39-5" aria-hidden="true" tabindex="-1"></a>normalizing_constant</span></code></pre></div>
<pre><code>## 0.1428571 with absolute error &lt; 0.0000000000000016</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="bayes.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Unif(0.05,0.5);</span></span>
<span id="cb41-2"><a href="bayes.html#cb41-2" aria-hidden="true" tabindex="-1"></a>integrand2 <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) {((<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">6</span>)<span class="sc">/</span>(<span class="fl">0.5-0.05</span>)}</span>
<span id="cb41-3"><a href="bayes.html#cb41-3" aria-hidden="true" tabindex="-1"></a>normalizing_constant2<span class="ot">&lt;-</span><span class="fu">integrate</span>(integrand2, <span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>)</span>
<span id="cb41-4"><a href="bayes.html#cb41-4" aria-hidden="true" tabindex="-1"></a>normalizing_constant2</span></code></pre></div>
<pre><code>## 0.3174603 with absolute error &lt; 0.0000000000000035</code></pre>
<div class="fold s">

<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="bayes.html#cb43-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">p_grid =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">1000</span>),</span>
<span id="cb43-2"><a href="bayes.html#cb43-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">y      =</span> <span class="dv">0</span>, </span>
<span id="cb43-3"><a href="bayes.html#cb43-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">n      =</span> <span class="dv">6</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb43-4"><a href="bayes.html#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">prior1 =</span> <span class="fu">dunif</span>(p_grid, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb43-5"><a href="bayes.html#cb43-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">prior2 =</span> <span class="fu">dunif</span>(p_grid, <span class="fl">0.05</span>, <span class="fl">0.5</span>),</span>
<span id="cb43-6"><a href="bayes.html#cb43-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">likelihood =</span> <span class="fu">dbinom</span>(y, n, p_grid)) <span class="sc">%&gt;%</span> </span>
<span id="cb43-7"><a href="bayes.html#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">posterior1 =</span> likelihood <span class="sc">*</span> prior1 <span class="sc">/</span> normalizing_constant<span class="sc">$</span>value,</span>
<span id="cb43-8"><a href="bayes.html#cb43-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">posterior2 =</span> likelihood <span class="sc">*</span> prior2 <span class="sc">/</span> normalizing_constant2<span class="sc">$</span>value)</span>
<span id="cb43-9"><a href="bayes.html#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="bayes.html#cb43-10" aria-hidden="true" tabindex="-1"></a>d <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(posterior1<span class="sc">:</span>posterior2) <span class="sc">%&gt;%</span> </span>
<span id="cb43-11"><a href="bayes.html#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p_grid, <span class="at">y =</span> value, <span class="at">group =</span> <span class="fu">as.factor</span>(name))) <span class="sc">+</span></span>
<span id="cb43-12"><a href="bayes.html#cb43-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_area</span>(<span class="fu">aes</span>(<span class="at">fill =</span> name),<span class="at">position=</span><span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb43-13"><a href="bayes.html#cb43-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_fill_manual</span>(<span class="at">name =</span> <span class="st">&quot;Prior&quot;</span>, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;UNIF(0,1)&quot;</span>,<span class="st">&quot;UNIF(0.05,0.5)&quot;</span>),<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;#d8b365&quot;</span>,  <span class="st">&quot;#5ab4ac&quot;</span>)) <span class="sc">+</span></span>
<span id="cb43-14"><a href="bayes.html#cb43-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.2</span>, <span class="at">size=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb43-15"><a href="bayes.html#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x=</span><span class="fl">0.3</span>, <span class="at">y=</span><span class="dv">6</span>, <span class="at">label=</span> <span class="fu">expression</span>(<span class="fu">paste</span>(theta, <span class="st">&quot;=0.2&quot;</span>)), <span class="at">size =</span> <span class="dv">5</span>) <span class="sc">+</span> </span>
<span id="cb43-16"><a href="bayes.html#cb43-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(theta), <span class="at">y =</span> <span class="st">&quot;posterior probability density&quot;</span>, <span class="at">title=</span><span class="st">&quot;Posterior distribution given uniform priors and binary data&quot;</span>)<span class="sc">+</span></span>
<span id="cb43-17"><a href="bayes.html#cb43-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-47"></span>
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-47-1.png" alt="Posterior distribution obtained using Bayes' rule with UNIF(0,1) and UNIF(0.05,0.5) priors" width="672" />
<p class="caption">
Figure 3.3: Posterior distribution obtained using Bayes’ rule with UNIF(0,1) and UNIF(0.05,0.5) priors
</p>
</div>
<div>

<div class="guidedexercise">
<p><strong>Practice posterior estimation with brms package in R (Tutorial Practice)</strong></p>
<blockquote>
<p>we will introduce MCMC in later sessions.</p>
</blockquote>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="bayes.html#cb44-1" aria-hidden="true" tabindex="-1"></a>dat1<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">y=</span><span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">6</span>))</span>
<span id="cb44-2"><a href="bayes.html#cb44-2" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">brm</span>(<span class="at">data =</span> dat1, </span>
<span id="cb44-3"><a href="bayes.html#cb44-3" aria-hidden="true" tabindex="-1"></a>      <span class="at">family =</span> <span class="fu">bernoulli</span>(<span class="at">link =</span> <span class="st">&quot;identity&quot;</span>),</span>
<span id="cb44-4"><a href="bayes.html#cb44-4" aria-hidden="true" tabindex="-1"></a>      y <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb44-5"><a href="bayes.html#cb44-5" aria-hidden="true" tabindex="-1"></a>      <span class="at">prior =</span> <span class="fu">c</span>(<span class="fu">prior</span>(<span class="fu">uniform</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">class =</span> Intercept)),</span>
<span id="cb44-6"><a href="bayes.html#cb44-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">iter =</span> <span class="dv">1000</span> <span class="sc">+</span> <span class="dv">2500</span>, <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">chains =</span> <span class="dv">2</span>, <span class="at">cores =</span> <span class="dv">2</span>,</span>
<span id="cb44-7"><a href="bayes.html#cb44-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">seed =</span> <span class="dv">123</span>)</span>
<span id="cb44-8"><a href="bayes.html#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="bayes.html#cb44-9" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">brm</span>(<span class="at">data =</span> dat1, </span>
<span id="cb44-10"><a href="bayes.html#cb44-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">family =</span> <span class="fu">bernoulli</span>(<span class="at">link =</span> <span class="st">&quot;identity&quot;</span>),</span>
<span id="cb44-11"><a href="bayes.html#cb44-11" aria-hidden="true" tabindex="-1"></a>      y <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb44-12"><a href="bayes.html#cb44-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">prior =</span> <span class="fu">c</span>(<span class="fu">prior</span>(<span class="fu">uniform</span>(<span class="fl">0.05</span>, <span class="fl">0.5</span>), <span class="at">class =</span> Intercept)),</span>
<span id="cb44-13"><a href="bayes.html#cb44-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">iter =</span> <span class="dv">1000</span> <span class="sc">+</span> <span class="dv">2500</span>, <span class="at">warmup =</span> <span class="dv">1000</span>, <span class="at">chains =</span> <span class="dv">2</span>, <span class="at">cores =</span> <span class="dv">2</span>,</span>
<span id="cb44-14"><a href="bayes.html#cb44-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">seed =</span> <span class="dv">123</span>)</span>
<span id="cb44-15"><a href="bayes.html#cb44-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-16"><a href="bayes.html#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit1)</span>
<span id="cb44-17"><a href="bayes.html#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span>
<span id="cb44-18"><a href="bayes.html#cb44-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb44-19"><a href="bayes.html#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit1)</span>
<span id="cb44-20"><a href="bayes.html#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit2)</span>
<span id="cb44-21"><a href="bayes.html#cb44-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-22"><a href="bayes.html#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="fu">mcmc_areas</span>(</span>
<span id="cb44-23"><a href="bayes.html#cb44-23" aria-hidden="true" tabindex="-1"></a>  fit1, </span>
<span id="cb44-24"><a href="bayes.html#cb44-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;b_Intercept&quot;</span>),</span>
<span id="cb44-25"><a href="bayes.html#cb44-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob =</span> <span class="fl">0.80</span>, <span class="co"># 80% inner intervals;</span></span>
<span id="cb44-26"><a href="bayes.html#cb44-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob_outer =</span> <span class="fl">0.95</span>, <span class="co"># 99% outter intervals;</span></span>
<span id="cb44-27"><a href="bayes.html#cb44-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">point_est =</span> <span class="st">&quot;mean&quot;</span></span>
<span id="cb44-28"><a href="bayes.html#cb44-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-29"><a href="bayes.html#cb44-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-30"><a href="bayes.html#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="fu">mcmc_areas</span>(</span>
<span id="cb44-31"><a href="bayes.html#cb44-31" aria-hidden="true" tabindex="-1"></a>  fit2, </span>
<span id="cb44-32"><a href="bayes.html#cb44-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;b_Intercept&quot;</span>),</span>
<span id="cb44-33"><a href="bayes.html#cb44-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob =</span> <span class="fl">0.80</span>, <span class="co"># 80% inner intervals;</span></span>
<span id="cb44-34"><a href="bayes.html#cb44-34" aria-hidden="true" tabindex="-1"></a>  <span class="at">prob_outer =</span> <span class="fl">0.95</span>, <span class="co"># 99% outter intervals;</span></span>
<span id="cb44-35"><a href="bayes.html#cb44-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">point_est =</span> <span class="st">&quot;mean&quot;</span></span>
<span id="cb44-36"><a href="bayes.html#cb44-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-37"><a href="bayes.html#cb44-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-38"><a href="bayes.html#cb44-38" aria-hidden="true" tabindex="-1"></a><span class="fu">hypothesis</span>(fit1, <span class="st">&#39;Intercept &lt; 0.2&#39;</span>)</span>
<span id="cb44-39"><a href="bayes.html#cb44-39" aria-hidden="true" tabindex="-1"></a><span class="fu">hypothesis</span>(fit2, <span class="st">&#39;Intercept &lt; 0.2&#39;</span>)</span></code></pre></div>
</div>
</div>
<div id="the-beta-binomial-model" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> The Beta-binomial model<a href="bayes.html#the-beta-binomial-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>With a single sample of <span class="math inline">\(n\)</span> binary outcomes, we have one unknown parameter: <span class="math inline">\(\theta\)</span> and <span class="math inline">\(0 \leq \theta \leq 1\)</span>.</li>
<li>We need a prior distribution for <span class="math inline">\(\theta\)</span> (e.g., proportion, risk, probability of event etc): <span class="math inline">\(P(\theta)\)</span>
<ul>
<li>To express indifference between all values of <span class="math inline">\(\theta\)</span>, we can use a uniform distribution on <span class="math inline">\(\theta\)</span>, as we did in the previous example</li>
<li>To express belief (e.g. based on external evidence) that some values of <span class="math inline">\(\theta\)</span> are more likely that others, it is convenient to use a <strong>beta distribution</strong></li>
<li>This has two parameters, often labelled as <span class="math inline">\(\alpha\)</span> (also written as a) and <span class="math inline">\(\beta\)</span> (also written as b), which we can choose to represent the strength of the external evidence</li>
<li>If a parameter has a Beta(a,b) distribution, then the prior mean is
<span class="math display">\[\frac{a}{a+b}\]</span></li>
<li>The beta distribution prior for the binomial is useful for illustrating how the Bayesian approach combines prior information and new data</li>
</ul></li>
</ul>
<div class="important">
<p><strong>Beta-binomial model</strong></p>
<ul>
<li>Recall the likelihood for a binomial outcome of x successes in n trials,</li>
</ul>
<p><span class="math display">\[ P(x \mid \theta) \propto \theta^x (1-\theta)^{n-x}\]</span></p>
<ul>
<li>The <span class="math inline">\(Beta(\alpha,\beta)\)</span> prior has the same functional form for <span class="math inline">\(\theta\)</span>,</li>
</ul>
<p><span class="math display">\[ P( \theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}\]</span></p>
<ul>
<li>We find the posterior as</li>
</ul>
<p><span class="math display">\[\begin{align}
P(\theta \mid x) &amp; \propto P(x \mid \theta) \times P( \theta) \\
&amp; \propto \theta^x (1-\theta)^{n-x} \times \theta^{\alpha-1} (1-\theta)^{\beta-1} \\
&amp;  \propto \theta^{x+\alpha-1}  (1-\theta)^{n-x+\beta-1}
\end{align}\]</span></p>
<ul>
<li><p>Thus, comparing to the <span class="math inline">\(Beta(\alpha,\beta)\)</span> prior, the posterior just changes the exponents by adding <strong>x and n-x</strong>, respectively.</p>
<ul>
<li>Comparing to the prior, make two changes to get the posterior:
<ol style="list-style-type: decimal">
<li><span class="math inline">\(a \rightarrow a+x\)</span>, [a + number of events]</li>
<li><span class="math inline">\(b \rightarrow b+(n-x)\)</span>, [b + number of non-events]</li>
</ol></li>
<li>Quite simply, when <span class="math inline">\(x\)</span> events have been observed in n subjects, the prior</li>
</ul>
<p><span class="math display">\[ \theta \sim Beta(\alpha, \beta) \]</span></p>
<ul>
<li>gives the posterior</li>
</ul>
<p><span class="math display">\[ \theta \mid x \sim Beta(\alpha+x, \beta+n-x) \]</span></p></li>
<li><p>The prior and posterior are both <strong>beta distributions!</strong></p></li>
</ul>
</div>
<p><strong>Interpretation of Beta Prior</strong></p>
<ul>
<li>Suppose we start with a beta prior with small parameters</li>
</ul>
<p><span class="math display">\[ \theta \sim Beta(0.001, 0.001) \]</span></p>
<ul>
<li>Observe x events in n trials, the posterior</li>
</ul>
<p><span class="math display">\[ \theta \mid x \sim Beta(0.001+x, 0.001+n-x) \approx Beta(x,n-x)\]</span></p>
<ul>
<li><p>Posterior mean of <span class="math inline">\(\theta \approx \frac{x}{n}\)</span>, the equivalent to the MLE based only on the data</p></li>
<li><p>Interpretation of the <span class="math inline">\(Beta(\alpha,\beta)\)</span> prior.</p>
<ul>
<li>Like having seen <span class="math inline">\(\alpha\)</span> events and <span class="math inline">\(\beta\)</span> non events in a sample size of <span class="math inline">\(\alpha + \beta\)</span></li>
<li>Strength of prior information equivalent to prior “sample size” <span class="math inline">\(\alpha + \beta\)</span></li>
<li>Prior mean = <span class="math inline">\(\frac{\alpha}{\alpha + \beta}\)</span></li>
</ul></li>
</ul>
<div class="workedexample">
<p>Consider Beta(3,7)and Beta(12,28) priors</p>
<ul>
<li>Gold line: prior belief that assumes approximately 3 events in 10 subjects</li>
<li>Blue line: prior belief that assumes approximately 12 events in 40(=12+28) subjects</li>
</ul>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
</div>
<div class="guidedexercise">
<p><strong>Plot Beta densities in R (Tutorial Practice)</strong> Try plotting Beta(2,8), Beta(8,2), and Beta(8,8). Example R code provided below.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="bayes.html#cb45-1" aria-hidden="true" tabindex="-1"></a>a<span class="ot">&lt;-</span><span class="dv">2</span></span>
<span id="cb45-2"><a href="bayes.html#cb45-2" aria-hidden="true" tabindex="-1"></a>b<span class="ot">&lt;-</span><span class="dv">2</span></span>
<span id="cb45-3"><a href="bayes.html#cb45-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">theta =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">101</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb45-4"><a href="bayes.html#cb45-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">priorDensity =</span> <span class="fu">dbeta</span>(theta, <span class="at">shape1 =</span> a,<span class="at">shape2 =</span> b))</span>
<span id="cb45-5"><a href="bayes.html#cb45-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-6"><a href="bayes.html#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(d, <span class="fu">aes</span>(theta, priorDensity))<span class="sc">+</span></span>
<span id="cb45-7"><a href="bayes.html#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()<span class="sc">+</span></span>
<span id="cb45-8"><a href="bayes.html#cb45-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(theta))<span class="sc">+</span></span>
<span id="cb45-9"><a href="bayes.html#cb45-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">p</span>(theta)))<span class="sc">+</span></span>
<span id="cb45-10"><a href="bayes.html#cb45-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="fu">paste0</span>(<span class="st">&quot;Beta distribution with &quot;</span>, <span class="st">&quot;a = &quot;</span>,a,<span class="st">&quot;; b = &quot;</span>,b))<span class="sc">+</span></span>
<span id="cb45-11"><a href="bayes.html#cb45-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
</div>
<p><strong>Summarizing Posterior Distribution</strong></p>
<ul>
<li>Since we know the form of the posterior distribution, we can easily calculate functions such as:
<ol style="list-style-type: decimal">
<li>Posterior mean <span class="math inline">\(E(\theta) = \frac{\alpha+x}{\alpha + \beta+n}\)</span></li>
<li>95% Credible intervals (we will talk more about them next week)</li>
<li><span class="math inline">\(P(\theta &lt; 0.2)\)</span>, <span class="math inline">\(P(\theta &lt; 0.5)\)</span>, <span class="math inline">\(P(0.4&lt; \theta &lt; 0.6)\)</span>, etc, which can be directly used to make probabilistic statement about the <span class="math inline">\(\theta\)</span>. E.g., the probability of the posterior adverse event rate &lt; 0.2 is about 0.95.</li>
</ol></li>
<li>Generate informative plots for assessing priors and posteriors. All this can easily be done using R.</li>
</ul>
<div class="guidedexercise">
<p><strong>Beta densities posterior summary in R (Tutorial Practice)</strong> Suppose we observe 1 adverse events among 10 patients and we assume the prior distribution of adverse event proportion <span class="math inline">\(\theta \sim Beta(1,1)\)</span>.</p>
<ul>
<li>What is the posterior Beta distribution of <span class="math inline">\(\theta\)</span>?</li>
<li>What is the posterior mean Beta distribution of <span class="math inline">\(\theta\)</span>?</li>
<li>What is the posterior probability <span class="math inline">\(P(\theta&lt;0.2)\)</span>?</li>
</ul>
</div>
<p><strong>Data overwhelming the prior</strong></p>
<ul>
<li>The posterior for the beta-binomial model after seeing x events in n trials is <span class="math inline">\(\theta \mid x ~ Beta(\alpha + x, \beta + n - x)\)</span> with posterior mean as</li>
</ul>
<p><span class="math display">\[E(\theta \mid x) = \frac{\alpha + x}{\alpha + \beta + n}\]</span></p>
<ul>
<li><p>In <span class="math inline">\(n \gg \alpha + \beta\)</span> and <span class="math inline">\(x \gg \alpha\)</span> (sample size and number of events is large), recall when using prior <span class="math inline">\(Beta(0.001, 0.001)\)</span>,
<span class="math display">\[ E(\theta \mid x) = \frac{\alpha + x}{\alpha + \beta + n} \approx \frac{x}{n}\]</span></p></li>
<li><p>Here, prior is of little importance!</p></li>
</ul>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-51-1.png" width="768" /></p>
<p><strong>A Beta(1,1) is Unif(0,1)</strong></p>
<ul>
<li>When <span class="math inline">\(\alpha = \beta = 1\)</span>, <span class="math inline">\(P(\theta) \propto \theta^{1-1}(1-\theta)^{1-1} = 1\)</span>, which is the probability density of a unif(0,1).</li>
</ul>
<p><strong>Posterior mean as a weighted average</strong></p>
<ul>
<li>The posterior mean is</li>
</ul>
<p><span class="math display">\[\begin{align}
E(\theta \mid x) &amp; = \frac{\alpha + x}{\alpha + \beta + n} \\
&amp; = \frac{\alpha }{\alpha + \beta + n} + \frac{x }{\alpha + \beta + n}\\
&amp; = \Big( \frac{\alpha+ \beta }{\alpha + \beta + n} \Big) \times \Big( \frac{\alpha }{\alpha + \beta} \Big) + \Big( \frac{n }{\alpha + \beta + n} \Big) \times \Big( \frac{x }{n} \Big) \\
&amp; = w \times \Big( \frac{\alpha }{\alpha + \beta} \Big) + (1-w) \times \Big( \frac{x }{n} \Big) \\
&amp; = w \times (\text{prior mean}) + (1-w) \times (\text{sample estimate})
\end{align}\]</span></p>
<ul>
<li><p>Where <span class="math inline">\(w\)</span> is the ratio of the “prior sample size” to the “total sample size” <span class="math display">\[ w = \frac{\alpha+ \beta }{\alpha + \beta + n}.\]</span></p></li>
<li><p>This is a common theme in Bayesian models with actual prior information</p></li>
<li><p>The posterior distribution will lie between the prior and likelihood</p></li>
<li><p>The posterior mean is a weighted average of the prior mean and data-based estimate</p></li>
<li><p><strong>As the sample size increases, the contribution of the prior diminishes</strong></p></li>
</ul>
<div class="guidedexercise">
<p><strong>Revisit the adverse event example, now using the beta-binomial model</strong></p>
<ul>
<li>Suppose you have observed 0 adverse events in 6 patients in a phase I clinical trial
on a given dose of a drug</li>
<li>Assume a prior: <span class="math inline">\(\theta \sim Beta(1,1)\)</span> ; What is the posterior distribution?</li>
<li>How much evidence that the AE rate is &lt; 20%? Find <span class="math inline">\(P(\theta &lt; 0.2)\)</span> using the posterior distribution?</li>
<li>What is the AE rate at a lower dose was 1/6? Can we use this external evidence?
<ul>
<li>New prior for the lower dose, <span class="math inline">\(\theta \sim Beta(1,5)\)</span></li>
<li>New Posterior is given 0 adverse events in 6 patients: <span class="math inline">\(\theta \sim Beta(1,11)\)</span></li>
<li>Find <span class="math inline">\(P(\theta &lt; 0.2)\)</span>.</li>
</ul></li>
</ul>
</div>
</div>
<div id="the-normal-normal-model" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> The Normal-normal model<a href="bayes.html#the-normal-normal-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Parameters with normal priors and likelihoods</strong></p>
<ul>
<li><p>Continuing with a single parameter model</p></li>
<li><p>The likelihood of the parameter given some data is that of a normal distribution with known variance <span class="math inline">\(\sigma^2\)</span></p></li>
</ul>
<p><span class="math display">\[ y \mid \theta \sim N(\theta, \sigma^2) \]</span></p>
<ul>
<li>suppose the prior for <span class="math inline">\(\theta\)</span>, the mean, follows a normal distribution</li>
</ul>
<p><span class="math display">\[ \theta \sim N(\mu_0, V_0)\]</span></p>
<ul>
<li>We want to make inference about <span class="math inline">\(\theta\)</span> given the data y, what is the posterior distribution <span class="math inline">\(P(\theta \mid y)\)</span>?</li>
</ul>
<div class="important">
<p><strong>Normal-normal model</strong></p>
<p><strong>1. The normal model (likelihood) of one data point</strong></p>
<ul>
<li><p>Consider a single observation <span class="math inline">\(y\)</span> from a normal distribution parameterized by a mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, we assume that <span class="math inline">\(\sigma^2\)</span> is known.</p></li>
<li><p>The normal likelihood for <span class="math inline">\(y\)</span> given <span class="math inline">\(\theta\)</span> is
<span class="math display">\[ P(y\mid \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y-\theta)^2}{2\sigma^2}}, y \in (-\infty, \infty)\]</span></p></li>
<li><p>The <span class="math inline">\(E(y) = \theta\)</span> and <span class="math inline">\(V(y) = \sigma^2\)</span> and <span class="math inline">\(SD(y) = \sigma\)</span>.</p></li>
<li><p>Given <span class="math inline">\(y \mid \theta \sim N(\theta,\sigma^2 )\)</span>, <span class="math inline">\(z = \frac{y-\theta}{\sigma} \sim N(0,1)\)</span>. Thus, if we rescale <span class="math inline">\(y\)</span> by <span class="math inline">\(\sigma\)</span>, we have</p>
<ul>
<li>95%CI of y: <span class="math inline">\(\theta \pm 1.96 \times \sigma\)</span></li>
</ul></li>
</ul>
<p><strong>2. Prior</strong></p>
<p>We can often use a normal prior to represent <span class="math inline">\(\theta \sim N(\mu_0, V_0)\)</span></p>
<ul>
<li><p><span class="math inline">\(\mu_0\)</span> is the prior mean</p></li>
<li><p><span class="math inline">\(V_0\)</span> is the prior variance, which expresses the uncertainty around the mean <span class="math inline">\(\mu_0\)</span></p></li>
<li><p><span class="math inline">\(\tau_0 = \frac{1}{V_0}\)</span> is called precision, it can also be used to express the uncertainty around the mean, e.g. <span class="math inline">\(\theta \sim N(\mu_0, \tau_0)\)</span></p></li>
<li><p>High precision = low variance, in other words, the distribution of the normal random variable <span class="math inline">\(\theta\)</span> is closely centered around its mean.</p></li>
</ul>
<p><strong>3. Posterior</strong></p>
<ul>
<li>With a normal prior and a normal likelihood of the data given parameter <span class="math inline">\(\theta\)</span>, (here we use precision to express the normal distributions)</li>
</ul>
<p><span class="math display">\[ \theta \sim N(\mu_0,\tau_0 ) \text{ and } y \mid \theta \sim N(\theta,\tau_y )\]</span></p>
<ul>
<li><p>where <span class="math inline">\(\tau_y = \frac{1}{\sigma^2}\)</span> is the precision of the observed data</p></li>
<li><p>It can be proven mathematically using the Bayes’ rule that the posterior distribution given a normal prior and normal likelihood is also normal,</p></li>
</ul>
<p><span class="math display">\[ \theta \mid y \sim  N(\mu_1, \tau_1)\]</span></p>
<ul>
<li>The posterior mean <span class="math inline">\(\mu_1\)</span> is</li>
</ul>
<p><span class="math display">\[ \frac{\tau_0 \times \mu_0 + \tau_y \times y}{\tau_0+ \tau_y} \]</span></p>
<ul>
<li>The posterior precision is</li>
</ul>
<p><span class="math display">\[ \tau_1 = \tau_0 + \tau_y\]</span></p>
<ul>
<li>Again, <strong>the prior and posterior are both normal distributions!</strong></li>
</ul>
</div>
<p><strong>Posterior mean as a weighted average of the prior mean and data-based estimate</strong></p>
<ul>
<li>Using the precision parameters</li>
</ul>
<p><span class="math display">\[ \tau_y = \frac{1}{\sigma^2_y} \text{ and } \tau_0 = \frac{1}{V_0}\]</span></p>
<ul>
<li>The posterior distribution of <span class="math inline">\(\theta\)</span> is then</li>
</ul>
<p><span class="math display">\[\theta \mid y \sim N(\mu_1, \tau_1)\]</span></p>
<ul>
<li>where the mean and precision are</li>
</ul>
<p><span class="math display">\[\begin{align}
&amp; \mu_1 = \Big(\frac{\tau_0}{\tau_0 + \tau_y}\Big) \times \mu_0 + \Big(\frac{\tau_y}{\tau_0 + \tau_y}\Big) \times y \\
&amp; \tau_1 = \tau_0 + \tau_y
\end{align}\]</span></p>
<ul>
<li>The posterior mean is a weighted sum of the prior mean and the observed value.</li>
<li>The weights are the relative precisions of the prior and the likelihood
<span class="math display">\[\mu_1 = w_0 \times \mu_0 + w_y \times y\]</span>
<span class="math display">\[w_0 = \frac{\tau_0}{\tau_0 + \tau_y}\]</span>
and</li>
</ul>
<p><span class="math display">\[w_y = 1-w_0 = \frac{\tau_y}{\tau_0 + \tau_y}\]</span></p>
<p><strong>Data overwhelming the prior and vice versa</strong></p>
<ul>
<li>With relatively low prior precision (imprecise prior and substantial data), i.e., <span class="math inline">\(\tau_0 \ll \tau_y\)</span> and <span class="math inline">\(0 \approx w_0 \ll w_y \approx 1\)</span>,</li>
</ul>
<p><span class="math display">\[ \mu_1 = w_0 \times \mu_0 + w_y \times y \approx y\]</span></p>
<ul>
<li>With relative little data and aprior that is not too diffuse, i.e., <span class="math inline">\(\tau_y \ll \tau_0\)</span> and <span class="math inline">\(0 \approx w_y \ll w_0 \approx 1\)</span>,</li>
</ul>
<p><span class="math display">\[ \mu_1 = w_0 \times \mu_0 + w_y \times y \approx \mu_0\]</span></p>
<ul>
<li>In most cases, the posterior mean is a compromise between what we believed before and what we observed in the data.</li>
</ul>
<p><strong>Normal model with multiple observations</strong></p>
<ul>
<li><p>Consider a set of <span class="math inline">\(n\)</span> observations <span class="math inline">\(y = (y_1, \ldots, y_n)\)</span> sample from a <span class="math inline">\(N(\theta, \sigma^2)\)</span>, where <span class="math inline">\(y_1,\ldots, y_n\)</span> are identically, independently distributed and <span class="math inline">\(y_i \sim N(\theta, \sigma^2)\)</span>, <span class="math inline">\(i = 1, \ldots, n\)</span>.</p></li>
<li><p>The normal likelihood of the joint distribution <span class="math inline">\(y = (y_1, \ldots, y_n)\)</span> is</p></li>
</ul>
<p><span class="math display">\[\begin{align}
P((y_1, \ldots, y_n \mid \theta) &amp;= \prod_{i=1}^n P(y_i \mid \theta) \\
&amp; = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_i-\theta)^2}{2\sigma^2}}, y \in (-\infty, \infty)
\end{align}\]</span></p>
<ul>
<li>Given prior <span class="math inline">\(\theta \sim N(\mu_0, \tau_0)\)</span>, the posterior of <span class="math inline">\(\theta \mid y\)</span> is</li>
</ul>
<p><span class="math display">\[P(\theta \mid y_1, \ldots, y_n) = N(\mu_n, \tau_n)\]</span></p>
<ul>
<li>where the posterior mean <span class="math inline">\(\mu_n\)</span> is</li>
</ul>
<p><span class="math display">\[\mu_n = \frac{\tau_0 \times \mu_0 + n\tau_y \times \bar{y}}{\tau_0+ n\tau_y} = \Big( \frac{\tau_0}{\tau_0+ n\tau_y} \Big) \times \mu_0 + \Big( \frac{n\tau_y}{\tau_0+ n\tau_y} \Big) \times \bar{y}\]</span></p>
<ul>
<li>and the posterior precision is</li>
</ul>
<p><span class="math display">\[ \tau_n = \tau_0 + n\tau_y\]</span></p>
<ul>
<li><p><span class="math inline">\(\bar{y} = \frac{\sum_{i=1}^2y_i}{n}\)</span> is the sample mean and <span class="math inline">\(\tau_y = \frac{1}{\sigma^2}\)</span></p></li>
<li><p>Again, the posterior mean is a weighted sum between prior and data. As sample size <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(\Big( \frac{\tau_0}{\tau_0+ n\tau_y} \Big) \rightarrow 0\)</span> and <span class="math inline">\(\Big( \frac{n\tau_y}{\tau_0+ n\tau_y} \Big) \rightarrow 1\)</span>, <strong>data will overwhelm the prior and dominate the posterior distribution</strong>.</p></li>
</ul>
<p><strong>The use of normal-normal model in clinical applications</strong></p>
<ul>
<li><p>Consider estimating a parameter <span class="math inline">\(\theta\)</span>, e.g. mean, log-hazard ratio, log-odds-ratio, log-relative-risk</p></li>
<li><p>Suppose we have an estimate of <span class="math inline">\(\theta\)</span>; We will call it <span class="math inline">\(y\)</span> and let <span class="math inline">\(\hat{\sigma}^2_y\)</span> be the estimated variance</p></li>
<li><p>This estimate <span class="math inline">\(y\)</span> can be viewed as a single data observation of <span class="math inline">\(\theta\)</span>!</p></li>
<li><p>In large samples, it is approximately true that <span class="math inline">\(y \sim N(\theta, \sigma^2_y)\)</span>, where <span class="math inline">\(\theta\)</span> is the true value and <span class="math inline">\(\sigma^2_y\)</span> is the true variance (sample size dependent) and the 95%CI of <span class="math inline">\(y\)</span> is <span class="math inline">\(\theta \pm 1.96 \times \sigma\)</span>.</p></li>
<li><p>We can construct a normal likelihood of data for the parameter <span class="math inline">\(\theta\)</span> as
<span class="math display">\[y \mid N(\theta, \hat{\sigma}^2_y)\]</span></p></li>
</ul>
<div class="workedexample">
<p><strong>Treatment effect estimate on the mean</strong></p>
<ul>
<li><p>For this example, we will simulate data for a small study of n=100 subjects and randomized 50% to treat=0 and 50% to treat=1 (mimicking an RCT). We will then generate the outcome Y from a normal distribution depending on the treatment indicator.</p></li>
<li><p>Suppose we have the following table of results from a linear regression estimating treatment effect <span class="math inline">\(\theta\)</span> on the outcome</p></li>
</ul>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="bayes.html#cb46-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb46-2"><a href="bayes.html#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb46-3"><a href="bayes.html#cb46-3" aria-hidden="true" tabindex="-1"></a>treat <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span>(<span class="fu">runif</span>(n, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)<span class="sc">&lt;</span><span class="fl">0.5</span>)</span>
<span id="cb46-4"><a href="bayes.html#cb46-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">5</span><span class="sc">*</span>treat<span class="sc">+</span><span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span><span class="dv">10</span>)</span>
<span id="cb46-5"><a href="bayes.html#cb46-5" aria-hidden="true" tabindex="-1"></a>fit<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>treat)</span>
<span id="cb46-6"><a href="bayes.html#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ treat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.8662  -6.7095  -0.5731   5.7503  24.1866 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)    1.303      1.423   0.916   0.3618  
## treat          4.080      1.918   2.127   0.0359 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.543 on 98 degrees of freedom
## Multiple R-squared:  0.04414,    Adjusted R-squared:  0.03438 
## F-statistic: 4.525 on 1 and 98 DF,  p-value: 0.03591</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="bayes.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">confint.default</span>(fit, <span class="st">&#39;treat&#39;</span>, <span class="at">level=</span><span class="fl">0.95</span>),<span class="dv">3</span>) <span class="co"># based on asymptotic normality</span></span></code></pre></div>
<pre><code>##       2.5 % 97.5 %
## treat 0.321   7.84</code></pre>
<ul>
<li><p>We are interested in using this published results to make Bayesian inference on <span class="math inline">\(\theta\)</span></p></li>
<li><p>Our observed data (estimate) of <span class="math inline">\(\theta\)</span>, denoted as <span class="math inline">\(y\)</span> is 4.08 with standard error 1.918. Here, <span class="math inline">\(y\)</span> plays the role of data! We call <span class="math inline">\(y\)</span> the observed datum.</p></li>
<li><p>Relying on the large sample approximation, we assume that the estimate y arose from a normal distribution with true mean <span class="math inline">\(\theta\)</span>, <strong>We treat the observed standard error <span class="math inline">\(\hat{\sigma}_y = 1.918\)</span> as the true standard deviation of <span class="math inline">\(y\)</span></strong></p>
<ul>
<li>Recall, the standard deviation of the sampling distribution of a parameter estimate is called its standard error</li>
</ul></li>
<li><p>This gives us the likelihood
<span class="math display">\[y \mid \theta \sim N(\theta, 1.918^2)\]</span></p></li>
<li><p>The width of the 95% CI is approximately equal to <span class="math inline">\(2 \times 1.96 \times \sigma_y\)</span></p></li>
<li><p>Suppose we are not provided with <span class="math inline">\(\sigma_y\)</span> from the table of results, but know the 95% CI of y is (1.568,0.064), we can calculate <span class="math inline">\(\sigma_y\)</span> with</p></li>
</ul>
<p><span class="math display">\[\sigma_y = \frac{7.84- 0.321}{2\times 1.96}=1.918\]</span></p>
<ul>
<li>The normal likelihood is this example is <span class="math inline">\(y \sim N(4.08, 1.918^2)\)</span>, if we assume a prior <span class="math inline">\(\theta \sim N(0, 2 \times 1.918^2)\)</span>, we have a posterior</li>
</ul>
<p><span class="math display">\[\begin{align}
&amp; \theta \mid y \sim N(\mu_1, \tau_1) \\
\mu_1  &amp;= \frac{ \frac{1}{prior.sd^2} }{\frac{1}{prior.sd^2} + \frac{1}{se.obs^2} } \times prior.mean + \frac{\frac{1}{se.obs^2}}{\frac{1}{prior.sd^2} + \frac{1}{se.obs^2}} \times y.obs \\
 &amp; = \frac{ \frac{1}{2 \times 1.918^2} }{\frac{1}{2 \times 1.918^2} + \frac{1}{1.918^2} } \times 0 + \frac{\frac{1}{1.918^2}}{\frac{1}{2 \times 1.918^2} + \frac{1}{1.918^2}} \times 4.08 = 2.72 \\
 
 \tau_1 &amp; = \tau_0 + \tau_y \\
        &amp; = \frac{1}{prior.sd^2} + \frac{1}{se.obs^2} \\
        &amp; = \frac{1}{2 \times 1.918^2} + \frac{1}{1.918^2} = 0.408
\end{align}\]</span></p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="bayes.html#cb50-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">theta =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">10</span>, <span class="at">to =</span> <span class="dv">15</span>, <span class="at">length.out =</span> <span class="dv">200</span>),</span>
<span id="cb50-2"><a href="bayes.html#cb50-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">y.obs=</span><span class="fl">4.08</span>,</span>
<span id="cb50-3"><a href="bayes.html#cb50-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">se.obs=</span><span class="fl">1.918</span>,</span>
<span id="cb50-4"><a href="bayes.html#cb50-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">prior.mean=</span><span class="dv">0</span>,</span>
<span id="cb50-5"><a href="bayes.html#cb50-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">prior.sd=</span><span class="fu">sqrt</span>(<span class="dv">2</span>)<span class="sc">*</span><span class="fl">1.918</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb50-6"><a href="bayes.html#cb50-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">priorDensity =</span> <span class="fu">dnorm</span>(theta, prior.mean, prior.sd)) <span class="sc">%&gt;%</span></span>
<span id="cb50-7"><a href="bayes.html#cb50-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">likelihood.0 =</span> <span class="fu">dnorm</span>(theta, y.obs, se.obs)) <span class="sc">%&gt;%</span></span>
<span id="cb50-8"><a href="bayes.html#cb50-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">w =</span> (<span class="dv">1</span><span class="sc">/</span>se.obs<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">/</span>se.obs<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>prior.sd<span class="sc">^</span><span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb50-9"><a href="bayes.html#cb50-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">post.prec =</span> <span class="dv">1</span><span class="sc">/</span>se.obs<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>prior.sd<span class="sc">^</span><span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb50-10"><a href="bayes.html#cb50-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">posteriorDensity =</span> <span class="fu">dnorm</span>(theta, w<span class="sc">*</span>y.obs<span class="sc">+</span>(<span class="dv">1</span><span class="sc">-</span>w)<span class="sc">*</span>prior.mean, <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>post.prec))) <span class="sc">%&gt;%</span></span>
<span id="cb50-11"><a href="bayes.html#cb50-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">likelihood =</span> likelihood<span class="fl">.0</span><span class="sc">/</span><span class="fu">sum</span>(<span class="fu">diff</span>(theta)<span class="sc">*</span>likelihood<span class="fl">.0</span>[<span class="sc">-</span><span class="dv">1</span>])) <span class="sc">%&gt;%</span></span>
<span id="cb50-12"><a href="bayes.html#cb50-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols=</span><span class="fu">c</span>(priorDensity,likelihood,posteriorDensity),<span class="at">names_to=</span><span class="st">&quot;Function&quot;</span>,<span class="at">values_to=</span><span class="st">&quot;p&quot;</span>)</span>
<span id="cb50-13"><a href="bayes.html#cb50-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-14"><a href="bayes.html#cb50-14" aria-hidden="true" tabindex="-1"></a>d<span class="sc">$</span>Function <span class="ot">&lt;-</span> <span class="fu">factor</span>(d<span class="sc">$</span>Function, </span>
<span id="cb50-15"><a href="bayes.html#cb50-15" aria-hidden="true" tabindex="-1"></a>                     <span class="at">levels=</span><span class="fu">c</span>(<span class="st">&quot;priorDensity&quot;</span>,<span class="st">&quot;likelihood&quot;</span>,<span class="st">&quot;posteriorDensity&quot;</span>),</span>
<span id="cb50-16"><a href="bayes.html#cb50-16" aria-hidden="true" tabindex="-1"></a>                     <span class="at">labels=</span><span class="fu">c</span>(<span class="st">&quot;Prior&quot;</span>,<span class="st">&quot;Likelihood&quot;</span>,<span class="st">&quot;Posterior&quot;</span>))</span>
<span id="cb50-17"><a href="bayes.html#cb50-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-18"><a href="bayes.html#cb50-18" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="fu">ggplot</span>(d, <span class="fu">aes</span>(theta, p,<span class="at">colour=</span>Function))<span class="sc">+</span></span>
<span id="cb50-19"><a href="bayes.html#cb50-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size=</span><span class="dv">1</span>)<span class="sc">+</span></span>
<span id="cb50-20"><a href="bayes.html#cb50-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(theta))<span class="sc">+</span></span>
<span id="cb50-21"><a href="bayes.html#cb50-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">p</span>(theta)))<span class="sc">+</span></span>
<span id="cb50-22"><a href="bayes.html#cb50-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>)<span class="sc">+</span></span>
<span id="cb50-23"><a href="bayes.html#cb50-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">&quot;goldenrod&quot;</span>,<span class="st">&quot;steelblue&quot;</span>,<span class="st">&quot;black&quot;</span>)) <span class="sc">+</span><span class="fu">theme_bw</span>()</span>
<span id="cb50-24"><a href="bayes.html#cb50-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-25"><a href="bayes.html#cb50-25" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
</div>
<div class="workedexample">
<p><strong>Hazard Ratio on time-to-event from Cox proportional hazard model</strong></p>
<blockquote>
<p><a href="https://rss.org.uk/news-publication/news-publications/2022/general-news/sir-david-cox-1924-2022/">In memory of Sir David Cox, 1925-2022</a></p>
</blockquote>
<ul>
<li><p>Effect of a Resuscitation Strategy Targeting Peripheral Perfusion Status vs Serum Lactate Levels on 28-Day Mortality Among Patients With Septic Shock
The ANDROMEDA-SHOCK Randomized Clinical Trial <span class="citation">(<a href="#ref-hernandez2019effect" role="doc-biblioref">Hernández et al. 2019</a>)</span></p></li>
<li><p>Open-access link: <a href="https://jamanetwork.com/journals/jama/fullarticle/2724361" class="uri">https://jamanetwork.com/journals/jama/fullarticle/2724361</a></p></li>
<li><p>We are interested to make inference on the hazard ratio of peripheral perfusion versus Lactate level on 28-Day mortality among patients with septic shock.</p></li>
<li><p>We obtain a point estimate of HR and it’s 95%CI from Table 2 of <span class="citation">(<a href="#ref-hernandez2019effect" role="doc-biblioref">Hernández et al. 2019</a>)</span>.</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-54"></span>
<img src="images/ANDROMEDA-SHOCK_table2.png" alt="HR estimates of the effectiveness of peripheral perfusion on primary outcome - Death within 28 days" width="982" />
<p class="caption">
Figure 3.4: HR estimates of the effectiveness of peripheral perfusion on primary outcome - Death within 28 days
</p>
</div>
<p><strong>1. The normal likelihood on log(HR)</strong></p>
<ul>
<li><p>The estimated adjusted (HR) on 28-day mortality, 0.75 (0.55 to 1.02), is called the <strong>observed datum</strong> (viewed as a single data observation on <span class="math inline">\(\theta\)</span>, denoted as <span class="math inline">\(y\)</span>)</p></li>
<li><p>It’s known that <strong>log of the parameter estimate, such as HR, RR and OR, follow a normal distribution around its true log value</strong>. Thus, We need to use the reported output information in order to specify the likelihood
for on the log scale.</p></li>
<li><p>Let <span class="math inline">\(\theta\)</span> denote the random variable representing log(HR)</p></li>
<li><p>Let <span class="math inline">\(y\)</span> denote an observed data point of <span class="math inline">\(\theta\)</span>, we have <span class="math inline">\(y \sim N(\theta, \sigma_y^2)\)</span>, where <span class="math inline">\(\theta\)</span> is log(HR) and <span class="math inline">\(sigma_y\)</span> is the standard error of the log(HR)</p></li>
<li><p>In order to get <span class="math inline">\(\sigma_y^2\)</span> from the result table, we will use the reported 95% CI of the <strong>log of HR</strong>!</p></li>
<li><p>The limits of the CI for the log value are obtained by taking the logs of the reported CI, i.e., 95% CI on the log(HR) in this example is [log(0.55)=-0.598, log(1.02)=0.02].</p></li>
<li><p>Take the width of the 95% CI and divided by <span class="math inline">\(2\times 1.96\)</span>, we have</p></li>
</ul>
<p><span class="math display">\[\hat{\sigma}_y = \frac{log(1.02) - log(0.55)}{2\times 1.96} = 0.158\]</span></p>
<ul>
<li>We use <span class="math inline">\(\hat{\sigma}_y\)</span> to approximate <span class="math inline">\(\sigma_y\)</span>, therefore, the data likelihood <span class="math inline">\(y \mid \theta \sim (\theta,0.158^2)\)</span> where we have observed <span class="math inline">\(y=log(0.75)=-0.287\)</span></li>
</ul>
<p><strong>2. The prior for log(HR)</strong></p>
<p>Suppose we consider a neutral prior for log(HR), such that 95% of the prior for probability for the HR is in the range 0.8 to 1.25 and the median of HR is 1 (indicating the intervention is neither beneficial or harmful)</p>
<ul>
<li><p>The prior of <span class="math inline">\(log(HR) \sim N(log(1), \sigma_0^2)\)</span>, where
<span class="math display">\[ \sigma_0 = \frac{\log(1.25) - \log(0.8)}{2 \times 1.96} = 0.11\]</span></p></li>
<li><p>We can express the prior normal distribution on log(HR) as
<span class="math display">\[\theta \sim N(\mu_0 = 0, \sigma_0^2=0.11^2)\]</span></p></li>
<li><p><strong>We specify normal prior for log(HR)!</strong>. In probability theory, a lognormal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. In this case, HR follows a lognormal distribution!</p></li>
</ul>
<p><strong>3. The posterior of log(HR)</strong></p>
<ul>
<li><p>Given prior <span class="math inline">\(\theta \sim N(0, 0.11^2)\)</span> and likelihood <span class="math inline">\(y \mid \theta \sim (\theta,0.158^2)\)</span> where <span class="math inline">\(y=log(0.75)=-0.287\)</span></p></li>
<li><p>Prior precision is <span class="math inline">\(\frac{1}{0.11^2} = 82.6\)</span> and the likelihood precision is <span class="math inline">\(\frac{1}{0.158^2} = 40.1\)</span>, we can see that the prior has more information than the likelihood!</p></li>
<li><p>The posterior precision on log(HR) is
<span class="math inline">\(\tau_1 = 82.6 + 40.1=122.7\)</span></p></li>
<li><p>The posterior mean is</p></li>
</ul>
<p><span class="math display">\[ \frac{82.6}{82.6 + 40.1} \times log(1) + \frac{40.1}{82.6 + 40.1} \times log(0.75) = -0.094\]</span></p>
<ul>
<li>The posterior variance of <span class="math inline">\(\log(HR) = \frac{1}{122.7}\)</span> and the posterior standard deviation is <span class="math inline">\(\sqrt{\frac{1}{122.7}} = 0.09\)</span>, thus</li>
</ul>
<p><span class="math display">\[\theta \mid y \sim N(\mu_1 = -0.094, \sigma_1^2 = 0.09^2\]</span></p>
<ul>
<li>The posterior median HR = exp(-0.094) = 0.91 and the posterior 95% Credible Interval for HR is
<span class="math display">\[\exp(-0.094 \pm 1.96\times 0.09) \rightarrow (0.763, 1.086) \]</span></li>
</ul>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="bayes.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co">#prior on log(HR);</span></span>
<span id="cb51-2"><a href="bayes.html#cb51-2" aria-hidden="true" tabindex="-1"></a>mu0<span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb51-3"><a href="bayes.html#cb51-3" aria-hidden="true" tabindex="-1"></a>sd0<span class="ot">&lt;-</span> <span class="fl">0.11</span></span>
<span id="cb51-4"><a href="bayes.html#cb51-4" aria-hidden="true" tabindex="-1"></a>prec0 <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>sd0<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb51-5"><a href="bayes.html#cb51-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-6"><a href="bayes.html#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="co">#likelihood on log(HR);</span></span>
<span id="cb51-7"><a href="bayes.html#cb51-7" aria-hidden="true" tabindex="-1"></a>ybar<span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.287</span></span>
<span id="cb51-8"><a href="bayes.html#cb51-8" aria-hidden="true" tabindex="-1"></a>sd.ybar<span class="ot">&lt;-</span> <span class="fl">0.158</span> </span>
<span id="cb51-9"><a href="bayes.html#cb51-9" aria-hidden="true" tabindex="-1"></a>prec.y <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>sd.ybar<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb51-10"><a href="bayes.html#cb51-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-11"><a href="bayes.html#cb51-11" aria-hidden="true" tabindex="-1"></a><span class="co">#posterior on log(HR)</span></span>
<span id="cb51-12"><a href="bayes.html#cb51-12" aria-hidden="true" tabindex="-1"></a>prec1 <span class="ot">&lt;-</span> prec0 <span class="sc">+</span> prec.y</span>
<span id="cb51-13"><a href="bayes.html#cb51-13" aria-hidden="true" tabindex="-1"></a>mu1 <span class="ot">&lt;-</span> (prec0 <span class="sc">*</span> mu0 <span class="sc">+</span> prec.y<span class="sc">*</span>ybar)<span class="sc">/</span>prec1</span>
<span id="cb51-14"><a href="bayes.html#cb51-14" aria-hidden="true" tabindex="-1"></a>sigma1 <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(prec1)</span>
<span id="cb51-15"><a href="bayes.html#cb51-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-16"><a href="bayes.html#cb51-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-17"><a href="bayes.html#cb51-17" aria-hidden="true" tabindex="-1"></a>mu.plot <span class="ot">&lt;-</span> <span class="fu">seq</span>(ybar<span class="dv">-1</span>,ybar<span class="sc">+</span><span class="dv">1</span>,</span>
<span id="cb51-18"><a href="bayes.html#cb51-18" aria-hidden="true" tabindex="-1"></a>               <span class="at">length.out=</span><span class="dv">200</span>)</span>
<span id="cb51-19"><a href="bayes.html#cb51-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-20"><a href="bayes.html#cb51-20" aria-hidden="true" tabindex="-1"></a>lik <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(ybar, mu.plot, sd.ybar)</span>
<span id="cb51-21"><a href="bayes.html#cb51-21" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu.plot, mu0, sd0)</span>
<span id="cb51-22"><a href="bayes.html#cb51-22" aria-hidden="true" tabindex="-1"></a>posterior<span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu.plot, mu1,sigma1)</span>
<span id="cb51-23"><a href="bayes.html#cb51-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-24"><a href="bayes.html#cb51-24" aria-hidden="true" tabindex="-1"></a>d<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">mu.plot=</span><span class="fu">rep</span>(mu.plot, <span class="dv">3</span>),</span>
<span id="cb51-25"><a href="bayes.html#cb51-25" aria-hidden="true" tabindex="-1"></a>              <span class="at">p =</span><span class="fu">c</span>(lik,prior,posterior),</span>
<span id="cb51-26"><a href="bayes.html#cb51-26" aria-hidden="true" tabindex="-1"></a>              <span class="at">Function=</span><span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;likelihood&quot;</span>,<span class="st">&quot;prior&quot;</span>,<span class="st">&quot;posterior&quot;</span>),<span class="at">each=</span><span class="fu">length</span>(mu.plot)))</span>
<span id="cb51-27"><a href="bayes.html#cb51-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-28"><a href="bayes.html#cb51-28" aria-hidden="true" tabindex="-1"></a>p1<span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d, <span class="fu">aes</span>(mu.plot, p,<span class="at">colour=</span>Function))<span class="sc">+</span></span>
<span id="cb51-29"><a href="bayes.html#cb51-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>)<span class="sc">+</span></span>
<span id="cb51-30"><a href="bayes.html#cb51-30" aria-hidden="true" tabindex="-1"></a>     <span class="fu">xlab</span>(<span class="st">&quot;log(HR)&quot;</span>)<span class="sc">+</span></span>
<span id="cb51-31"><a href="bayes.html#cb51-31" aria-hidden="true" tabindex="-1"></a>     <span class="fu">ylab</span>(<span class="st">&quot;Probability Density&quot;</span>)<span class="sc">+</span></span>
<span id="cb51-32"><a href="bayes.html#cb51-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_colour_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">&quot;goldenrod&quot;</span>,<span class="st">&quot;steelblue&quot;</span>,<span class="st">&quot;black&quot;</span>))<span class="sc">+</span></span>
<span id="cb51-33"><a href="bayes.html#cb51-33" aria-hidden="true" tabindex="-1"></a>     <span class="fu">theme_bw</span>()<span class="sc">+</span></span>
<span id="cb51-34"><a href="bayes.html#cb51-34" aria-hidden="true" tabindex="-1"></a>     <span class="fu">ggtitle</span>(<span class="st">&quot;Neural prior on log(HR): plot on log(HR) scale&quot;</span>)</span>
<span id="cb51-35"><a href="bayes.html#cb51-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-36"><a href="bayes.html#cb51-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-37"><a href="bayes.html#cb51-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-38"><a href="bayes.html#cb51-38" aria-hidden="true" tabindex="-1"></a>p2<span class="ot">&lt;-</span> <span class="fu">ggplot</span>(d, <span class="fu">aes</span>(<span class="fu">exp</span>(mu.plot), p,<span class="at">colour=</span>Function))<span class="sc">+</span></span>
<span id="cb51-39"><a href="bayes.html#cb51-39" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>)<span class="sc">+</span></span>
<span id="cb51-40"><a href="bayes.html#cb51-40" aria-hidden="true" tabindex="-1"></a>     <span class="fu">xlab</span>(<span class="st">&quot;HR&quot;</span>)<span class="sc">+</span></span>
<span id="cb51-41"><a href="bayes.html#cb51-41" aria-hidden="true" tabindex="-1"></a>     <span class="fu">ylab</span>(<span class="st">&quot;Probability Density&quot;</span>)<span class="sc">+</span></span>
<span id="cb51-42"><a href="bayes.html#cb51-42" aria-hidden="true" tabindex="-1"></a>    <span class="fu">scale_colour_manual</span>(<span class="at">values=</span><span class="fu">c</span>(<span class="st">&quot;goldenrod&quot;</span>,<span class="st">&quot;steelblue&quot;</span>,<span class="st">&quot;black&quot;</span>))<span class="sc">+</span></span>
<span id="cb51-43"><a href="bayes.html#cb51-43" aria-hidden="true" tabindex="-1"></a>     <span class="fu">theme_bw</span>()<span class="sc">+</span></span>
<span id="cb51-44"><a href="bayes.html#cb51-44" aria-hidden="true" tabindex="-1"></a>     <span class="fu">ggtitle</span>(<span class="st">&quot;Neural prior on log(HR): plot on HR scale&quot;</span>)</span>
<span id="cb51-45"><a href="bayes.html#cb51-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-46"><a href="bayes.html#cb51-46" aria-hidden="true" tabindex="-1"></a><span class="fu">ggarrange</span>(p1,p2, <span class="at">nrow=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p><strong>Posterior summary statistics from R</strong></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="bayes.html#cb52-1" aria-hidden="true" tabindex="-1"></a>thresholds<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.8</span>)</span>
<span id="cb52-2"><a href="bayes.html#cb52-2" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span><span class="fu">exp</span>( <span class="fu">qnorm</span>(<span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.025</span>, <span class="fl">0.975</span>), mu1, sigma1))</span>
<span id="cb52-3"><a href="bayes.html#cb52-3" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> <span class="fu">c</span>(res, <span class="fu">pnorm</span>(<span class="fu">log</span>(thresholds), mu1, sigma1))</span>
<span id="cb52-4"><a href="bayes.html#cb52-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">names</span>(res) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;median&quot;</span>,<span class="st">&quot;q2.5&quot;</span>,<span class="st">&quot;q97.5&quot;</span>,<span class="fu">paste0</span>(<span class="st">&quot;Pr(OR &lt; &quot;</span>,thresholds,<span class="st">&quot;)&quot;</span>))</span>
<span id="cb52-5"><a href="bayes.html#cb52-5" aria-hidden="true" tabindex="-1"></a>  res</span></code></pre></div>
<pre><code>##       median         q2.5        q97.5   Pr(OR &lt; 1) Pr(OR &lt; 0.8) 
##    0.9105607    0.7628965    1.0868065    0.8503338    0.0757977</code></pre>
</div>
</div>
</div>
<div id="summary-of-conjugate-priors-models" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Summary of Conjugate priors &amp; models<a href="bayes.html#summary-of-conjugate-priors-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>There are a small number of prior-likelihood pairs where the prior and the posterior are in the same family (e.g., both beta, both normal): these are called conjugate models</p></li>
<li><p>These posterior distributions can be computed without specialized software</p></li>
<li><p>These examples are useful for illustrating how Bayesian methods combine prior information with data</p></li>
<li><p>They have only limited practical usefulness - limits on types priors, limits on number of parameters</p></li>
<li><p>They are useful teaching tools, soon we will see how we can extend beyond these simple models</p></li>
</ul>
</div>
<div id="why-bayesian-statistical-estimation-revisit" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Why Bayesian: Statistical Estimation Revisit<a href="bayes.html#why-bayesian-statistical-estimation-revisit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="classic-frequentist-inference" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Classic Frequentist Inference<a href="bayes.html#classic-frequentist-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Many common intuitive data-based estimators are maximum likelihood estimators
<ul>
<li>sample mean, <span class="math inline">\(\bar{x}\)</span></li>
<li>sample proportion, <span class="math inline">\(\hat{p}\)</span></li>
<li>rate of an event, <span class="math inline">\(\hat{\lambda} = \frac{\text{Number Events}}{\text{Follow-up Time}}\)</span></li>
<li>least square estimators from linear regression model</li>
<li>OR &amp; RR</li>
<li>HR, cox regression uses partial-likelihood approach</li>
</ul></li>
<li>Likelihood-based inference does two things
<ul>
<li>Finds the MLE, the parameter value that is most likely, given the observed data.</li>
<li>Finds how much curvature there is near this maximum. This leads to estimates of standard errors and CIs.</li>
</ul></li>
<li>The 95% CI refers to the frequency with which hypothetical future random intervals from repeated sampling constructed in the same way will include the true value
<ul>
<li>A particular interval will either include the value or not.</li>
<li><strong>It does not mean that the interval contains the true value with 0.95 probability</strong>, i.e. we do not have <span class="math inline">\(P(\theta \mid data)\)</span></li>
</ul></li>
</ul>
<p><strong>Hypothesis Testing in a nutshell</strong></p>
<p>Hypothesis testing is a procedure for determining whether a hypothesis is valid or not, based on sample evidence</p>
<ol style="list-style-type: decimal">
<li><p>Set up two hypotheses: Null (<span class="math inline">\(H_0\)</span>) &amp; Αlternative (<span class="math inline">\(H_A\)</span>)</p>
<ul>
<li><span class="math inline">\(H_0\)</span>, Specifies population parameter of interest &amp; proposes baseline value for it. Null presumed true unless disproved</li>
<li><span class="math inline">\(H_A\)</span>, Contains value of parameter different from H0; typically what you want to show</li>
</ul></li>
<li><p>Specify appropriate model (find test statistic)</p>
<ul>
<li>Z test, T test, chi-square test etc</li>
<li>under the assumption that the model is correct</li>
</ul></li>
<li><p>Mechanics (calculate p-value)</p>
<ul>
<li>We want to compare data to what we would expect to see if <span class="math inline">\(H_0\)</span> was true and ask how likely it is to get these data, if <span class="math inline">\(H_0\)</span> was true</li>
<li>P-value is used to quantify evidence against <span class="math inline">\(H_0\)</span>; The probability of getting equally or more extreme data (i.e. test statistic) under repeated sampling if <span class="math inline">\(H_0\)</span> is true</li>
</ul>
<p><span class="math display">\[ P_{-value} = P\Bigg( \begin{matrix}
\text{observed or more}  \\
\text{extreme test statistics} 
\end{matrix} \mid H_0 \ is \ true \Bigg) \]</span></p>
<ul>
<li>P-value tells how “surprising” are data you observed under <span class="math inline">\(H_0\)</span></li>
</ul></li>
<li><p>Arrive at conclusion: Reject <span class="math inline">\(H_0\)</span> or not</p></li>
</ol>
<ul>
<li>Decisions about rejecting H0 or not are usually done by computing a p-value and comparing to <span class="math inline">\(\alpha\)</span>, significance level</li>
<li>significance level, an arbitrary threshold for P-value to define “rare/extreme event.” If P-value falls below this level, we conclude results are statistically significance and we reject the null</li>
</ul>
<p><strong>Hypothesis Tests Errors</strong></p>
<ul>
<li>Type I error (<span class="math inline">\(\alpha\)</span>): <span class="math inline">\(H_0\)</span> is true, but we reject it <span class="math inline">\(\alpha = P(Reject \ H_0 | H_0 \ is \ true)\)</span>
<ul>
<li>We commonly control for Type I error - “innocent until proven guilty”</li>
</ul></li>
<li>Type II error (<span class="math inline">\(\beta\)</span>): <span class="math inline">\(H_0\)</span> is false, but we fail reject it <span class="math inline">\(\beta = P(Fail \ to \ Reject \ H_0 | H_0 \ is \ false)\)</span>
<ul>
<li>A test’s ability to detect a false null hypothesis is called the power of the test, <span class="math inline">\(1-\beta\)</span></li>
</ul></li>
<li>For any test, there is trade-off between <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> (e.g., for fixed n, decreasing<span class="math inline">\(\alpha\)</span> increases <span class="math inline">\(\beta\)</span>, vice-versa )</li>
</ul>
<div class="workedexample">
<p><strong>Revisit Estimating a Proportion</strong></p>
<p>Suppose you have observed 6 patients in a Phase I RCT on a given dose of drug,
- 0 out of 6 patients have had an adverse event
- decision to escalate to a higher dose if it’s unlikely that the current dosing results in a true proportion of adverse events above 20%</p>
<p>Let <span class="math inline">\(\theta\)</span> be the true adverse event proportion, we want to test the following hypothesis</p>
<p><span class="math display">\[H_0: \theta = 0.2\]</span>
<span class="math display">\[H_a: \theta \leq 0.2\]</span></p>
<ul>
<li>We implement an exact binomial test</li>
</ul>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="bayes.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">binom.test</span>(<span class="at">x=</span><span class="dv">0</span>, <span class="at">n=</span><span class="dv">6</span>, <span class="at">p =</span> <span class="fl">0.2</span>,</span>
<span id="cb54-2"><a href="bayes.html#cb54-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">alternative =</span> <span class="fu">c</span>(<span class="st">&quot;less&quot;</span>),</span>
<span id="cb54-3"><a href="bayes.html#cb54-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Exact binomial test
## 
## data:  0 and 6
## number of successes = 0, number of trials = 6, p-value = 0.2621
## alternative hypothesis: true probability of success is less than 0.2
## 95 percent confidence interval:
##  0.0000000 0.3930378
## sample estimates:
## probability of success 
##                      0</code></pre>
<ul>
<li><p>P-value = P(0 or more extreme: <span class="math inline">\(X \leq 0 \mid \theta = 0.2\)</span>) = P( 0 adverse event among 6 patients <span class="math inline">\(\mid\)</span> a binomial model with <span class="math inline">\(\theta = 0.2\)</span>) = <code>dbinom(x=0,size=6,prob=0.2)</code> = 0.262144.</p></li>
<li><p>Using significance level/type I error at 5%, 0.262144 &gt; 0.05, thus, we fail to reject <span class="math inline">\(H_0\)</span>.</p></li>
</ul>
</div>
<p><strong>More about p-value</strong> <span class="citation">(<a href="#ref-wasserstein2016asa" role="doc-biblioref">Wasserstein and Lazar 2016</a>)</span></p>
<ul>
<li><p>By its definition, <strong>P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.</strong></p></li>
<li><p><strong>Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.</strong></p>
<ul>
<li>“A conclusion does not immediately become “true” on one side of the divide and “false” on the other.”</li>
</ul></li>
<li><p><strong>A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.</strong></p></li>
<li><p>“By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.”</p>
<ul>
<li>“For example, a p-value near 0.05 taken by itself offers only weak evidence against the null hypothesis.”</li>
<li>absence of evidence is not evidence of absence</li>
</ul></li>
<li><p>Inference is performed based on data that have not been observed, under an assumption that is almost certain not to be true</p>
<ul>
<li>Would you be willing to bet that the true probability a coin comes up heads is exactly P(head)=0.5, or that the exact true adverse rate is fixed at 0.2?</li>
</ul></li>
</ul>
</div>
<div id="bayesian-inference" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Bayesian Inference<a href="bayes.html#bayesian-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>In Bayesian statistics, inference is done using the posterior distribution</p></li>
<li><p>Given a posterior distribution, we can calculate any summary of it we like</p>
<ul>
<li><span class="math inline">\(P(0.008 &lt; \theta &lt; 0.012)\)</span>, <span class="math inline">\(P( \theta &lt; 0.1)\)</span>, interval with 95% probability of containing <span class="math inline">\(\theta\)</span></li>
<li>We can use quantities to summarize the posterior distribution (mean, median, sd, IQR)</li>
<li>We can plot the density function and shade areas of interest</li>
</ul></li>
</ul>
<p><strong>Posterior uncertainty</strong></p>
<ul>
<li>Posterior variance is used to characterize the posterior uncertainty
<ul>
<li>In most cases, it is smaller than the prior variance</li>
<li>In some cases, for some models, the opposite is true, when prior is in direct conflict with the data</li>
</ul></li>
<li><strong>Credible intervals</strong> play the role of the confidence intervals
<ul>
<li>For the 95% credible interval <span class="math inline">\([\alpha, \beta]\)</span>, <span class="math inline">\(P(\alpha&lt; \theta &lt;\beta \mid y)\)</span> = 0.95</li>
</ul></li>
</ul>
<p><strong>Hypothesis testing</strong></p>
<ul>
<li><p>For a continuous parameter <span class="math inline">\(\theta\)</span>, the hypothesis that <span class="math inline">\(\theta\)</span> is exactly some fix value (e.g., 1 for OR) is rarely reasonable and it’s more of interest to estimate a posterior distribution or a corresponding interval estimate of <span class="math inline">\(\theta\)</span></p>
<ul>
<li>The question ‘Dose <span class="math inline">\(\theta\)</span> equal 1’ can be rephrased as ‘what is the posterior distribution for <span class="math inline">\(\theta\)</span>’</li>
</ul></li>
</ul>
<div class="important">
<p><strong>Bayesian Factor for one-sided test</strong></p>
<p>The <strong>Bayes Factor (BF)</strong> compares the posterior odds to the prior odds, and provides insight into just how much our updated belief about the parameter evolved upon observing sample data</p>
<p>In a hypothesis test of two competing hypotheses, <span class="math inline">\(H_0\)</span> vs <span class="math inline">\(H_a\)</span>,</p>
<p><span class="math display">\[\text{Bayes Factor} = \frac{\text{Posterior odds}}{\text{Prior odds}} = \frac{\frac{P(H_0 \mid y)}{P(H_1 \mid y)}}{\frac{P(H_0)}{P(H_a)}}\]</span></p>
<p>Example: (Beta-binomial model) Data
- Data: 0 out of 6 patients have had an adverse event
- Prior: <span class="math inline">\(\theta \sim Beta(1,1)\)</span>
- Posterior: <span class="math inline">\(\theta \mid y \sim Beta(1,7)\)</span></p>
<p>Let <span class="math inline">\(\theta\)</span> be the true adverse event proportion, we are interested in testing the following hypothesis
- <span class="math inline">\(H_0: \theta \geq 0.2\)</span>
- <span class="math inline">\(H_a: \theta \leq 0.2\)</span></p>
<p><span class="math display">\[BF = \frac{\frac{P(\theta \geq 0.2 \mid y)}{P(\theta \leq 0.2 \mid y)}}{\frac{P(\theta \geq 0.2)}{P(\theta \leq 0.2)}} = \frac{\frac{qbeta(0.2, 1, 7)}{1-qbeta(0.2, 1, 7)}}{\frac{qbeta(0.2, 1, 1)}{1- qbeta(0.2, 1, 1)}} = \frac{0.032}{0.25} = 0.1295647\]</span></p>
<p>Bayes factor calibration table from <span class="citation">(<a href="#ref-spiegelhalter2003" role="doc-biblioref">David J. Spiegelhalter, Abrams, and Myles 2003</a>)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-58"></span>
<img src="images/bayes_factor_spiegelhalter.jpg" alt="Calibration of Bayes factor (likelihood ratio) provided by Jeffreys from Spiegelhalter 2003 " width="436" />
<p class="caption">
Figure 3.5: Calibration of Bayes factor (likelihood ratio) provided by Jeffreys from Spiegelhalter 2003
</p>
</div>
<ul>
<li>Our BF estimate, 1/10 &lt; 0.1295647 &lt; 1/3.2, based on the calibration table, we would conclude substantial evidence against <span class="math inline">\(H_0: \theta \geq 0.2\)</span> and in favour of <span class="math inline">\(H_0: \theta \leq 0.2\)</span>.</li>
</ul>
</div>
<p><strong>Posterior predictive distributions</strong></p>
<ul>
<li>Often we are interested in extending our (Bayesian) analysis using priors and data, and predicting outcomes in data observed in the future
<ul>
<li>e.g., predicting number of successes in a future trial , where <span class="math inline">\(n* = 25\)</span>, 25 new observations</li>
</ul></li>
<li>Essentially we want the probability density $P(y^* y), where with <span class="math inline">\(y^*\)</span> we denote the new data
<ul>
<li>This probability is not always possible to compute directly, it does not correspond to some known distribution</li>
<li>We will see that these kinds of questions can be answered using Markov Chain Monte Carlo method and brms (coming up in a couple of weeks)</li>
</ul></li>
</ul>
<div class="important">
<p><strong>Posterior predictive distribution</strong></p>
<p>Let <span class="math inline">\(y^*\)</span> denote a new data, the posterior predictive distribution for <span class="math inline">\(y^*\)</span> is</p>
<p><span class="math display">\[P(y^* \mid y)  = \int P(y^* \mid \theta) P(\theta \mid y) \ d \theta\]</span></p>
<ul>
<li><p>The posterior predictive distribution as an average of conditional predictions over the posterior distribution of <span class="math inline">\(\theta\)</span></p></li>
<li><p>We assume conditional independence of <span class="math inline">\(y^*\)</span> and <span class="math inline">\(y\)</span> given <span class="math inline">\(\theta\)</span></p></li>
</ul>
</div>
<p>In the beta-binomial adverse event example, let <span class="math inline">\(y^*\)</span> denote the result of a new trial of 6 patients given posterior <span class="math inline">\(\theta \mid y \sim Beta(1,7)\)</span>,</p>
<p><span class="math display">\[P(y^* \mid y)  = \int_0^1 {6 \choose y^*} \theta^{y^*} (1- \theta)^{6-y^*} P(\theta \mid y) \ d \theta, y^* \in \{0,1,2,3,4,5,6\}\]</span>
<span class="math display">\[P(\theta \mid y) = \frac{\Gamma(8)}{\Gamma(1)\Gamma(7)} \theta^{0}(1-theta)^{6}\]</span></p>
<div class="fold s">

<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="bayes.html#cb56-1" aria-hidden="true" tabindex="-1"></a>likelihood<span class="ot">&lt;-</span><span class="fu">length</span>(<span class="dv">7</span>)</span>
<span id="cb56-2"><a href="bayes.html#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>) {</span>
<span id="cb56-3"><a href="bayes.html#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="bayes.html#cb56-4" aria-hidden="true" tabindex="-1"></a>integrand_pred <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) {</span>
<span id="cb56-5"><a href="bayes.html#cb56-5" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">choose</span>(<span class="dv">6</span>, i<span class="dv">-1</span>)<span class="sc">*</span>theta<span class="sc">^</span>(i<span class="dv">-1</span>)<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span>(<span class="dv">6</span><span class="sc">-</span>i<span class="sc">+</span><span class="dv">1</span>))<span class="sc">*</span>((<span class="fu">gamma</span>(<span class="dv">8</span>)<span class="sc">/</span><span class="fu">gamma</span>(<span class="dv">1</span>)<span class="sc">/</span><span class="fu">gamma</span>(<span class="dv">7</span>))<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>theta)<span class="sc">^</span><span class="dv">6</span>)}</span>
<span id="cb56-6"><a href="bayes.html#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="bayes.html#cb56-7" aria-hidden="true" tabindex="-1"></a>likelihood[i] <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrand_pred,<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>)<span class="sc">$</span>value   </span>
<span id="cb56-8"><a href="bayes.html#cb56-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb56-9"><a href="bayes.html#cb56-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb56-10"><a href="bayes.html#cb56-10" aria-hidden="true" tabindex="-1"></a>posterior_pred<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">y_star=</span><span class="dv">0</span><span class="sc">:</span><span class="dv">6</span>, <span class="at">likelihood =</span> likelihood)</span>
<span id="cb56-11"><a href="bayes.html#cb56-11" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(posterior_pred, <span class="at">caption =</span> <span class="st">&quot;Posterior predictive distribution for a new trial of 6 patients&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-59">Table 3.1: </span>Posterior predictive distribution for a new trial of 6 patients</caption>
<thead>
<tr class="header">
<th align="right">y_star</th>
<th align="right">likelihood</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0.5384615</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0.2692308</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0.1223776</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.0489510</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.0163170</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.0040793</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.0005828</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="bayes.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(posterior_pred<span class="sc">$</span>likelihood)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div>


<p><strong>Why Bayesian? Practical Reasons</strong></p>
<ul>
<li>Apart from making more sense to use then P-value, the modern Bayesian approach offers some practical advantages to data analysis
<ul>
<li>Prior information</li>
<li>Direct probability statements</li>
<li>Subjectivity acknowledged and formally represented</li>
</ul></li>
</ul>
<p><strong>Prior information</strong></p>
<ul>
<li>Science rarely proceeds in the absence of any prior information or opinion</li>
<li>Bayesian reasoning allows the formal incorporation of this opinion</li>
<li>The classical approach introduces prior information and opinion in study planning
and, informally, at the ” stage of results - high risk of inconsistency</li>
<li>In Bayesian approach:
<ul>
<li>For small sample sizes, a prior will “rein in” imprecise data-based estimates</li>
<li>For large sample sizes, most priors are overwhelmed by the likelihood (data)</li>
</ul></li>
</ul>
<p><strong>Probability Statements</strong></p>
<ul>
<li><p>A result of a Bayesian analysis is a posterior distribution, <span class="math inline">\(P(\theta \mid data)\)</span>, for an
unknown parameter (e.g. an odds ratio <span class="math inline">\(\theta\)</span>) given the data</p></li>
<li><p>We can compute probabilities directly from the posterior distribution</p></li>
</ul>
<p><span class="math display">\[P(\theta &lt; 1 \mid data) = \int_0^1 P(\theta \mid data) d\theta\]</span></p>
<ul>
<li><p>this is the area under the posterior density <span class="math inline">\(P(\theta \mid data)\)</span> between the value of 0 and 1 for <span class="math inline">\(\theta\)</span></p></li>
<li><p>this is what we prefer p-value to be!</p></li>
<li><p>we can define <span class="math inline">\(P(\theta &lt; 0.9 )\)</span> as a clincically important effect, then we calculate the probability of <span class="math inline">\(P(\theta &lt; 0.9 \mid data )\)</span>. No need to choose a “significance” level, we can directly use this probably to make clinical decision!</p></li>
<li><p>We can also ask which is more likely - an important effect (<span class="math inline">\(H_a\)</span>) or an unimportant effect (<span class="math inline">\(H_0\)</span>), e.g., <span class="math inline">\(P(\theta &lt; 0.9)\)</span> or <span class="math inline">\(P(0.95 &lt; \theta &lt; 1.05)\)</span></p></li>
<li><p>We can compute probabilities for statements combining more than one parameter (e.g. regression parameters), e.g., <span class="math inline">\(P(\beta_1 &lt;0 \cap \beta_2 &lt;0)\)</span>, gives the probability that two slopes are both negative.</p>
<ul>
<li>Statements about compound events can facilitate direct group comparisons</li>
</ul>
<p><span class="math display">\[ E(Y_i \mid stage_i) = \beta_1 + \beta_2 \times I(stage_i=II) + \beta_3 \times I(stage_i=III) + \beta_4 \times I(stage_i=IV)\]</span></p>
<ul>
<li>The probability that mean of outcome under stage II is greater than mean under stage I: <span class="math inline">\(P(\beta_2 &gt; 0 )\)</span></li>
<li>The probability that mean of outcome under stage IV is greater than mean under stage II: <span class="math inline">\(P(\beta_4 &gt; \beta_2 )\)</span></li>
</ul></li>
</ul>
<p><strong>Summary</strong></p>
<p>Practical advantages of Bayesian approach</p>
<ul>
<li><p>Direct probability statements</p></li>
<li><p>Incorporation of external evidence</p></li>
<li><p>Complex models can be built step-by-step</p></li>
<li><p>Makes you understand the analysis better</p></li>
</ul>
<p>Disadvantages including</p>
<ul>
<li><p>Learning how to code</p></li>
<li><p>Need to know more about distributions</p></li>
<li><p>Require good probability and statistical model knowledge</p></li>
<li><p>More time consuming (computationally more expensive with complex model and multiple parameters)</p></li>
</ul>
</div>
<div id="r-session-information-2" class="section level3 unnumbered hasAnchor">
<h3>R Session information<a href="bayes.html#r-session-information-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<pre><code>## R version 4.1.3 (2022-03-10)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19044)
## 
## Matrix products: 
## 
## locale:
## [1] LC_COLLATE=English_Canada.1252  LC_CTYPE=English_Canada.1252   
## [3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C                   
## [5] LC_TIME=English_Canada.1252    
## 
## attached base packages:
## [1] grid      stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] bayesplot_1.9.0     ggmcmc_1.5.1.1      tidyr_1.2.0        
##  [4] ggpubr_0.4.0        tweenr_1.0.2        gganimate_1.0.7    
##  [7] VennDiagram_1.7.1   futile.logger_1.4.3 truncnorm_1.0-8    
## [10] brms_2.16.3         Rcpp_1.0.8.3        dplyr_1.0.8        
## [13] ggplot2_3.3.5</code></pre>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-hernandez2019effect" class="csl-entry">
Hernández, Glenn, Gustavo A Ospina-Tascón, Lucas Petri Damiani, Elisa Estenssoro, Arnaldo Dubin, Javier Hurtado, Gilberto Friedman, et al. 2019. <span>“Effect of a Resuscitation Strategy Targeting Peripheral Perfusion Status Vs Serum Lactate Levels on 28-Day Mortality Among Patients with Septic Shock: The ANDROMEDA-SHOCK Randomized Clinical Trial.”</span> <em>Jama</em> 321 (7): 654–64.
</div>
<div id="ref-spiegelhalter2003" class="csl-entry">
Spiegelhalter, David J., Keith R. Abrams, and Jonathan P. Myles. 2003. <em>Bayesian Approaches to Clinical Trials and Health-Care Evaluation</em>. John Wiley &amp; Sons, Ltd. <a href="https://doi.org/10.1002/0470092602">https://doi.org/10.1002/0470092602</a>.
</div>
<div id="ref-wasserstein2016asa" class="csl-entry">
Wasserstein, Ronald L, and Nicole A Lazar. 2016. <span>“The ASA Statement on p-Values: Context, Process, and Purpose.”</span> <em>The American Statistician</em>. Taylor &amp; Francis.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prob.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Prior.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": "twitter"
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
