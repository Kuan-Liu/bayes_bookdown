<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Session 4 Considering Prior Distributions | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research</title>
  <meta name="description" content="Course notes for HAD5314H Winter 2022" />
  <meta name="generator" content="bookdown 0.22.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Session 4 Considering Prior Distributions | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://kuan-liu.github.io/bayes_bookdown/" />
  
  <meta property="og:description" content="Course notes for HAD5314H Winter 2022" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Session 4 Considering Prior Distributions | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research" />
  
  <meta name="twitter:description" content="Course notes for HAD5314H Winter 2022" />
  

<meta name="author" content="Kuan Liu   Institute of Health Policy, Management and Evaluation   University of Toronto" />


<meta name="date" content="2022-01-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayes.html"/>
<link rel="next" href="BayesReg.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">HAD5314H - Winter 2022 </a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>University of Toronto Statement of Acknowledgment of Traditional Land</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html"><i class="fa fa-check"></i>Course Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#course-info"><i class="fa fa-check"></i>Course Info</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#course-description"><i class="fa fa-check"></i>Course Description</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#course-textbook-and-structure"><i class="fa fa-check"></i>Course Textbook and Structure</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#calendar-and-outline"><i class="fa fa-check"></i>Calendar and Outline</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#accessibility-and-accommodations"><i class="fa fa-check"></i>Accessibility and Accommodations</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#academic-integrity"><i class="fa fa-check"></i>Academic Integrity</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#key-resources-and-supports-for-dslph-graduate-students"><i class="fa fa-check"></i>Key Resources and Supports for DSLPH Graduate Students</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="into.html"><a href="into.html"><i class="fa fa-check"></i><b>1</b> Introduction to the course</a>
<ul>
<li class="chapter" data-level="1.1" data-path="into.html"><a href="into.html#about-me"><i class="fa fa-check"></i><b>1.1</b> About me</a></li>
<li class="chapter" data-level="1.2" data-path="into.html"><a href="into.html#syllabus"><i class="fa fa-check"></i><b>1.2</b> Syllabus</a></li>
<li class="chapter" data-level="1.3" data-path="into.html"><a href="into.html#some-history"><i class="fa fa-check"></i><b>1.3</b> Some history</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="into.html"><a href="into.html#bayesian-history"><i class="fa fa-check"></i><b>1.3.1</b> Bayesian history</a></li>
<li class="chapter" data-level="1.3.2" data-path="into.html"><a href="into.html#history-of-this-course"><i class="fa fa-check"></i><b>1.3.2</b> History of this course</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="into.html"><a href="into.html#thinking-like-a-bayesian-using-the-concept-of-probability"><i class="fa fa-check"></i><b>1.4</b> Thinking like a Bayesian using the concept of probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="into.html"><a href="into.html#probability-is-not-unitary"><i class="fa fa-check"></i><b>1.4.1</b> Probability is not unitary</a></li>
<li class="chapter" data-level="1.4.2" data-path="into.html"><a href="into.html#bayes-rule"><i class="fa fa-check"></i><b>1.4.2</b> Bayesâ€™ Rule</a></li>
<li class="chapter" data-level="1.4.3" data-path="into.html"><a href="into.html#the-scientific-method-in-steps"><i class="fa fa-check"></i><b>1.4.3</b> The Scientific Method in steps</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html"><i class="fa fa-check"></i>Lab1 Getting started with R &amp; Rstudio</a>
<ul>
<li class="chapter" data-level="1.5" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#r-and-rstudio-installation"><i class="fa fa-check"></i><b>1.5</b> R and Rstudio Installation</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#windows-operating-system"><i class="fa fa-check"></i><b>1.5.1</b> Windows operating system</a></li>
<li class="chapter" data-level="1.5.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#macos-operating-system"><i class="fa fa-check"></i><b>1.5.2</b> macOS operating system</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#r-packages"><i class="fa fa-check"></i><b>1.6</b> R Packages</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#bayesian-analysis-in-r-using-brms-package"><i class="fa fa-check"></i><b>1.6.1</b> Bayesian Analysis in R using brms package</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#working-in-rstudio"><i class="fa fa-check"></i><b>1.7</b> Working in Rstudio</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#rstudio-layout"><i class="fa fa-check"></i><b>1.7.1</b> Rstudio layout</a></li>
<li class="chapter" data-level="1.7.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#customization"><i class="fa fa-check"></i><b>1.7.2</b> Customization</a></li>
<li class="chapter" data-level="1.7.3" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#working-directory"><i class="fa fa-check"></i><b>1.7.3</b> Working directory</a></li>
<li class="chapter" data-level="1.7.4" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#getting-help-with-r"><i class="fa fa-check"></i><b>1.7.4</b> Getting help with R</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#basic-r-a-crash-introduction"><i class="fa fa-check"></i><b>1.8</b> Basic R (a crash introduction)</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#arithmetic"><i class="fa fa-check"></i><b>1.8.1</b> Arithmetic</a></li>
<li class="chapter" data-level="1.8.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#vectors"><i class="fa fa-check"></i><b>1.8.2</b> Vectors</a></li>
<li class="chapter" data-level="1.8.3" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#data-frame---the-titanic-dataset"><i class="fa fa-check"></i><b>1.8.3</b> Data frame - The Titanic dataset</a></li>
<li class="chapter" data-level="1.8.4" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#simple-plots"><i class="fa fa-check"></i><b>1.8.4</b> Simple plots</a></li>
<li class="chapter" data-level="" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#r-session-information"><i class="fa fa-check"></i>R Session information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prob.html"><a href="prob.html"><i class="fa fa-check"></i><b>2</b> Probability, random variables and distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prob.html"><a href="prob.html#introduction-to-set-theory"><i class="fa fa-check"></i><b>2.1</b> Introduction to Set Theory</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prob.html"><a href="prob.html#composition-of-events"><i class="fa fa-check"></i><b>2.1.1</b> Composition of events</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prob.html"><a href="prob.html#probability-rules"><i class="fa fa-check"></i><b>2.2</b> Probability Rules</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prob.html"><a href="prob.html#simple-rules"><i class="fa fa-check"></i><b>2.2.1</b> Simple Rules</a></li>
<li class="chapter" data-level="2.2.2" data-path="prob.html"><a href="prob.html#conditional-probability"><i class="fa fa-check"></i><b>2.2.2</b> Conditional Probability</a></li>
<li class="chapter" data-level="2.2.3" data-path="prob.html"><a href="prob.html#summary-of-probability-rules"><i class="fa fa-check"></i><b>2.2.3</b> Summary of Probability Rules</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prob.html"><a href="prob.html#discrete-random-variables"><i class="fa fa-check"></i><b>2.3</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prob.html"><a href="prob.html#introduction-to-discrete-random-variables"><i class="fa fa-check"></i><b>2.3.1</b> Introduction to Discrete Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prob.html"><a href="prob.html#common-discrete-distributions"><i class="fa fa-check"></i><b>2.4</b> Common Discrete Distributions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prob.html"><a href="prob.html#binomial-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Binomial Distribution</a></li>
<li class="chapter" data-level="2.4.2" data-path="prob.html"><a href="prob.html#poisson-distribution"><i class="fa fa-check"></i><b>2.4.2</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="prob.html"><a href="prob.html#continuous-random-variables"><i class="fa fa-check"></i><b>2.5</b> Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="prob.html"><a href="prob.html#uniform01-distribution"><i class="fa fa-check"></i><b>2.5.1</b> Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="2.5.2" data-path="prob.html"><a href="prob.html#exponential-distribution"><i class="fa fa-check"></i><b>2.5.2</b> Exponential Distribution</a></li>
<li class="chapter" data-level="2.5.3" data-path="prob.html"><a href="prob.html#normal-distribution"><i class="fa fa-check"></i><b>2.5.3</b> Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="prob.html"><a href="prob.html#r-quick-reference"><i class="fa fa-check"></i><b>2.6</b> R Quick Reference</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>3</b> Bayesian inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayes.html"><a href="bayes.html#priors"><i class="fa fa-check"></i><b>3.1</b> Priors</a></li>
<li class="chapter" data-level="3.2" data-path="bayes.html"><a href="bayes.html#posterior"><i class="fa fa-check"></i><b>3.2</b> Posterior</a></li>
<li class="chapter" data-level="3.3" data-path="bayes.html"><a href="bayes.html#prediction"><i class="fa fa-check"></i><b>3.3</b> Prediction</a></li>
<li class="chapter" data-level="3.4" data-path="bayes.html"><a href="bayes.html#decision-theory"><i class="fa fa-check"></i><b>3.4</b> Decision theory</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Prior.html"><a href="Prior.html"><i class="fa fa-check"></i><b>4</b> Considering Prior Distributions</a></li>
<li class="chapter" data-level="5" data-path="BayesReg.html"><a href="BayesReg.html"><i class="fa fa-check"></i><b>5</b> Bayesian Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="BayesReg.html"><a href="BayesReg.html#normal-models-and-linear-regression"><i class="fa fa-check"></i><b>5.1</b> Normal Models and Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="BayesReg.html"><a href="BayesReg.html#hierarchical-models-and-convergence"><i class="fa fa-check"></i><b>5.2</b> Hierarchical models and convergence</a></li>
<li class="chapter" data-level="5.3" data-path="BayesReg.html"><a href="BayesReg.html#models-for-binary-data"><i class="fa fa-check"></i><b>5.3</b> Models for Binary Data</a></li>
<li class="chapter" data-level="5.4" data-path="BayesReg.html"><a href="BayesReg.html#models-for-count-data"><i class="fa fa-check"></i><b>5.4</b> Models for Count Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a> <a href="https://www.kuan-liu.com/" target="blank">Developed by Kuan Liu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Prior" class="section level1" number="4">
<h1><span class="header-section-number">Session 4</span> Considering Prior Distributions</h1>
<!-- One of the most commonly asked questions when one first encounters Bayesian statistics is "how do we choose a prior?"  While there is never one "perfect" prior in any situation, we'll discuss in this chapter some issues to consider when choosing a prior.  But first, here are a few big picture ideas to keep in mind. -->
<!-- - The prior is only one part of the Bayesian model.  The likelihood is the other part.  And there is the data that is used to fit the model.  Choice of prior is just one of many modeling assumptions that should be evaluated and checked. -->
<!-- - In many situations, the posterior distribution is not too sensitive to reasonable changes in prior.  In these situations, the important question isn't "what is the prior?" but rather "is there a prior at all"? That is, are you adopting a Bayesian approach, treating parameters as random variables, and quantifying uncertainty about parameters with probability distributions? -->
<!-- - One criticism of Bayesian statistics in general and priors in particular is that they are subjective. -->
<!-- However, any statistical analysis is inherently subjective, filled with many assumptions and decisions along the way. -->
<!-- Except in the simplest situations, if you ask five statisticians how to approach a particular problem, you will likely get five different answers. -->
<!-- Priors and Bayesian data analysis are no more inherently subjective than any of the myriad other assumptions made in statistical analysis. -->
<!-- Subjectivity is OK, and often beneficial. Choosing a subjective prior allows us to explicitly incorporate a wealth of past experience into our analysis. -->
<!-- ```{example, pepsi} -->
<!-- Xiomara claims that she can predict which way a coin flip will land. To test her claim, you flip a fair coin 10 times, and she correctly predicts the result of each of the 10 flips. -->
<!-- Rogelio claims that he can taste the difference between Coke and Pepsi.  To test his claim, you give him a blind taste test of 10 cups, flipping a coin for each cup to determine whether to serve Coke or Pespi.  Rogelio correctly identifies each of the 10 cups. -->
<!-- Whose claim do you find more convincing, Xiomara's or Rogelio's? Or are you equally convinced? -->
<!-- Let $\theta_X$ be the probability that Xiomara correctly guess the result of a fair coin flip. -->
<!-- Let $\theta_R$ be the probability that Rogelio correctly guess the soda in a randomly selected cup. -->
<!-- ``` -->
<!-- 1. How might a frequentist address this situation?  What would the conclusion be? -->
<!-- 1. Consider a Bayesian approach.  Describe, in general terms, your prior distributions for the two parameters.  How do they compare?  How would this impact your conclusions? -->
<!-- 1. For Xiomara, a frequentist might conduct a hypothesis test of the null hypothesis $H_0:\theta_X = 0.5$ versus the alternative hypothesis: $H_a:\theta_X > 0.5$. The p-value would be about 0.001, the probability of observing 10 out of 10 successes from a Binomial distribution with parameters 10 and 0.5.  Rogelio's set up would be similar and would yield the same p-value.  So a strict frequentist would be equally convinced of the two claims. -->
<!-- 1. Prior to observing data, we are probably more skeptical of Xiomara's claim than Rogelio's.  Since coin flips are unpredictable, we would have a strong prior belief that $\theta_X$ is close to 0.5 (what it would be if she were just guessing).  Our prior for $\theta_X$ would have a mean of 0.5 and a small prior SD, to reflect that only values close to 0.5 seem plausible.  Therefore, it would require a lot of evidence to sway our prior beliefs.   -->
<!--     On the other hand, we might be familiar with people who can tell the difference between Coke and Pepsi; maybe we even can ourselves.  Our prior for $\theta_R$ would have a smaller prior SD than that of $\theta_X$ to allow for a wider range of plausible values.  We might even have a prior mean for $\theta_R$ above 0.5 if we have experience with a lot of people who can tell the difference between Coke and Pepsi.  Given the sample data, our posterior probability that $\theta_R>0.5$ would be larger than the posterior probability that $\theta_X > 0.5$, and we would be more convinced by Rogelio's claim than by Xiomara's. -->
<!-- Even if a prior does not represent strong prior beliefs, just having a prior distribution at all allows for Bayesian analysis.  Remember, both Bayesian and frequentist are valid approaches to statistical analyses, each with advantages and disadvantages.  That said, there are some issues with frequentist approaches that incorporating a prior distribution and adpopting a Bayesian approach alleviates. (To be fair, an upcoming investigation will address some of disadvantages of the Bayesian approach compare with the frequenstist approach.) -->
<!-- ```{example, tamika} -->
<!-- Tamika is a basketball player who throughout her career has had a probability of 0.5 of making any three point attempt. -->
<!-- However, her coach is afraid that her three point shooting has gotten worse. -->
<!-- To check this, the coach has Tamika shoot a series of three pointers; she makes 7 out of 24. -->
<!-- Does the coach have evidence that Tamika has gotten worse? -->
<!-- Let $\theta$ be the probability that Tamika successfully makes any three point attempt. -->
<!-- Assume attempts are independent. -->
<!-- ``` -->
<!-- 1. Prior to collecting data, the coach decides that he'll have convincing evidence that Tamika has gotten worse if the p-value is less than 0.025. Suppose the coach told Tamika to *shoot 24 attempts and then stop* and count the number of successful attempts. Use software to compute the p-value.  Is the coach convinced that Tamika has gotten worse? -->
<!-- 1. Prior to collecting data, the coach decides that he'll have convincing evidence that Tamika has gotten worse if the p-value is less than 0.025. Suppose the coach told Tamika to *shoot until she makes 7 three pointers and then stop* and count the number of total attempts. Use software to compute the p-value.  Is the coach convinced that Tamika has gotten worse? (Hint: the total number of attempts has a Negative Binomial distribution.) -->
<!-- 1. Now suppose the coach takes a Bayesian approach and assumes a Beta($\alpha$, $\beta$) prior distribution for $\theta$.  Suppose the coach told Tamika to *shoot 24 attempts and then stop* and count the number of successful attempts. Identify the likelihood function and the posterior distribution of $\theta$. -->
<!-- 1. Now suppose the coach takes a Bayesian approach and assumes a Beta($\alpha$, $\beta$) prior distribution for $\theta$.  Suppose the coach told Tamika to *shoot until she makes 7 three pointers and then stop* and count the number of total attempts. Identify the likelihood function and the posterior distribution of $\theta$. -->
<!-- 1. Compare the Bayesian and frequentist approaches in this example.  Does the "strength of the evidence" depend on how the data were collected? -->
<!-- ```{solution, tamika-sol} -->
<!-- to Example \@ref(exm:tamika) -->
<!-- ``` -->
<!-- 1. The null hypothesis is $H_0:\theta = 0.5$ and the alternative hypothesis is $H_a:\theta < 0.5$. If the null hypothesis is true and Tamika has not gotten worse, then $Y$, the number of successful attempts, has a Binomial(24, 0.5) distribution. The p-value is $P(Y \le 7) = 0.032$ from `pbinom(7, 24, 0.5)`. Using a strict threshold of 0.025, the coach has NOT been convinced that Tamika has gotten worse. -->
<!-- 1. The null hypothesis is $H_0:\theta = 0.5$ and the alternative hypothesis is $H_a:\theta < 0.5$. If the null hypothesis is true and Tamika has not gotten worse, then $N$, the number of total attempts required to achieve 7 successful attempts, has a Negative Binomial(7, 0.5) distribution. The p-value is $P(N \ge 24) = 0.017$ from `1 - pnbinom(23 - 7, 7, 0.5)`. (In R, `nbinom` only counts the total number of failures, not the total number of trials.) Using a strict threshold of 0.025, the coach has  been convinced that Tamika has gotten worse. -->
<!-- 1. The data is $Y$, the number of successful attempts in 24 attempts, which follows a Binomial(24, $\theta$) distribution.  The likelihood is $P(Y=7|\theta)$ -->
<!-- \[ -->
<!-- f(y=7|\theta) = \binom{24}{7}\theta^7(1-\theta)^{17} \propto \theta^{7}(1-\theta)^{17}, \qquad 0 < \theta < 1. -->
<!-- \] -->
<!-- The posterior distribution is the Beta($\alpha + 7$, $\beta + 17$) distribution. -->
<!-- 1. The data is $N$, the number of total attempts required to achieve 7 successful attempts, which follows a Negative Binomial(7, $\theta$) distribution.  The likelihood is $P(N=24|\theta)$ -->
<!-- \[ -->
<!-- f(n = 24|\theta) = \binom{24 - 1}{7 - 1}\theta^7(1-\theta)^{17} \propto \theta^{7}(1-\theta)^{17}, \qquad 0 < \theta < 1. -->
<!-- \] -->
<!-- (The $\binom{24 - 1}{7 - 1}$ follows from the fact that the last attempt has to be success.) Note that the shape of the likelihood as a function of $\theta$ is the same as in the previous part.  Therefore, the posterior distribution is the Beta($\alpha + 7$, $\beta + 17$) distribution. -->
<!-- 1. Even though both frequentist scenario involves 7 successes in 24 attempts, the p-value measuring the strength of the evidence to reject the null hypothesis differed depending on how the data were collected.  Using a strict cutoff of 0.025 led the coach to reject the null hypothesis in one scenario but not the other. -->
<!--     However, the Bayesian analysis is the same in either scenario since the posterior distributions were the same.  For the Bayesian analysis, all that mattered about the data was that there were 7 successes in 24 attempts. -->
<!-- Bayesian data analysis treats parameters as random variables with probability distributions. -->
<!-- The prior distribution quantifies the researcher's uncertainty about parameters *before* observing data.  Some issues to consider when choosing a prior include, in no particular order: -->
<!-- - The researcher's prior beliefs! A prior distribution is part of a statistical model, and should be consistent with knowledge about the underlying scientific problem. Researchers are often experts with a wealth of past experience that can be explicitly incorporated into the analysis via the prior distribution. Such a prior is called an informative or weakly informative prior. -->
<!-- - A regularizing prior.  A prior which, when tuned properly, reduces overfitting or "overreacting" to the data. -->
<!-- - Noninformative prior a.k.a., (reference, vague, flat prior).  A prior is sought that plays a minimal role in inference so that "the data can speak for itself". -->
<!-- - Mathematical convenience.  The prior is chosen so that computation of the posterior is simplified, as in the case of conjugate priors. -->
<!-- - Interpretation.  The posterior is a compromise between the data and prior.  Some priors allow for easy interpretation of the relative contributions of data and prior to the posterior.  For example, think of the "prior successes and prior failures" interpretation in the Beta-Binomial model. -->
<!-- - Prior based on *past* data.  Bayesian updating can be viewed as an iterative process.  The posterior distribution obtained from one round of data collection can inform the prior distribution for another round. -->
<!-- For those initially skeptical of prior distributions at all, the strategy of always choosing an informative or flat prior might be appealing.  Flat priors are common, but are rarely ever the best choices from a modeling perspective. Just like you would not want to assume a Normal distribution for the likelihood in every problem, you would not to use a flat prior in every problem. -->
<!-- Furthermore, there are some subtle issues that arise when attempting to choose a noninformative prior. -->
<!-- ```{example, flat-prior} -->
<!-- Suppose we want to estimate $\theta$, the population proportion of Cal Poly students who wore socks at any point yesterday. -->
<!-- ``` -->
<!-- 1. What are the possible values for $\theta$?  What prior distribution might you consider a noninformative prior distribution? -->
<!-- 1. You might choose a Uniform(0, 1) prior, a.k.a., a Beta(1, 1) prior. Recall how we interpreted the parameters $\alpha$ and $\beta$ in the Beta-Binomial model.  Does the Beta(1, 1) distribution represent "no prior information"?   -->
<!-- 1. Suppose in a sample of 20 students, 4 wore socks yesterday. How would you estimate $\theta$ with a single number based only on the data? -->
<!-- 1. Assume a Beta(1, 1) prior and the 4/20 sample data. Identify the posterior distribution.  Recall that one Bayesian point estimate of $\theta$ is the posterior mean. Find the posterior mean of $\theta$.  Does this estimate let the "data speak entirely for itself"? -->
<!-- 1. How could you change $\alpha$ and $\beta$ in the Beta distribution prior to represent no prior information?  Sketch the prior. Do you see any potential problems? -->
<!-- 1. Assume a Beta(0, 0) prior for $\theta$ and the 4/20 sample data. Identify the posterior distribution.  Find the posterior *mode* of $\theta$.  Does this estimate let the "data speak entirely for itself"? -->
<!-- 1. Now suppose the parameter you want to estimate is the *odds* that a student wore socks yesterday, $\phi=\frac{\theta}{1-\theta}$.  What are the possible values of $\phi$?  What might a non-informative prior look like? Is this a proper prior? -->
<!-- 1. Assume a Beta(1, 1) prior for $\theta$.  Use simulation to approximate the prior distribution of the odds $\phi$.  Would you say this is a noninformative prior for $\phi$? -->
<!-- ```{solution, flat-prior-sol} -->
<!-- to Example \@ref(exm:flat-prior) -->
<!-- ``` -->
<!-- 1. $\theta$ takes values in (0, 1).  We might assume a flat prior on (0, 1), that is a Uniform(0, 1) prior. -->
<!-- 1. We interpreted $\alpha$ as "prior successes" and $\beta$ as "prior failures".  So a Beta(1, 1) is in some some equivalent to a "prior sample size" of 2.  Certainly not a lot of prior information, but it's not "no prior information" either. -->
<!-- 1. The sample proportion, 4/20 = 0.2. -->
<!-- 1. With a Beta(1, 1) prior and the 4/20 sample data, the posterior distribution is Beta(5, 17). -->
<!-- The posterior mean of $\theta$ is 5/22 = 0.227.  The posterior mean is a weighted average of the prior mean and the sample proportion: 0.227 = (0.5)(2/22) + (0.2)(20/22).  The "noninformative" prior does have influence; the data does not "speak entirely for itself". -->
<!-- 1. If $\alpha+\beta$ represents "prior sample size", we could try a Beta(0, 0) prior. Unfortunately, such a probability distribution does not actually exist.  For a Beta distribution, the parameters $\alpha$ and $\beta$ have to be strictly positive in order to have a valid pdf. The Beta(0, 0) density would be proportional to -->
<!-- \[ -->
<!-- \pi(\theta) \propto \theta^{-1}(1-\theta)^{-1}, \qquad 0 < \theta <1. -->
<!-- \] -->
<!-- However, this is not a valid pdf since $\int_0^1 \theta^{-1}(1-\theta)^{-1}d\theta = \infty$, so there is no constant that can normalize it to integrate to 1. -->
<!-- Even so, here is a plot of the "density".   -->
<!--     ```{r, echo = FALSE} -->
<!-- theta = seq(0.001,  0.999, 0.001) -->
<!-- plot(theta, 1/(theta*(1-theta)), xlim=c(0, 1), yaxt='n', ylab="", -->
<!--      main="Improper Beta(0, 0)", type="l", xlab = "theta") -->
<!--     ``` -->
<!--     Would you say this is a "noninformative" prior?  It seems to concentrate almost all prior "density" near 0 and 1.  -->
<!-- 1. Beta(0, 0) is an "improper" prior. -->
<!-- It's not a proper prior distribution, but it can lead to a proper posterior distribution. -->
<!-- The likelihood is $f(y=4|\theta) \propto \theta^4 (1-\theta)^{16}, 0 < \theta < 1$. -->
<!-- If we assume the prior is $\pi(\theta)\propto\theta^{-1}(1-\theta)^{-1}, 0 < \theta <1$, then the posterior is -->
<!-- \[ -->
<!-- \pi(\theta|y = 4) \propto \left(\theta^{-1}(1-\theta)^{-1}\right)\left(\theta^4 (1-\theta)^{16}\right) = \theta^{4 - 1} (1-\theta)^{16 - 1}, \qquad 0 <\theta < 1 -->
<!-- \] -->
<!--     That is, the posterior distribution is the Beta(4, 16) distribution.  The posterior mean is 4/20=0.2, the sample proportion. Hoever, the posterior *mode* is $\frac{4- 1}{4 + 16 -2}= \frac{3}{18} = 0.167$.  So the posterior mode does not let the "data speak entirely for itself". -->
<!-- 1. If $\theta=0$ then $\phi=0$; if $\theta=1$ then $\phi = \infty$. -->
<!-- So $\phi$ takes values in $(0, \infty)$. -->
<!-- We might choose a flat prior on $(0,\infty)$, $\pi(\phi) \propto 1, \phi > 0$. -->
<!-- However, this would be an improper prior. -->
<!-- 1. Simulate a value of $\theta$ from a Beta(1, 1) distribution, compute $\phi = \frac{\theta}{1-\theta}$, and repeat many times. The simulation results are below. (The distribution is extremely skewed to the right, so we're only plotting values in (0, 50).) -->
<!--     ```{r} -->
<!-- theta = rbeta(1000000, 1, 1) -->
<!-- odds = theta / (1 - theta) -->
<!-- hist(odds[odds<50], breaks = 100, xlab = "odds", freq = FALSE, -->
<!--      ylab = "density", -->
<!--      main = "Prior distribution of odds if prior distribution of probability is Uniform(0, 1)") -->
<!--     ``` -->
<!--     Even though the prior for $\theta$ was flat, the prior for a transformation of $\theta$ is not. -->
<!-- An *improper* prior distribution is a prior distribution that does not integrate to 1, so is not a proper probability density. However, an improper proper often results in a proper posterior distribution.  Thus, improper prior distributions are sometimes used in practice. -->
<!-- Flat priors are common choices in some situations, but are rarely ever the best choices from a modeling perspective.  Furthermore, flat priors are generally not preserved under transformations of parameters.  So a prior that is flat under one parametrization of the problem will generally not be flat under another. For example, when trying to estimate a population SD $\sigma$, assuming a flat prior for $\sigma$ will result in a non-flat prior for the population variance $\sigma^2$, and vice versa. -->
<!-- ```{example, rare-disease} -->
<!-- Suppse that $\theta$ represents the population proportion of adults who have a particular rare disease. -->
<!-- ``` -->
<!-- 1. Explain why you might not want to use a flat Uniform(0, 1) prior for $\theta$. -->
<!-- 1. Assume a Uniform(0, 1) prior. Suppose you will test $n=100$ suspected cases.  Use simulation to approximate the prior predictive distribution of the number in the sample who have the disease. Does this seem reasonable? -->
<!-- 1. Assume a Uniform(0, 1) prior. Suppose that in $n=100$ suspected cases, none actually has the disease.  Find and interpret the posterior median. Does this seem reasonable?   -->
<!-- 1. We know it's a rare disease! We want to concentrate most of our prior probability for $\theta$ near 0.  -->
<!-- 1. If the disease is rare, we might not expect any actual cases in a sample of 100, maybe 1 or 2.  However, the prior predictive distribution says that any value between 0 and 100 actual cases is equally likely!  This seems very unreasonable given that the disease is rare. -->
<!--     ```{r} -->
<!--     theta_sim = runif(10000) -->
<!--     y_sim = rbinom(10000, 100, theta_sim) -->
<!--     hist(y_sim, -->
<!--          xlab = "Simulated number of successes", -->
<!--          main = "Prior predictive distribution") -->
<!--     ``` -->
<!-- 1. The posterior distribution is the Beta(1, 101) distribution. -->
<!-- The posterior median is 0.007 (`qbeta(0.5, 1, 101)`). Based on a sample of 100 suspected cases with no actual cases, there is a posterior probability of 50% that more than 0.7% of people have the disease.  A rate of 7 actual cases in 1000 is not a very rare disease, and we think there's a 50% chance that the rate is even greater than this?  Again, this does not seem very reasonable based on our knowledge that the disease is rare. -->
<!-- **Prior predictive distributions** can be used to check the reasonableness of a prior for a given situation before observing sample data. Do the simulated samples seem consistent with what you might expect of the data based on your background knowledge of the situation? If not, another prior might be more reasonable. -->
<!-- ## What NOT to do when considering priors -->
<!-- You have a great deal of flexibility in choosing a prior, and there are many reasonable approaches.  However, there are a few things that you should NOT do. -->
<!-- **Do NOT choose a prior that assigns 0 probability/density to *possible* values of the parameter** regardless of how initially implausible the values are.  Even very stubborn priors can be overturned with enough data, but no amount of data can turn a prior probability of 0 into a positive posterior probability.  Always consider the range of possible values of the parameter, and be sure the prior density is non-zero over that range of values.  -->
<!-- **Do NOT base the *prior* on the observed data.** The prior reflects the degree of uncertainty about parameters *before* observing data.  Adjusting the *prior* to reflect observed data to achieve some desired result is akin to "data snooping" or "p-hacking" and is bad statistics.  (Of course, the *posterior* is based on the observed data. But not the prior.) -->
<!-- **Do NOT feel like you have to find that one, perfect prior.** -->
<!-- The prior is just one assumption of the model and should be considered just like other assumptions. -->
<!-- In practice, no assumption of a statistical model is ever satisfied exactly. -->
<!-- We only hope that our set of assumptions provides a reasonable model for reality. -->
<!-- No one prior will ever be just right for a situation, but some might be more reasonable than others. -->
<!-- You are not only allowed but encouraged to try different priors to see how sensitive the results are to the choice of prior.  (Remember, you should check the other assumptions too!) -->
<!-- There is also no requirement that you have to choose a single prior.  It's possible to consider several models, each consisting of its own prior, and average over these models.  (We'll see a little more detail about model averaging later.) -->
<!-- **Do NOT worry too much about the prior!** In general, in Bayesian estimation the larger the sample size the smaller the role that the prior plays. -->
<!-- But it is often desirable for the prior to play some role. -->
<!-- You should not feel the need to apologize for priors when significant prior knowledge is available. -->

</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="BayesReg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": "twitter"
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
