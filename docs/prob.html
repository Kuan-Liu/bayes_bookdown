<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Session 2 Probability, random variables and distributions | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research</title>
  <meta name="description" content="Course notes for HAD5314H Winter 2022" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Session 2 Probability, random variables and distributions | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://kuan-liu.github.io/bayes_bookdown/" />
  
  <meta property="og:description" content="Course notes for HAD5314H Winter 2022" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Session 2 Probability, random variables and distributions | HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research" />
  
  <meta name="twitter:description" content="Course notes for HAD5314H Winter 2022" />
  

<meta name="author" content="Kuan Liu   Institute of Health Policy, Management and Evaluation   University of Toronto" />


<meta name="date" content="2022-01-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lab1-getting-started-with-r-rstudio.html"/>
<link rel="next" href="bayes.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">HAD5314H - Winter 2022 </a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>University of Toronto Statement of Acknowledgment of Traditional Land</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html"><i class="fa fa-check"></i>Course Syllabus</a>
<ul>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#course-info"><i class="fa fa-check"></i>Course Info</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#course-description"><i class="fa fa-check"></i>Course Description</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#course-textbook-and-structure"><i class="fa fa-check"></i>Course Textbook and Structure</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#calendar-and-outline"><i class="fa fa-check"></i>Calendar and Outline</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#accessibility-and-accommodations"><i class="fa fa-check"></i>Accessibility and Accommodations</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#academic-integrity"><i class="fa fa-check"></i>Academic Integrity</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#key-resources-and-supports-for-dslph-graduate-students"><i class="fa fa-check"></i>Key Resources and Supports for DSLPH Graduate Students</a></li>
<li class="chapter" data-level="" data-path="course-syllabus.html"><a href="course-syllabus.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="into.html"><a href="into.html"><i class="fa fa-check"></i><b>1</b> Introduction to the course</a>
<ul>
<li class="chapter" data-level="1.1" data-path="into.html"><a href="into.html#about-me"><i class="fa fa-check"></i><b>1.1</b> About me</a></li>
<li class="chapter" data-level="1.2" data-path="into.html"><a href="into.html#syllabus"><i class="fa fa-check"></i><b>1.2</b> Syllabus</a></li>
<li class="chapter" data-level="1.3" data-path="into.html"><a href="into.html#some-history"><i class="fa fa-check"></i><b>1.3</b> Some history</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="into.html"><a href="into.html#bayesian-history"><i class="fa fa-check"></i><b>1.3.1</b> Bayesian history</a></li>
<li class="chapter" data-level="1.3.2" data-path="into.html"><a href="into.html#history-of-this-course"><i class="fa fa-check"></i><b>1.3.2</b> History of this course</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="into.html"><a href="into.html#thinking-like-a-bayesian-using-the-concept-of-probability"><i class="fa fa-check"></i><b>1.4</b> Thinking like a Bayesian using the concept of probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="into.html"><a href="into.html#probability-is-not-unitary"><i class="fa fa-check"></i><b>1.4.1</b> Probability is not unitary</a></li>
<li class="chapter" data-level="1.4.2" data-path="into.html"><a href="into.html#bayes-rule"><i class="fa fa-check"></i><b>1.4.2</b> Bayes’ Rule</a></li>
<li class="chapter" data-level="1.4.3" data-path="into.html"><a href="into.html#the-scientific-method-in-steps"><i class="fa fa-check"></i><b>1.4.3</b> The Scientific Method in steps</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html"><i class="fa fa-check"></i>Lab1 Getting started with R &amp; RStudio</a>
<ul>
<li class="chapter" data-level="1.5" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#r-and-rstudio-installation"><i class="fa fa-check"></i><b>1.5</b> R and RStudio Installation</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#windows-operating-system"><i class="fa fa-check"></i><b>1.5.1</b> Windows operating system</a></li>
<li class="chapter" data-level="1.5.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#macos-operating-system"><i class="fa fa-check"></i><b>1.5.2</b> macOS operating system</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#r-packages"><i class="fa fa-check"></i><b>1.6</b> R Packages</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#bayesian-analysis-in-r-using-brms-package"><i class="fa fa-check"></i><b>1.6.1</b> Bayesian Analysis in R using brms package</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#working-in-rstudio"><i class="fa fa-check"></i><b>1.7</b> Working in RStudio</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#rstudio-layout"><i class="fa fa-check"></i><b>1.7.1</b> RStudio layout</a></li>
<li class="chapter" data-level="1.7.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#customization"><i class="fa fa-check"></i><b>1.7.2</b> Customization</a></li>
<li class="chapter" data-level="1.7.3" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#working-directory"><i class="fa fa-check"></i><b>1.7.3</b> Working directory</a></li>
<li class="chapter" data-level="1.7.4" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#getting-help-with-r"><i class="fa fa-check"></i><b>1.7.4</b> Getting help with R</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#basic-r-a-crash-introduction"><i class="fa fa-check"></i><b>1.8</b> Basic R (a crash introduction)</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#arithmetic"><i class="fa fa-check"></i><b>1.8.1</b> Arithmetic</a></li>
<li class="chapter" data-level="1.8.2" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#vectors"><i class="fa fa-check"></i><b>1.8.2</b> Vectors</a></li>
<li class="chapter" data-level="1.8.3" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#data-frame---the-titanic-dataset"><i class="fa fa-check"></i><b>1.8.3</b> Data frame - The Titanic dataset</a></li>
<li class="chapter" data-level="1.8.4" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#simple-plots"><i class="fa fa-check"></i><b>1.8.4</b> Simple plots</a></li>
<li class="chapter" data-level="" data-path="lab1-getting-started-with-r-rstudio.html"><a href="lab1-getting-started-with-r-rstudio.html#r-session-information"><i class="fa fa-check"></i>R Session information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prob.html"><a href="prob.html"><i class="fa fa-check"></i><b>2</b> Probability, random variables and distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prob.html"><a href="prob.html#probability"><i class="fa fa-check"></i><b>2.1</b> Probability</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prob.html"><a href="prob.html#venn-diagrams"><i class="fa fa-check"></i><b>2.1.1</b> Venn Diagrams</a></li>
<li class="chapter" data-level="2.1.2" data-path="prob.html"><a href="prob.html#probability-rules"><i class="fa fa-check"></i><b>2.1.2</b> Probability Rules</a></li>
<li class="chapter" data-level="2.1.3" data-path="prob.html"><a href="prob.html#how-to-define-and-assign-probabilities-in-general"><i class="fa fa-check"></i><b>2.1.3</b> How to define and assign probabilities in general?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prob.html"><a href="prob.html#probability-distributions"><i class="fa fa-check"></i><b>2.2</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prob.html"><a href="prob.html#probability-density-functions"><i class="fa fa-check"></i><b>2.2.1</b> Probability density functions</a></li>
<li class="chapter" data-level="2.2.2" data-path="prob.html"><a href="prob.html#discrete-distributions"><i class="fa fa-check"></i><b>2.2.2</b> Discrete Distributions</a></li>
<li class="chapter" data-level="2.2.3" data-path="prob.html"><a href="prob.html#continous-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Continous Distributions</a></li>
<li class="chapter" data-level="" data-path="prob.html"><a href="prob.html#r-session-information-1"><i class="fa fa-check"></i>R Session information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>3</b> Introduction to Bayesian inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayes.html"><a href="bayes.html#classical-frequentist-approach"><i class="fa fa-check"></i><b>3.1</b> Classical frequentist approach</a>
<ul>
<li class="chapter" data-level="" data-path="bayes.html"><a href="bayes.html#r-session-information-2"><i class="fa fa-check"></i>R Session information</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Prior.html"><a href="Prior.html"><i class="fa fa-check"></i><b>4</b> Considering Prior Distributions</a></li>
<li class="chapter" data-level="5" data-path="BayesReg.html"><a href="BayesReg.html"><i class="fa fa-check"></i><b>5</b> Bayesian Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="BayesReg.html"><a href="BayesReg.html#normal-models-and-linear-regression"><i class="fa fa-check"></i><b>5.1</b> Normal Models and Linear Regression</a></li>
<li class="chapter" data-level="5.2" data-path="BayesReg.html"><a href="BayesReg.html#hierarchical-models-and-convergence"><i class="fa fa-check"></i><b>5.2</b> Hierarchical models and convergence</a></li>
<li class="chapter" data-level="5.3" data-path="BayesReg.html"><a href="BayesReg.html#models-for-binary-data"><i class="fa fa-check"></i><b>5.3</b> Models for Binary Data</a></li>
<li class="chapter" data-level="5.4" data-path="BayesReg.html"><a href="BayesReg.html#models-for-count-data"><i class="fa fa-check"></i><b>5.4</b> Models for Count Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a> <a href="https://www.kuan-liu.com/" target="blank">Developed by Kuan Liu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="prob" class="section level1" number="2">
<h1><span class="header-section-number">Session 2</span> Probability, random variables and distributions</h1>
<div class="chapterintro">
<ul>
<li>Review of probability terminologies, probability rules, and Venn Diagrams</li>
<li>Review conditional probability, independence, and Bayes’ theorem.</li>
<li>Review on random variables and common probability distributions</li>
</ul>
</div>
<script src="hideOutput.js"></script>
<p></br></p>
<div id="probability" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Probability</h2>
<p><strong>Probability has a central place in Bayesian analysis</strong></p>
<ul>
<li>we put a prior probability distribution on the unknowns (parameters),</li>
<li>we model the observed data with a probability distribution (likelihood),</li>
<li>and we combine the two into a posterior probability distribution</li>
</ul>
<div class="important">
<p><strong>Probability Terminology</strong><span class="citation">(<a href="#ref-evans2004probability" role="doc-biblioref">Evans and Rosenthal 2004</a>)</span></p>
<ul>
<li><p><strong>Sample space</strong> This set of all possible
outcomes of an experiment/trial is known as the sample space of the experiment/trial and is denoted by <span class="math inline">\(S\)</span> or <span class="math inline">\(\Omega\)</span>.</p></li>
<li><p><strong>Experiment/Trial</strong>: each occasion we observe a random phenomenon that we know what outcomes can occur, but we do not know which outcome will occur</p></li>
<li><p><strong>Event</strong>: Any subset of the sample space <span class="math inline">\(S\)</span> is known as an event, denoted by <span class="math inline">\(A\)</span>. Note that, <span class="math inline">\(A\)</span> is also a collection of one or more outcomes.</p></li>
<li><p><strong>Probability defined on events</strong>: For each event <span class="math inline">\(A\)</span> of the sample space <span class="math inline">\(S\)</span>, <span class="math inline">\(P(A)\)</span>, the probability of the event <span class="math inline">\(A\)</span>, satisfies the following three conditions:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(0 \leq P(A) \leq 1\)</span></li>
<li><span class="math inline">\(P(S) = 1\)</span> and <span class="math inline">\(P(\emptyset) = 0\)</span>; <span class="math inline">\(\emptyset\)</span> denotes the empty set</li>
<li><span class="math inline">\(P\)</span> is (countably) additive, meaning that if <span class="math inline">\(A_1, A_2, \ldots\)</span> is a finite or countable sequence of disjoint (also known as mutually exclusive events), then
<span class="math display">\[P(A_1 \cup A_2 \cup \ldots ) = P(A_1)+P(A_2)+\ldots \]</span></li>
</ol></li>
<li><p><strong>Union:</strong> Denote the event that either <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> occurs as <span class="math inline">\(A\cup B\)</span>.</p></li>
<li><p><strong>Intersection:</strong> Denote the event that <strong>both</strong> <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur as <span class="math inline">\(A\cap B\)</span></p></li>
<li><p><strong>Complement:</strong> Denote the event that <span class="math inline">\(A\)</span> does not occur as <span class="math inline">\(\bar{A}\)</span> or <span class="math inline">\(A^{C}\)</span> or <span class="math inline">\(A^\prime\)</span> (different people use different notations)</p></li>
<li><p><strong>Disjoint</strong> (or <strong>mutually exclusive</strong>): Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>disjoint</strong> if the occurrence of one event precludes the occurrence of the other. <em>If two events are mutually exclusive, then</em> <span class="math inline">\(P(A\cup B)=P(A)+P(B)\)</span></p></li>
</ul>
</div>
<div class="workedexample">
<p><strong>Sample Space</strong></p>
<ol style="list-style-type: decimal">
<li>If the experiment consists of the flipping of a coin, then <span class="math display">\[ S = \{H, T\}\]</span></li>
<li>If the experiment consists of rolling a die, then the sample space is <span class="math display">\[ S = \{1, 2, 3, 4, 5, 6\}\]</span></li>
<li>If the experiments consists of flipping two coins, then the sample space consists of the following four points:
<span class="math display">\[ S = \{(H,H), (H,T), (T,H), (T,T)\}\]</span></li>
</ol>
<p><strong>Event</strong></p>
<ol style="list-style-type: decimal">
<li>In Example (1), if E = {H}, then E is the event that a head appears on the flip of the coin. Similarly, if <span class="math inline">\(E = \{T \}\)</span>, then <span class="math inline">\(E\)</span> would be the event that a tail appears</li>
<li>In Example (2), if <span class="math inline">\(E = \{1\}\)</span>, then <span class="math inline">\(E\)</span> is the event that one appears on the roll of the die. If <span class="math inline">\(E = \{2, 4, 6\}\)</span>, then <span class="math inline">\(E\)</span> would be the event that an even number appears on the roll.</li>
<li>In Example (3), if <span class="math inline">\(E = \{(H, H), (H, T )\}\)</span>, then <span class="math inline">\(E\)</span> is the event that a head appears on the first coin.</li>
</ol>
<p><strong>Probability of an event</strong>. Let <span class="math inline">\(R\)</span> be the sum of two standard dice. Suppose we are interested in <span class="math inline">\(P(R \le 4)\)</span>. Notice that the pair of dice can fall 36 different ways (6 ways for the first die and six for the second results in 6x6 possible outcomes, and each way has equal probability <span class="math inline">\(1/36\)</span>.
<span class="math display">\[\begin{aligned} P(R \le 4 )   
  &amp;=    P(R=2)+P(R=3)+P(R=4) \\
    &amp;=  P(\left\{ 1,1\right\} )+P(\left\{ 1,2\right\} \mathrm{\textrm{ or }}\left\{ 2,1\right\} )+P(\{1,3\}\textrm{ or }\{2,2\}\textrm{ or }\{3,1\}) \\
    &amp;=  \frac{1}{36}+\frac{2}{36}+\frac{3}{36} \\
    &amp;=  \frac{6}{36} \\
    &amp;=  \frac{1}{6} \end{aligned}\]</span></p>
</div>
<div id="venn-diagrams" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Venn Diagrams</h3>
<p><strong>Venn diagrams provide a very useful graphical method for depicting the sample space S and subsets of it.</strong></p>
<p>Figure taken from <span class="citation">(<a href="#ref-cardinal2019sets" role="doc-biblioref">Cardinal 2019</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-17"></span>
<img src="images/venn.jpg" alt="Venn Diagram for two events" width="75%" />
<p class="caption">
Figure 2.1: Venn Diagram for two events
</p>
</div>
<div class="guidedexercise">
<ol style="list-style-type: decimal">
<li><p><strong>Venn Diagram for two disjoint events</strong> How would you draw this venn diagram?</p></li>
<li><p><strong>Label sub-regions in a Venn Diagram for three events</strong> Using set theory, how would you write out areas <em>a</em>, <em>d</em>, and <em>f</em> ?</p></li>
</ol>
</div>
<div class="fold s">

<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="prob.html#cb23-1" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;a&quot;</span>,<span class="st">&quot;b&quot;</span>, <span class="st">&quot;d&quot;</span>,<span class="st">&quot;e&quot;</span>)</span>
<span id="cb23-2"><a href="prob.html#cb23-2" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;b&quot;</span>,<span class="st">&quot;c&quot;</span>, <span class="st">&quot;d&quot;</span>,<span class="st">&quot;f&quot;</span>)</span>
<span id="cb23-3"><a href="prob.html#cb23-3" aria-hidden="true" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;d&quot;</span>,<span class="st">&quot;e&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;g&quot;</span>)</span>
<span id="cb23-4"><a href="prob.html#cb23-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">A=</span>A , <span class="at">B=</span>B , <span class="at">C=</span>C)</span>
<span id="cb23-5"><a href="prob.html#cb23-5" aria-hidden="true" tabindex="-1"></a>v0 <span class="ot">&lt;-</span> <span class="fu">venn.diagram</span>( x, <span class="at">filename=</span><span class="cn">NULL</span>, <span class="at">fill =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>),</span>
<span id="cb23-6"><a href="prob.html#cb23-6" aria-hidden="true" tabindex="-1"></a>                    <span class="at">alpha =</span> <span class="fl">0.50</span>, <span class="at">col =</span> <span class="st">&quot;transparent&quot;</span>)</span>
<span id="cb23-7"><a href="prob.html#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="prob.html#cb23-8" aria-hidden="true" tabindex="-1"></a>overlaps <span class="ot">&lt;-</span> <span class="fu">calculate.overlap</span>(x)</span>
<span id="cb23-9"><a href="prob.html#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co"># extract indexes of overlaps from list names</span></span>
<span id="cb23-10"><a href="prob.html#cb23-10" aria-hidden="true" tabindex="-1"></a>indx <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">substr</span>(<span class="fu">names</span>(overlaps),<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb23-11"><a href="prob.html#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="co"># labels start at position 7 in the list for Venn&#39;s with 3 circles</span></span>
<span id="cb23-12"><a href="prob.html#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(overlaps)){</span>
<span id="cb23-13"><a href="prob.html#cb23-13" aria-hidden="true" tabindex="-1"></a>  v0[[<span class="dv">6</span> <span class="sc">+</span> indx[i] ]]<span class="sc">$</span>label <span class="ot">&lt;-</span> <span class="fu">paste</span>(overlaps[[i]], <span class="at">collapse =</span> <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>) </span>
<span id="cb23-14"><a href="prob.html#cb23-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-15"><a href="prob.html#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="prob.html#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.draw</span>(v0)</span></code></pre></div>
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-18-1.png" width="384" style="display: block; margin: auto;" />
<div>

</div>
<div id="probability-rules" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Probability Rules</h3>
<p><strong>General Addition Rule:</strong> <span class="math inline">\(P(A\cup B)=P(A)+P(B)-P(A\cap B)\)</span></p>
<p>The reason behind this fact is that if there is if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not disjoint, then some area is added twice when we calculate <span class="math inline">\(P\left(A\right)+P\left(B\right)\)</span>. To account for this, we simply subtract off the area that was double counted. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint, <span class="math inline">\(P(A\cup B)=P(A)+P(B)\)</span>.</p>
<p><strong>Complement Rule:</strong> <span class="math inline">\(P(A)+P(A^c)=1\)</span></p>
<p>This rule follows from the partitioning of the set of all events (<span class="math inline">\(S\)</span>) into two disjoint sets, <span class="math inline">\(A\)</span> and <span class="math inline">\(A^c\)</span>. We learned above that <span class="math inline">\(A \cup A^c = S\)</span> and that <span class="math inline">\(P(S) = 1\)</span>. Combining those statements, we obtain the complement rule.</p>
<p><strong>Completeness Rule:</strong> <span class="math inline">\(P(A)=P(A\cap B)+P(A\cap B^c)\)</span></p>
<p>This identity is just breaking the event <span class="math inline">\(A\)</span> into two disjoint pieces.</p>
<p><strong>Law of total probability (unconditioned version):</strong> Let <span class="math inline">\(A_1, A_2, \ldots\)</span> be events that form a partition of the sample space <span class="math inline">\(S\)</span>, Let <span class="math inline">\(B\)</span> be any event. Then,
<span class="math display">\[P(B) = P(A_1 \cap B) + P(A_2 \cap B) + \ldots. \]</span></p>
<p>This law is key in deriving the marginal event probability in Bayes’ rule. Recall the HIV example from session 1, we have <span class="math inline">\(P(T^+) = P(T^+ \cap D^+)+P(T^+ \cap D^-)\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-19"></span>
<img src="images/law_tot_prob.png" alt="Demonstrate Law of total probabiliy using Venn Diagram" width="60%" />
<p class="caption">
Figure 2.2: Demonstrate Law of total probabiliy using Venn Diagram
</p>
</div>
<div class="workedexample">
<p><strong>Law of Total Probability</strong> Suppose I know that whenever it rains there is 10% chance of being late at work, while the chance is only 1% if it does not rain. Suppose that there is 30% chance of raining tomorrow; what is the chance of being late at work?</p>
<p>Denote with event <span class="math inline">\(A\)</span> as “I will be late to work tomorrow” and event <span class="math inline">\(B\)</span> as “It is going
to rain tomorrow”</p>
<p><span class="math display">\[\begin{aligned}
P(A) &amp;= P(A\mid B)P(B) + P(A \mid B^c) P(B^c) \\
              &amp;=    0.1\times 0.3+0.01\times 0.7=0.037
\end{aligned}\]</span></p>
</div>
<p><strong>Conditional Probability</strong>: The probability of even <span class="math inline">\(A\)</span> occurring under the restriction that <span class="math inline">\(B\)</span> is true is called the conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>. Denoted as <span class="math inline">\(P(A|B)\)</span>.</p>
<p>In general we define conditional probability (assuming <span class="math inline">\(P(B) \ne 0\)</span>) as
<span class="math display">\[P(A|B)=\frac{P(A\cap B)}{P(B)}\]</span>
which can also be rearranged to show
<span class="math display">\[\begin{aligned}
P(A\cap B)  &amp;=  P(A\,|\,B)\,P(B) \\
              &amp;=    P(B\,|\,A)\,P(A)
\end{aligned}\]</span></p>
<ul>
<li>Because the order doesn’t matter and <span class="math inline">\(P\left(A\cap B\right)=P\left(B\cap A\right)\)</span>.</li>
<li><span class="math inline">\(P(A|B) = 1\)</span> means that the event <span class="math inline">\(B\)</span> implies the event <span class="math inline">\(A\)</span>.</li>
<li><span class="math inline">\(P(A|B) = 0\)</span> means that the event <span class="math inline">\(B\)</span> excludes the possibility of event <span class="math inline">\(A\)</span>.</li>
</ul>
<p><strong>Independent:</strong> Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>independent</strong> if <span class="math inline">\(P(A\cap B)=P(A)P(B)\)</span>.</p>
<p>What independence is saying that knowing the outcome of event <span class="math inline">\(A\)</span> doesn’t give you any information about the outcome of event <span class="math inline">\(B\)</span>. <em>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent events, then <span class="math inline">\(P(A|B) = P(A)\)</span> and <span class="math inline">\(P(B|A) = P(B)\)</span>.</em></p>
<blockquote>
<p>In simple random sampling, we assume that any two samples are independent. In cluster sampling, we assume that samples within a cluster are not independent, but clusters are independent of each other.</p>
</blockquote>
<ul>
<li><p>Assumptions of independence and non-independence in statistical modelling are
important.</p></li>
<li><p>In linear regression, for example, correct standard error estimates rely on
independence amongst observations.</p></li>
<li><p>In analysis of clustered data, non-independence means that standard regression
techniques are problematic.</p></li>
</ul>
<p><strong>Bayes’ Rule:</strong> This arises naturally from the rule on conditional probability. Since the order does not matter in <span class="math inline">\(A \cap B\)</span>, we can rewrite the equation:</p>
<p><span class="math display">\[\begin{aligned} 
P(A \cap B) &amp;=  P(B \cap A) \\
P(A\mid B)P(B) &amp;=   P(B\mid A)P(A) \\
P(A\mid B) &amp;=   \frac{P(B\mid A)P(A) }{P(B)}\\
&amp;=  \frac{P(B\mid A)P(A) }{P(B\mid A)P(A) + P(B\mid A^C)P(A^C)}
\end{aligned}\]</span></p>
</div>
<div id="how-to-define-and-assign-probabilities-in-general" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> How to define and assign probabilities in general?</h3>
<ol style="list-style-type: decimal">
<li><strong>Frequency-type (Empirical Probability)</strong>: based on the idea of frequency or long-run frequency of an event.
<ul>
<li>Observe sequence of coin tosses (trials) and the count number of times of event <span class="math inline">\(A=\{H\}\)</span>. The relative frequency of <span class="math inline">\(A\)</span> is given as,
<span class="math display">\[ P(A) = \frac{\text{Number of times A occurs}}{\text{Total number of trials}}. \]</span></li>
<li>Trials are independent. Relative frequency is unpredictable in short-term, but in long-run it’s predicable. Let <span class="math inline">\(n\)</span> be the total number of trials and <span class="math inline">\(m\)</span> be number of heads, then we have <span class="math display">\[ P(A) = \lim_{n \rightarrow \infty} \frac{m}{n}.\]</span></li>
<li><strong>Theoretical Probability</strong>: Sometimes <span class="math inline">\(P(A)\)</span> can be deduced from mathematical model using uniform probability property on finite spaces.
<ul>
<li>If the sample space S is finite, then one possible probability measure on <span class="math inline">\(S\)</span> is the uniform probability measure, which assigns probability <span class="math inline">\(1/|S|\)</span>, <strong>equal probability to each outcome</strong>. Here |S| is the number of elements in the sample space <span class="math inline">\(S\)</span>. By additivity, it then follows that for any event <span class="math inline">\(A\)</span> we have,</li>
</ul></li>
</ul>
<span class="math display">\[ P(A) = \frac{|A|}{|S|} = \frac{\text{Number of outcomes in S that satisfy A}}{\text{Total number of outcomes in S} } \]</span>
<ul>
<li><strong>Long-term frequency</strong> It is natural to think of large sequences of similar events defining frequency-type probability if we allow the number of trials to be indefinitely large.</li>
</ul></li>
</ol>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="prob.html#cb24-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb24-2"><a href="prob.html#cb24-2" aria-hidden="true" tabindex="-1"></a>pHeads <span class="ot">=</span><span class="fl">0.5</span></span>
<span id="cb24-3"><a href="prob.html#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb24-4"><a href="prob.html#cb24-4" aria-hidden="true" tabindex="-1"></a>flipSequence <span class="ot">=</span> <span class="fu">sample</span>( <span class="at">x=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">prob =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">-</span>pHeads,pHeads),<span class="at">size=</span>n,<span class="at">replace=</span>T)</span>
<span id="cb24-5"><a href="prob.html#cb24-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-6"><a href="prob.html#cb24-6" aria-hidden="true" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">cumsum</span>(flipSequence)</span>
<span id="cb24-7"><a href="prob.html#cb24-7" aria-hidden="true" tabindex="-1"></a>n<span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span>n</span>
<span id="cb24-8"><a href="prob.html#cb24-8" aria-hidden="true" tabindex="-1"></a>runProp <span class="ot">=</span> r<span class="sc">/</span>n</span>
<span id="cb24-9"><a href="prob.html#cb24-9" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb24-10"><a href="prob.html#cb24-10" aria-hidden="true" tabindex="-1"></a>flip_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">run=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,<span class="at">prop=</span>runProp)</span>
<span id="cb24-11"><a href="prob.html#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="prob.html#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(flip_data,<span class="fu">aes</span>(<span class="at">x=</span>run,<span class="at">y=</span>prop,<span class="at">frame=</span>run)) <span class="sc">+</span> </span>
<span id="cb24-13"><a href="prob.html#cb24-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_path</span>()<span class="sc">+</span></span>
<span id="cb24-14"><a href="prob.html#cb24-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">1</span>,<span class="dv">1000</span>)<span class="sc">+</span><span class="fu">ylim</span>(<span class="fl">0.0</span>,<span class="fl">1.0</span>)<span class="sc">+</span></span>
<span id="cb24-15"><a href="prob.html#cb24-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.5</span>)<span class="sc">+</span></span>
<span id="cb24-16"><a href="prob.html#cb24-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Proportion of Heads&quot;</span>)<span class="sc">+</span></span>
<span id="cb24-17"><a href="prob.html#cb24-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Number of Flips&quot;</span>)<span class="sc">+</span></span>
<span id="cb24-18"><a href="prob.html#cb24-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-20"></span>
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-20-1.png" alt="Demonstrate law of large number with the coin tossing example" width="384" />
<p class="caption">
Figure 2.3: Demonstrate law of large number with the coin tossing example
</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li><strong>Belief-type (Subjective Probability)</strong>: based on the idea of degree of belief (weight of evidence) about an event where the scale is anchored at certain (=1) and impossible (=0).</li>
</ol>
<div class="workedexample">
<p><strong>Subjective Probability</strong>
Consider these events</p>
<ol style="list-style-type: decimal">
<li>We will have more than 100cm of snow this winter.</li>
<li>An asteroid is responsible for the extinction of the dinosaurs</li>
<li>Mammography reduces the rate of death from breast cancer in women over 50 by more than 10%</li>
<li>The 70 year old man in your office, just diagnosed with lung cancer, will live at least 5 years</li>
</ol>
<p>Can you think of them in terms of long-run frequencies?</p>
<ul>
<li>We cannot always think in terms of long-run frequencies</li>
<li>Also, we may not care about long-run frequencies</li>
<li>How are they relevant to this patient, this hypothesis?</li>
<li>Even where long-run frequency could apply, often there is no such data available</li>
<li>However, opinions are formed and beliefs exist</li>
</ul>
</div>
<ol start="3" style="list-style-type: decimal">
<li><strong>Which probability to use?</strong></li>
</ol>
<ul>
<li><p>For inference, the Bayesian approach relies mainly on the belief interpretation of probability.</p></li>
<li><p>The laws of probability can be derived from some basic axioms that do not rely on long-run frequencies.</p></li>
</ul>
<div class="workedexample">
<p><strong>The Monty Hall Problem Revisit</strong></p>
<blockquote>
<p>Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice?</p>
</blockquote>
<p><img src="images/Monty_open_door.png" class="center"></p>
<p>Before any door is picked, we assume all three doors are equal likely to have a car behind it (prior probability of 1/3). Without loss of generality, let assume we pick door 1 and Monty opens door 2 (goat behind).</p>
<ul>
<li><p><span class="math inline">\(P( \text{door 1 has car}) = P( \text{door 2 has car}) = P( \text{door 3 has car)} = \frac{1}{3}\)</span></p></li>
<li><p><span class="math inline">\(P( \text{Monty opens door 2} \mid \text{door 1 has car} ) = \frac{1}{2}\)</span></p></li>
<li><p><span class="math inline">\(P( \text{Monty opens door 2} \mid \text{door 2 has car} ) = 0\)</span></p></li>
<li><p><span class="math inline">\(P( \text{Monty opens door 2} \mid \text{door 3 has car} ) = 1\)</span></p></li>
</ul>
<p>We want to know these two probabilities: <span class="math inline">\(P( \text{door 1 has car} \mid \text{Monty opens door 2} )\)</span> and <span class="math inline">\(P( \text{door 3 has car} \mid \text{Monty opens door 2} )\)</span>. Simplify notations, we have</p>
<p><span class="math display">\[\begin{aligned} 
&amp; P( D_1 = \text{car} \mid \text{open } D_2 ) \\
&amp; = \frac{P( \text{open } D_2 \mid D_1 = \text{car} )P(D_1 = \text{car})}{P(\text{open } D_2)} \\
&amp; = \frac{P( \text{open } D_2 \mid D_1 = \text{car} )P(D_1 = \text{car})}{P( \text{open } D_2 \mid D_1 = \text{car}  )P(D_1 = \text{car}) + P(\text{open } D_2 \mid D_3 = \text{car})P(D_3 = \text{car})} \\
&amp; = \frac{\frac{1}{2} \times \frac{1}{3}}{\frac{1}{2} \times \frac{1}{3} + 1 \times \frac{1}{3}} = \frac{1}{3}
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned} 
&amp; P( D_3 = \text{car} \mid \text{open } D_2 ) \\
&amp; = \frac{P( \text{open } D_2 \mid D_3 = \text{car} )P(D_3 = \text{car})}{P(\text{open } D_2)} \\
&amp; = \frac{P( \text{open } D_2 \mid D_3 = \text{car} )P(D_3 = \text{car})}{P( \text{open } D_2 \mid D_3 = \text{car}  )P(D_3 = \text{car}) + P(\text{open } D_2 \mid D_1 = \text{car})P(D_1 = \text{car})} \\
&amp; = \frac{ 1 \times \frac{1}{3}}{ 1 \times \frac{1}{3} + \frac{1}{2} \times \frac{1}{3}} = \frac{2}{3}
\end{aligned}\]</span></p>
<p>Given we pick door 1 and Monty opens door 2 (goat behind), the probability of the car behind door 1 is 1/3 while the probability of the car behind door 3 is 2/3. Therefore, we should switch to door 3 for a better chance of winning.</p>
</div>
<div class="guidedexercise">
<p><strong>The ELISA test Problem Revisit</strong>
The ELISA test for HIV was widely used in the mid-1990s for screening blood donations. As with most medical diagnostic tests, the ELISA test is not perfect.</p>
<ul>
<li>If a person actually carries the HIV virus, experts estimate that this test gives a positive result 97.7% of the time. (This number is called the <em>sensitivity</em> of the test.)</li>
<li>If a person does not carry the HIV virus, ELISA gives a negative (correct) result 92.6% of the time (the <em>specificity</em> of the test).</li>
<li>Assume the estimated HIV prevalence at the time was 0.5% of (the <em>prior base rate</em>).</li>
</ul>
<p>Suppose a randomly selected individual <strong>tested positive</strong>, please calculate <span class="math inline">\(P(D^+ \mid T^+)\)</span> and <span class="math inline">\(P(D^- \mid T^-)\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
P(D^+ \mid T^+) &amp; = \frac{P(D^+ \text{and } T^+)}{P(T^+)}\\
&amp; =  \frac{\text{Chance of HIV positive and test positive}}{\text{Chance of test positive}}\\
&amp; = \frac{P(T^+ \mid D^+)P(D^+)}{P(T^+)} \\
&amp; =  \frac{P(T^+ \mid D^+)P(D^+)}{P(T^+ \mid D^+)P(D^+) + P(T^+ \mid D^-)P(D^-)} \\
&amp; = \frac{0.977 \times 0.005}{0.977 \times 0.005 + 0.074 
\times 0.995} = \frac{0.0049}{0.0785} = 0.0622
\end{aligned}\]</span></p>
</div>
<p><strong>Demonstrate calculation in R</strong></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="prob.html#cb25-1" aria-hidden="true" tabindex="-1"></a>hypothesis <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;Carries HIV&quot;</span>, <span class="st">&quot;Does not carry HIV&quot;</span>)</span>
<span id="cb25-2"><a href="prob.html#cb25-2" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.005</span>, <span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.005</span>)</span>
<span id="cb25-3"><a href="prob.html#cb25-3" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.977</span>, <span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.926</span>) <span class="co"># given positive test</span></span>
<span id="cb25-4"><a href="prob.html#cb25-4" aria-hidden="true" tabindex="-1"></a>product <span class="ot">=</span> prior <span class="sc">*</span> likelihood</span>
<span id="cb25-5"><a href="prob.html#cb25-5" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">=</span> product <span class="sc">/</span> <span class="fu">sum</span>(product)</span>
<span id="cb25-6"><a href="prob.html#cb25-6" aria-hidden="true" tabindex="-1"></a>bayes_table <span class="ot">=</span> <span class="fu">data.frame</span>(hypothesis,</span>
<span id="cb25-7"><a href="prob.html#cb25-7" aria-hidden="true" tabindex="-1"></a>                         prior,</span>
<span id="cb25-8"><a href="prob.html#cb25-8" aria-hidden="true" tabindex="-1"></a>                         likelihood,</span>
<span id="cb25-9"><a href="prob.html#cb25-9" aria-hidden="true" tabindex="-1"></a>                         product,</span>
<span id="cb25-10"><a href="prob.html#cb25-10" aria-hidden="true" tabindex="-1"></a>                         posterior) <span class="sc">%&gt;%</span></span>
<span id="cb25-11"><a href="prob.html#cb25-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">hypothesis =</span> <span class="st">&quot;Sum&quot;</span>,</span>
<span id="cb25-12"><a href="prob.html#cb25-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">prior =</span> <span class="fu">sum</span>(prior),</span>
<span id="cb25-13"><a href="prob.html#cb25-13" aria-hidden="true" tabindex="-1"></a>          <span class="at">likelihood =</span> <span class="cn">NA</span>,</span>
<span id="cb25-14"><a href="prob.html#cb25-14" aria-hidden="true" tabindex="-1"></a>          <span class="at">product =</span> <span class="fu">sum</span>(product),</span>
<span id="cb25-15"><a href="prob.html#cb25-15" aria-hidden="true" tabindex="-1"></a>          <span class="at">posterior =</span> <span class="fu">sum</span>(posterior))</span>
<span id="cb25-16"><a href="prob.html#cb25-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-17"><a href="prob.html#cb25-17" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(bayes_table, <span class="at">digits =</span> <span class="dv">4</span>, <span class="at">align =</span> <span class="st">&#39;r&#39;</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">hypothesis</th>
<th align="right">prior</th>
<th align="right">likelihood</th>
<th align="right">product</th>
<th align="right">posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Carries HIV</td>
<td align="right">0.005</td>
<td align="right">0.977</td>
<td align="right">0.0049</td>
<td align="right">0.0622</td>
</tr>
<tr class="even">
<td align="right">Does not carry HIV</td>
<td align="right">0.995</td>
<td align="right">0.074</td>
<td align="right">0.0736</td>
<td align="right">0.9378</td>
</tr>
<tr class="odd">
<td align="right">Sum</td>
<td align="right">1.000</td>
<td align="right">NA</td>
<td align="right">0.0785</td>
<td align="right">1.0000</td>
</tr>
</tbody>
</table>
<!-- **Why we should think like a Bayesian: [the Case of Sally Clark ](https://en.wikipedia.org/wiki/Sally_Clark)** [@mcgrayne2011theory] [@hill2004multiple] -->
<!-- Sally was convicted in 1999 for the lost of her two sons. However, the prosecution relied on a flawed statistics analysis. More details on this case can be found on [Wikipedia](https://en.wikipedia.org/wiki/Sally_Clark). -->
<!-- Suppose the chance/probability of one random infant die of sudden infant death syndrome (SIDS) at natural causes is about 1 in 2000 and the probability  of a second infant die of SIDS naturally in the same family was 1 in 500. We assume the shared household environment and genetic increases the chance of another SIDS incidence in the same family (the two events are treated as dependent). -->
<!-- Let $A$ denote the event that two children from the same family naturally died of SIDS. We have is \[ P(A) = \frac{1}{2000} \times \frac{1}{100} = \frac{1}{200,000}. \] This probability can be viewed as small and that $A$ is considered a rare event. A flawed conclusion can raise saying since this is a rare event, if we observe such event, we have to question whether the death is due to natural cause or by human. -->
<!-- Under Bayesian thinking, a rare event should be compared to other highly unlikely events. In this case, the question should be is the death more likely due to natural causes or a crime. Both events, death due to natural causes and death due to crime are rare, with the later even more unlikely. -->
<!-- Suppose the estimated probability of an individual SIDS death is 1 in 8500. The probability of two children dying from a family, treating two events as independent, is 1 in 73 million ($\frac{1}{8,500} \times \frac{1}{8,500} = \frac{1}{72,250,000}$). Giving this calculation, the chance of two SID death is considered extremely rare that can't happen by naturally and hence the month is responsible. -->
</div>
</div>
<div id="probability-distributions" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Probability Distributions</h2>
<p><strong>Random variables</strong> are used to describe events and their probability. The formal definition just means that a random variable describes how numbers are attached to each possible elementary outcome. Random variables can be divided into two types:</p>
<ul>
<li><p>Continuous Random Variables: the variable takes on numerical values and could,
in principle, take any of an uncountable number of values. These typically have values that cannot be counted, e.g. age, height, blood
pressure, etc.</p></li>
<li><p>Discrete Random Variables: the variable takes on one of small set of values (or only a countable number of outcomes).</p></li>
</ul>
<p>The value of a particular measurement can be thought of as being determined by chance, even though there may be no true randomness</p>
<ul>
<li>a patient’s survival time after surgery for prostate cancer</li>
<li>distance an elderly person walks in 6 minutes
<ul>
<li>known factors may give an expected value and variance</li>
<li>unknown (or unknowable factors) lead to uncertainty given this mean</li>
</ul></li>
<li>the component that appears random to the analyst is modelled as being random.</li>
</ul>
<p><strong>Probability distributions</strong></p>
<ul>
<li>The true distribution of most random variables (outcomes) is not known.</li>
<li>It is convenient to approximate the distribution by a function (probability
distribution) with a small number of parameters.</li>
<li>We focus our attention on these parameters, rather than the whole distribution.</li>
<li>Statistical modelling that is based on estimating the parameters of distributions is
called ‘parametric’ modelling.</li>
<li>All the Bayesian models we will look at are parametric models.</li>
</ul>
<blockquote>
<p>“All models are wrong but some are useful.”</p>
</blockquote>
<div class="fold s">

<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="prob.html#cb26-1" aria-hidden="true" tabindex="-1"></a>Distance <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">600</span></span>
<span id="cb26-2"><a href="prob.html#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="prob.html#cb26-3" aria-hidden="true" tabindex="-1"></a>makePlot <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">frac=</span><span class="fl">0.3</span>){</span>
<span id="cb26-4"><a href="prob.html#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="prob.html#cb26-5" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Distance=</span>Distance,</span>
<span id="cb26-6"><a href="prob.html#cb26-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">f1 =</span> <span class="fu">dnorm</span>(Distance,<span class="dv">240</span>,<span class="dv">80</span>),</span>
<span id="cb26-7"><a href="prob.html#cb26-7" aria-hidden="true" tabindex="-1"></a>                 <span class="at">f2 =</span> <span class="fu">dnorm</span>(Distance,<span class="dv">450</span>,<span class="dv">70</span>),</span>
<span id="cb26-8"><a href="prob.html#cb26-8" aria-hidden="true" tabindex="-1"></a>                 <span class="at">p =</span> <span class="fu">rep</span>(frac, <span class="fu">length</span>(Distance)))</span>
<span id="cb26-9"><a href="prob.html#cb26-9" aria-hidden="true" tabindex="-1"></a>  df<span class="sc">$</span>f <span class="ot">&lt;-</span> df<span class="sc">$</span>p<span class="sc">*</span>df<span class="sc">$</span>f1 <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span>df<span class="sc">$</span>p)<span class="sc">*</span>df<span class="sc">$</span>f2</span>
<span id="cb26-10"><a href="prob.html#cb26-10" aria-hidden="true" tabindex="-1"></a>  maxH <span class="ot">&lt;-</span> <span class="fu">max</span>(df<span class="sc">$</span>f)<span class="sc">*</span><span class="fl">1.05</span></span>
<span id="cb26-11"><a href="prob.html#cb26-11" aria-hidden="true" tabindex="-1"></a>  p0 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(Distance,f))<span class="sc">+</span></span>
<span id="cb26-12"><a href="prob.html#cb26-12" aria-hidden="true" tabindex="-1"></a>         <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>)<span class="sc">+</span></span>
<span id="cb26-13"><a href="prob.html#cb26-13" aria-hidden="true" tabindex="-1"></a>         <span class="fu">ylab</span>(<span class="st">&quot;p(Distance)&quot;</span>)<span class="sc">+</span></span>
<span id="cb26-14"><a href="prob.html#cb26-14" aria-hidden="true" tabindex="-1"></a>         <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">600</span>))<span class="sc">+</span></span>
<span id="cb26-15"><a href="prob.html#cb26-15" aria-hidden="true" tabindex="-1"></a>         <span class="fu">theme_bw</span>()</span>
<span id="cb26-16"><a href="prob.html#cb26-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb26-17"><a href="prob.html#cb26-17" aria-hidden="true" tabindex="-1"></a>  p1 <span class="ot">&lt;-</span> p0<span class="sc">+</span><span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>,</span>
<span id="cb26-18"><a href="prob.html#cb26-18" aria-hidden="true" tabindex="-1"></a>                    <span class="at">x=</span><span class="dv">200</span>,</span>
<span id="cb26-19"><a href="prob.html#cb26-19" aria-hidden="true" tabindex="-1"></a>                    <span class="at">y=</span>maxH,</span>
<span id="cb26-20"><a href="prob.html#cb26-20" aria-hidden="true" tabindex="-1"></a>                    <span class="at">label=</span><span class="st">&quot;D ~ p(theta)&quot;</span>,</span>
<span id="cb26-21"><a href="prob.html#cb26-21" aria-hidden="true" tabindex="-1"></a>                    <span class="at">col=</span><span class="st">&quot;red&quot;</span>,</span>
<span id="cb26-22"><a href="prob.html#cb26-22" aria-hidden="true" tabindex="-1"></a>                    <span class="at">size=</span><span class="fl">3.5</span>)</span>
<span id="cb26-23"><a href="prob.html#cb26-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-24"><a href="prob.html#cb26-24" aria-hidden="true" tabindex="-1"></a>  p2 <span class="ot">&lt;-</span> p0 <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">data=</span>df, <span class="fu">aes</span>(Distance,p<span class="sc">*</span>f1),<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>)<span class="sc">+</span></span>
<span id="cb26-25"><a href="prob.html#cb26-25" aria-hidden="true" tabindex="-1"></a>     <span class="fu">geom_line</span>(<span class="at">data=</span>df, <span class="fu">aes</span>(Distance,(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">*</span>f2),<span class="at">col=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span></span>
<span id="cb26-26"><a href="prob.html#cb26-26" aria-hidden="true" tabindex="-1"></a>     <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>,<span class="at">x=</span><span class="dv">50</span>,<span class="at">y=</span>maxH<span class="sc">*</span><span class="fl">1.05</span>,<span class="at">hjust=</span><span class="dv">0</span>,</span>
<span id="cb26-27"><a href="prob.html#cb26-27" aria-hidden="true" tabindex="-1"></a>              <span class="at">label=</span><span class="fu">paste0</span>(<span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span>frac),</span>
<span id="cb26-28"><a href="prob.html#cb26-28" aria-hidden="true" tabindex="-1"></a>                           <span class="st">&quot;%: D | Hospitalized ~ N(mu1, V1)&quot;</span>),</span>
<span id="cb26-29"><a href="prob.html#cb26-29" aria-hidden="true" tabindex="-1"></a>              <span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">size=</span><span class="fl">3.5</span>)<span class="sc">+</span></span>
<span id="cb26-30"><a href="prob.html#cb26-30" aria-hidden="true" tabindex="-1"></a>    <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>,<span class="at">x=</span><span class="dv">50</span>,<span class="at">y=</span>maxH<span class="sc">*</span><span class="fl">1.1</span>,<span class="at">hjust=</span><span class="dv">0</span>,</span>
<span id="cb26-31"><a href="prob.html#cb26-31" aria-hidden="true" tabindex="-1"></a>         <span class="at">label=</span><span class="fu">paste0</span>(<span class="fu">round</span>(<span class="dv">100-100</span><span class="sc">*</span>frac),</span>
<span id="cb26-32"><a href="prob.html#cb26-32" aria-hidden="true" tabindex="-1"></a>                      <span class="st">&quot;%: D | Not Hospitalized ~ N(mu1+delta, V2)&quot;</span>),</span>
<span id="cb26-33"><a href="prob.html#cb26-33" aria-hidden="true" tabindex="-1"></a>                      <span class="at">col=</span><span class="st">&quot;orange&quot;</span>,<span class="at">size=</span><span class="fl">3.5</span>)</span>
<span id="cb26-34"><a href="prob.html#cb26-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-35"><a href="prob.html#cb26-35" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggarrange</span>(p1, p2,  <span class="at">ncol =</span> <span class="dv">1</span>, <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb26-36"><a href="prob.html#cb26-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-37"><a href="prob.html#cb26-37" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb26-38"><a href="prob.html#cb26-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-39"><a href="prob.html#cb26-39" aria-hidden="true" tabindex="-1"></a><span class="fu">makePlot</span>(<span class="at">frac =</span> <span class="fl">0.3</span>)</span></code></pre></div>
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-22-1.png" width="384" style="display: block; margin: auto;" />
<div>

<div id="probability-density-functions" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Probability density functions</h3>
<p>This is a function with two important properties</p>
<ul>
<li><p>The heights of the function at two different x-values indicate relatively how likely those x-values are.</p></li>
<li><p>The area under the curve between any two x-values is the probability that a randomly sampled value will fall in that range</p></li>
<li><p>The area under the whole curve is equal to the total probability, i.e. equal to 1</p></li>
</ul>
<div class="workedexample">
<p><strong>From histograms to PDFs</strong>
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-23-1.png" width="672" /><img src="bayes_bookdown_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<ul>
<li><p>With a large enough sample, and narrow enough intervals, histogram starts to look smooth.</p></li>
<li><p>The relative heights of the histogram at each blood pressure value tell us how likely these values are.</p></li>
<li><p>For example, near 120 mm Hg there are about 800 people in each 5 mm interval.</p></li>
<li><p>As the width of the interval approaches zero, the probability of being in that interval approaches zero. e.g. The event that “BP is exactly 129” has zero probability, with <span class="math display">\[P(BP = 129) \approx P(128.99999 &lt; BP &lt; 129.00001) \approx 0.\]</span></p></li>
<li><p>For a continuous random variable, we describe the shape of the distribution with a smooth curve (pdf) instead of a histogram.</p></li>
</ul>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<ul>
<li><p>The relative height of the curve at any two points indicates the relative likelihood of the two values. The height is not a probability.</p></li>
<li><p>The area under the curve between any two points is the probability that a randomly sampled value will fall in that range. e.g., the shaded area represent = 0.5. In fact, the probability of a BP from this population being between 112 and 145 is 0.5.</p></li>
</ul>
</div>
<div class="important">
<p><strong>Useful Probability Distributions</strong></p>
<ul>
<li><p><strong>Discrete random variables and distributions</strong> means that the random variable can take on only a countable number of values. We will describe these distributions using probability mass function.</p>
<ul>
<li>Binomial</li>
<li>Multinomial</li>
<li>Poisson</li>
<li>Negative binomial</li>
</ul></li>
<li><p><strong>Continuous random variables and distributions</strong>: We will describe these distributions using probability density function.</p>
<ul>
<li>Uniform</li>
<li>Normal</li>
<li>Beta</li>
<li>Gamma</li>
</ul></li>
<li><p>The expectation for discrete distributions
<span class="math display">\[E[X]= \sum_{x=0}^{n}x\,P(X=x) \]</span></p></li>
<li><p>and the variance for discrete distributions
<span class="math display">\[V[X]=\sum_{x=0}^{n}\left(x-E\left[X\right]\right)^{2}\,P(X=x) \]</span></p></li>
<li><p>The expectation for continuous distributions
<span class="math display">\[E[X]= \int_{a}^{b}x\,f(x) dx\]</span></p></li>
<li><p>and the variance for continuous distributions <span class="math display">\[V[X]= E[(X-E(X))^2] = E(X^2) - E(X)^2\]</span> and <span class="math display">\[E[X^2]= \int_{a}^{b}x^2\,f(x) dx\]</span></p></li>
</ul>
</div>
</div>
<div id="discrete-distributions" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Discrete Distributions</h3>
<div id="bernoulli-distribution" class="section level4" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> Bernoulli Distribution</h4>
<ul>
<li>A single binary outcome Y can be represented as taking on the values 0 or 1.</li>
<li>Of course, this could be success/failure, return/did not return, etc.</li>
<li>There is a single parameter - the probability that the variable takes on the value 1 (probability of success), denoted as <span class="math inline">\(p\)</span>.</li>
<li><span class="math inline">\(P(Y=1) = p\)</span> and <span class="math inline">\(P(Y=0) = 1 - p\)</span></li>
<li>The probability mass function is <span class="math display">\[P(Y=y) = p^y (1-p)^{1-y}, y \in \{0,1 \}\]</span></li>
</ul>
<blockquote>
<p>Daniel Bernoulli FRS (1700-1782) was a Swiss mathematician and physicist and was one of the many prominent mathematicians in the Bernoulli family from Basel.</p>
</blockquote>
</div>
<div id="binomial-distribution" class="section level4" number="2.2.2.2">
<h4><span class="header-section-number">2.2.2.2</span> Binomial Distribution</h4>
<ul>
<li>One of the most important discrete distribution used in statistics is the <em>binomial distribution</em>.</li>
<li>This is the distribution which counts the number of heads in <span class="math inline">\(n\)</span> independent coin tosses where each individual coin toss has the probability <span class="math inline">\(p\)</span> of being a head.</li>
<li>The same distribution is useful when not just in tossing coins, for example, when taking random samples from a disease cohort and interested in the number of patients in this sample that received surgery.</li>
</ul>
<p><strong>A binomial experiment is one that</strong>:</p>
<ol style="list-style-type: decimal">
<li>Experiment consists of <span class="math inline">\(n\)</span> identical trials.</li>
<li>Each trial results in one of two outcomes (Heads/Tails, presence/absence).
will be labelled a success and the other a failure.</li>
<li>The probability of success on a single trial is equal to <span class="math inline">\(p\)</span> and remains the
same from trial to trial.</li>
<li>The trials are independent (this is implied from property 3).</li>
<li>The random variable <span class="math inline">\(Y\)</span> is the number of successes observed during <span class="math inline">\(n\)</span> trials.</li>
</ol>
<p><strong>The Binomial Probability Mass Function</strong></p>
<p>The probability mass function for the binomial distribution is
<span class="math display">\[
P(Y=y) = \binom{n}{y} p^y(1-p)^{n-y}, \ \text{for } y=0,\ldots,n
\]</span>
where <span class="math inline">\(n\)</span> is a positive integer and <span class="math inline">\(p\)</span> is a probability between 0 and 1.</p>
<p>This probability mass function uses the expression <span class="math inline">\(\binom{n}{y}\)</span>,
called a binomial coefficient
<span class="math display">\[
\binom{n}{y} = \frac{n!}{y!(n-y)!}
\]</span>
which counts the number of ways to choose <span class="math inline">\(x\)</span> things from <span class="math inline">\(n\)</span>.</p>
<p>The mean of the binomial distribution is <span class="math inline">\(E(Y) = np\)</span> and the variance is <span class="math inline">\(Var(Y) = np(1-p)\)</span>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="prob.html#cb27-1" aria-hidden="true" tabindex="-1"></a>dist <span class="ot">&lt;-</span> <span class="fu">data.frame</span>( <span class="at">x=</span><span class="dv">0</span><span class="sc">:</span><span class="dv">10</span> ) <span class="sc">%&gt;%</span> </span>
<span id="cb27-2"><a href="prob.html#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">probability =</span> <span class="fu">dbinom</span>(x, <span class="at">size=</span><span class="dv">10</span>, <span class="at">prob=</span><span class="fl">0.5</span>))</span>
<span id="cb27-3"><a href="prob.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dist, <span class="fu">aes</span>(<span class="at">x=</span>x)) <span class="sc">+</span></span>
<span id="cb27-4"><a href="prob.html#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y=</span>probability)) <span class="sc">+</span></span>
<span id="cb27-5"><a href="prob.html#cb27-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="fu">aes</span>(<span class="at">ymax=</span>probability, <span class="at">ymin=</span><span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb27-6"><a href="prob.html#cb27-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">1</span>))<span class="sc">+</span></span>
<span id="cb27-7"><a href="prob.html#cb27-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&#39;Binomial distribution: n=10, p=0.5&#39;</span>) <span class="sc">+</span></span>
<span id="cb27-8"><a href="prob.html#cb27-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>We can use R to calculate binomial probabilities. In general, for any distribution,
the “d-function” gives the distribution function (pmf or pdf). <span class="math inline">\(P( x = 5 | n=10, p=0.8 ) = 0.02642412\)</span> from <code>dbinom(5, size=10, prob=0.8)</code> and <span class="math inline">\(P( x = 8 | n=10, p=0.8 )=0.3019899\)</span> from <code>dbinom(8, size=10, prob=0.8)</code>.</p>
</div>
<div id="multinomial-distribution" class="section level4" number="2.2.2.3">
<h4><span class="header-section-number">2.2.2.3</span> Multinomial Distribution</h4>
<ul>
<li>If a random variable <span class="math inline">\(Y\)</span> can take on one of <span class="math inline">\(K\)</span> categories and the probability of each category <span class="math inline">\(k\)</span> is given by the set of probabilities <span class="math inline">\(p_1, p_2, \ldots, p_K\)</span> and <span class="math inline">\(\sum_{k=1}^K p_k = 1\)</span>.</li>
<li>The probability that the single observation <span class="math inline">\(Y\)</span> is in category <span class="math inline">\(k\)</span> is given by <span class="math inline">\(P(Y=k) = p_k\)</span>.</li>
<li>With <span class="math inline">\(n\)</span> observations, we are interested in the number that fall into each of the <span class="math inline">\(k\)</span> categories. The random variable <span class="math inline">\(Y\)</span>, thus follows a multinomial distribution.</li>
</ul>
<p><strong>The multinomial Probability Mass Function</strong></p>
<p>The probability mass function for the multinomial distribution is</p>
<p><span class="math display">\[P(y_1, y_2, \ldots, y_K) = \frac{n! }{y_1!, y_2!, \ldots, y_K!} p_1^{y_1} p_2^{y_2} \cdots p_K^{y_K}, y_j= 0, \ldots, K, j = 1, \ldots, K \ and \ \sum y_j = K.  \]</span></p>
<ul>
<li>This is a useful distribution to use as a starting point for modelling categorical data with a relatively small number (&gt; 2) of possible values.</li>
</ul>
<p><strong>Example</strong> Suppose our outcome (random variable) is severity level</p>
<ul>
<li>This can take on the values (1) none/mild ; (2) moderate; and (3) severe.</li>
<li>Let the probabilities for the three categories be <span class="math inline">\(p_1\)</span>,<span class="math inline">\(p_2\)</span>, and <span class="math inline">\(p_3\)</span>.
<span class="math display">\[P(Y = 1) = p_1; \ P(Y = 2) = p_2;\ P(Y = 3) = p_3 = 1 - (p_1+p_2);\]</span></li>
<li>if we observe 100 subjects, with 20, 65 and 15 in the three categories,</li>
</ul>
<p><span class="math display">\[ P(20,65,15) = \frac{100! }{20! 65!15!} p_1^{20} p_2^{65} p_3^{16}\]</span></p>
<ul>
<li>if we assume <span class="math inline">\(p_1=0.2\)</span>, <span class="math inline">\(p_2 = 0.6\)</span>, <span class="math inline">\(p_3 = 0.2\)</span>, the above probability is 0.004644407 from R with <code>dmultinom(x=c(20,65,15),prob=c(0.2,0.6,0.2))</code>.</li>
</ul>
</div>
<div id="poisson-distribution" class="section level4" number="2.2.2.4">
<h4><span class="header-section-number">2.2.2.4</span> Poisson Distribution</h4>
<p>A commonly used distribution for count data is the Poisson. e.g., number of patients arriving at ER over a 12 hr interval.</p>
<p>The following conditions apply:</p>
<ol style="list-style-type: decimal">
<li>Two or more events do not occur at precisely the same time or in the same space</li>
<li>The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a non overlapping period or region.</li>
<li>The expected number of events during one period , <span class="math inline">\(\lambda\)</span>, is the same in all periods or regions of the same size.</li>
</ol>
<p>Assuming that these conditions hold for some count variable <span class="math inline">\(Y\)</span>, the the
probability mass function is given by
<span class="math display">\[P(Y=y)=\frac{\lambda^{y}e^{-\lambda}}{y!}, y = 0, 1, \ldots\]</span>
where <span class="math inline">\(\lambda\)</span> is the expected number of events over 1 unit of time or space and <span class="math inline">\(e\)</span> is the euler’s number <span class="math inline">\(2.718281828\dots\)</span>.</p>
<p><span class="math display">\[E[Y]  =   \lambda\]</span>
<span class="math display">\[Var[Y]    =   \lambda\]</span></p>
<p><strong>Example:</strong> Suppose we are interested in the population size of small mammals in
a region. Let <span class="math inline">\(Y\)</span> be the number of small mammals caught in a large trap over a 12
hour period. Finally, suppose that <span class="math inline">\(Y\sim Poisson(\lambda=2.3)\)</span>. What is the
probability of finding exactly 4 critters in our trap?
<span class="math display">\[P(Y=4)  =   \frac{2.3^{4}\,e^{-2.3}}{4!} =  0.1169\]</span>
What about the probability of finding at most 4?
<span class="math display">\[\begin{aligned} P(Y\le4)    
&amp;=  P(Y=0)+P(Y=1)+P(Y=2)+P(Y=3)+P(Y=4) \\
&amp;=  0.1003+0.2306+0.2652+0.2033+0.1169 \\
&amp;=  0.9163 \end{aligned}\]</span></p>
<p>What about the probability of finding 5 or more?
<span class="math display">\[P(Y\ge5)    =   1-P(Y\le4) =    1-0.9163 =  0.0837\]</span></p>
<p>These calculations can be done using the distribution function (d-function) for
the Poisson and the cumulative distribution function (p-function). e.g. <span class="math inline">\(P(Y=4\mid \lambda = 2.3\)</span> can be obtained using <code>dpois(4, lambda=2.3)</code> and <span class="math inline">\(P(Y \leq 4\mid \lambda = 2.3\)</span> can be obtained using <code>ppois(4, lambda=2.3)</code>.</p>
<!-- <div class="fold s"> -->
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-28-1.png" width="672" />
<!-- <div> --></p>
</div>
<div id="negative-binomial-distribution" class="section level4" number="2.2.2.5">
<h4><span class="header-section-number">2.2.2.5</span> Negative Binomial Distribution</h4>
<ul>
<li><p>If <span class="math inline">\(Y\)</span> is the count of independent Bernoulli trials required to achieve the <span class="math inline">\(r\)</span>th successful trial (<span class="math inline">\(k\)</span> failures) when the probability of success is <span class="math inline">\(p\)</span>.</p></li>
<li><p><span class="math inline">\(Y \sim NB(r, p)\)</span> and the probability mass function is</p></li>
</ul>
<p><span class="math display">\[ f(Y = k) = \frac{\Gamma(k+r)}{k! \Gamma(r)} (1-p)^y (p)^{r}, \ \text{for } y=0,1,\ldots \]</span></p>
<ul>
<li><p><span class="math inline">\(E(Y) = \frac{pr}{1-p}\)</span> and <span class="math inline">\(V(Y) = \frac{pr}{(1-p)^2}\)</span></p></li>
<li><p><strong>Connection between Poisson and Negative Binomial</strong>.</p>
<ul>
<li><p>Let <span class="math inline">\(\lambda = E(Y) = \frac{pr}{1-p}\)</span>, we thus have <span class="math inline">\(p=\frac{\lambda}{r+\lambda}\)</span> and <span class="math inline">\(V(Y) = \lambda(1 + \frac{\lambda}{r}) &gt; \lambda\)</span>.</p></li>
<li><p><strong>The variance of the NB is greater than its mean, where as the variance of Poisson equals to its mean!</strong></p></li>
<li><p>When we fit a Poisson regression and encounter over-dispersion - the observed variance of the data is greater than the variance of the Poisson model, we can fit a Negative Binomial regression instead!</p></li>
</ul></li>
</ul>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
</div>
<div id="continous-distributions" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Continous Distributions</h3>
<div id="uniform-distributions" class="section level4" number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> Uniform Distributions</h4>
<p>Suppose you wish to draw a random number number between 0 and 1 and any two
intervals of equal size should have the same probability of the value being in
them. This random variable is said to have a Uniform(0,1) distribution.</p>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<ul>
<li>The uniform distribution has two parameter, a lower bound (a) and an upper
bound (b)</li>
<li>The pdf is a constant - no value is any more likely than any other
<span class="math display">\[p(x \mid a, b) = \frac{1}{b-a}, b&gt;a, a \leq x \leq b\]</span></li>
<li>Mean = <span class="math inline">\(\frac{a+b}{2}\)</span> and variance = <span class="math inline">\(\frac{(b-a)^2}{12}\)</span></li>
</ul>
</div>
<div id="normal-distributions" class="section level4" number="2.2.3.2">
<h4><span class="header-section-number">2.2.3.2</span> Normal Distributions</h4>
<p>Undoubtedly the most important distribution in statistics is the normal
distribution. Normal Distributions has two parameters: mean <span class="math inline">\(\mu\)</span> and standard
deviation <span class="math inline">\(\sigma\)</span>. Its probability density function is given by
<span class="math display">\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right]\]</span>
where <span class="math inline">\(\exp[y]\)</span> is the exponential function <span class="math inline">\(e^{y}\)</span>. We could slightly rearrange
the function to</p>
<p><span class="math display">\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right]\]</span></p>
<p>and see this distribution is defined by its expectation <span class="math inline">\(E[X]=\mu\)</span> and its
variance <span class="math inline">\(Var[X]=\sigma^{2}\)</span>.</p>
<div class="fold s">
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-32-1.png" width="672" />
<div>

<p><strong>Example:</strong> It is known that the heights of adult males in the US is
approximately normal with a mean of 5 feet 10 inches (<span class="math inline">\(\mu=70\)</span> inches) and a
standard deviation of <span class="math inline">\(\sigma=3\)</span> inches. I am 5 feet 4 inches (64 inches). What proportion of the adult male population is shorter than me?</p>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Using R you can easily find this probability is 0.02275013 using <code>pnorm(64, mean=70, sd=3)</code>.</p>
</div>
<div id="beta-distributions" class="section level4" number="2.2.3.3">
<h4><span class="header-section-number">2.2.3.3</span> Beta Distributions</h4>
<ul>
<li>Beta distribution is denoted as <span class="math inline">\(Beta(\alpha, \beta)\)</span>, with <span class="math inline">\(\alpha&gt;0\)</span> (shape parameter 1) and <span class="math inline">\(\beta&gt;0\)</span> (shape parameter 2)</li>
<li>We can interpret <span class="math inline">\(\alpha\)</span> as the number of pseudo events and <span class="math inline">\(\beta\)</span> as the number of pseudo
non-events, so that <span class="math inline">\(\alpha + \beta\)</span> is the pseudo sample size</li>
<li>If the random variable follows a beta distribution, its value is bounded between 0 and 1, making this distribution a candidate distribution to model proportion. e.g. proportion of patients received surgery.</li>
<li>mean = <span class="math inline">\(\frac{\alpha}{\alpha+\beta} = \frac{\text(events)}{\text(sample size)} = p\)</span></li>
<li>variance = <span class="math display">\[ \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} = \frac{\alpha}{\alpha+\beta} \times \frac{\beta}{\alpha+\beta} \times \frac{1}{\alpha+\beta+1} = p(1-p)\frac{1}{\text(sample size)+1} \]</span></li>
</ul>
<p>The probability density function
<span class="math display">\[ f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1} (1-x)^{\beta - 1}, 0 \leq x \leq 1.\]</span>
where <span class="math display">\[ B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}\]</span> and <span class="math inline">\(\Gamma()\)</span> is called <a href="https://en.wikipedia.org/wiki/Gamma_function">the gamma function</a>.</p>
<p><img src="bayes_bookdown_files/figure-html/unnamed-chunk-35-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="gamma-distributions" class="section level4" number="2.2.3.4">
<h4><span class="header-section-number">2.2.3.4</span> Gamma Distributions</h4>
<ul>
<li>Gamma distribution is denoted as <span class="math inline">\(Gamma(\alpha, \beta)\)</span>, with <span class="math inline">\(\alpha&gt;0\)</span> (shape parameter) and <span class="math inline">\(\beta&gt;0\)</span> (rate parameter)</li>
<li>If the random variable follows a Gamma distribution, it can have values larger than 0</li>
<li>mean = <span class="math inline">\(\frac{\alpha}{\beta}\)</span> and variance = <span class="math inline">\(\frac{\alpha}{\beta^2}\)</span></li>
</ul>
<p>The probability density function
<span class="math display">\[f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x},x \geq 0.\]</span></p>
<div class="fold s">
<img src="bayes_bookdown_files/figure-html/unnamed-chunk-36-1.png" width="672" style="display: block; margin: auto;" />
<div>


<div class="important">
<p><strong>Role of these distributions</strong></p>
<ul>
<li><p>All of them are potentially distributions for observed outcomes. Similar choices as in non-Bayesian modelling</p></li>
<li><p>A select group of continuous distributions are frequently used to represent prior
distributions for parameters</p>
<ul>
<li>Normal - for regression parameters</li>
<li>Beta - for proportions, other bounded parameters</li>
<li>Uniform - for proportions, standard deviations</li>
<li>Gamma - for rates, variances</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="r-session-information-1" class="section level3 unnumbered">
<h3>R Session information</h3>
<pre><code>## R version 4.0.5 (2021-03-31)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19042)
## 
## Matrix products: 
## 
## locale:
## [1] LC_COLLATE=English_Canada.1252  LC_CTYPE=English_Canada.1252   
## [3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C                   
## [5] LC_TIME=English_Canada.1252    
## 
## attached base packages:
## [1] grid      stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] truncnorm_1.0-8     ggpubr_0.4.0        tweenr_1.0.2       
##  [4] gganimate_1.0.7     VennDiagram_1.7.1   futile.logger_1.4.3
##  [7] brms_2.16.3         Rcpp_1.0.7          forcats_0.5.1      
## [10] stringr_1.4.0       dplyr_1.0.7         purrr_0.3.4        
## [13] readr_2.1.1         tidyr_1.1.4         tibble_3.1.6       
## [16] ggplot2_3.3.5       tidyverse_1.3.1</code></pre>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-cardinal2019sets" class="csl-entry">
Cardinal, J Scott. 2019. <span>“Sets, Graphs, and Things We Can See: A Formal Combinatorial Ontology for Empirical Intra-Site Analysis.”</span> <em>Journal of Computer Applications in Archaeology</em> 2 (1).
</div>
<div id="ref-evans2004probability" class="csl-entry">
Evans, Michael J, and Jeffrey S Rosenthal. 2004. <em>Probability and Statistics: The Science of Uncertainty</em>. Macmillan.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lab1-getting-started-with-r-rstudio.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": "twitter"
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
