[["prob.html", "Session 2 Probability, random variables and distributions 2.1 Probability 2.2 Random Variables 2.3 R Quick Reference", " Session 2 Probability, random variables and distributions Review of probability terminologies, probability rules, and Venn Diagrams Review conditional probability, independence, and Bayes theorem. Review on random variables and common probability distributions Probability Terminology 2.1 Probability Probability has a central place in Bayesian analysis we put a prior probability distribution on the unknowns (parameters), we model the observed data with a probability distribution (likelihood), and we combine the two into a posterior probability distribution Probability Terminology(Evans and Rosenthal 2004) Sample space This set of all possible outcomes of an experiment/trial is known as the sample space of the experiment/trial and is denoted by \\(S\\) or \\(\\Omega\\). Experiment/Trial: each occasion we observe a random phenomenon that we know what outcomes can occur, but we do not know which outcome will occur Event: Any subset of the sample space \\(S\\) is known as an event, denoted by \\(A\\). Note that, \\(A\\) is also a collection of one or more outcomes. Probability defined on events: For each event \\(A\\) of the sample space \\(S\\), \\(P(A)\\), the probability of the event \\(A\\), satisfies the following three conditions: \\(0 \\leq P(A) \\leq 1\\) \\(P(S) = 1\\) and \\(P(\\emptyset) = 0\\); \\(\\emptyset\\) denotes the empty set \\(P\\) is (countably) additive, meaning that if \\(A_1, A_2, \\ldots\\) is a finite or countable sequence of disjoint (also known as mutually exclusive events), then \\[P(A_1 \\cup A_2 \\cup \\ldots ) = P(A_1)+P(A_2)+\\ldots \\] Union: Denote the event that either \\(A\\) or \\(B\\) occurs as \\(A\\cup B\\). Intersection: Denote the event that both \\(A\\) and \\(B\\) occur as \\(A\\cap B\\) Complement: Denote the event that \\(A\\) does not occur as \\(\\bar{A}\\) or \\(A^{C}\\) or \\(A^\\prime\\) (different people use different notations) Disjoint (or mutually exclusive): Two events \\(A\\) and \\(B\\) are said to be disjoint if the occurrence of one event precludes the occurrence of the other. If two events are mutually exclusive, then \\(P(A\\cup B)=P(A)+P(B)\\) Sample Space If the experiment consists of the flipping of a coin, then \\[ S = \\{H, T\\}\\] If the experiment consists of rolling a die, then the sample space is \\[ S = \\{1, 2, 3, 4, 5, 6\\}\\] If the experiments consists of flipping two coins, then the sample space consists of the following four points: \\[ S = \\{(H,H), (H,T), (T,H), (T,T)\\}\\] Event In Example (1), if E = {H}, then E is the event that a head appears on the flip of the coin. Similarly, if \\(E = \\{T \\}\\), then \\(E\\) would be the event that a tail appears In Example (2), if \\(E = \\{1\\}\\), then \\(E\\) is the event that one appears on the roll of the die. If \\(E = \\{2, 4, 6\\}\\), then \\(E\\) would be the event that an even number appears on the roll. In Example (3), if \\(E = \\{(H, H), (H, T )\\}\\), then \\(E\\) is the event that a head appears on the first coin. Probability of an event. Let \\(R\\) be the sum of two standard dice. Suppose we are interested in \\(P(R \\le 4)\\). Notice that the pair of dice can fall 36 different ways (6 ways for the first die and six for the second results in 6x6 possible outcomes, and each way has equal probability \\(1/36\\). \\[\\begin{aligned} P(R \\le 4 ) &amp;= P(R=2)+P(R=3)+P(R=4) \\\\ &amp;= P(\\left\\{ 1,1\\right\\} )+P(\\left\\{ 1,2\\right\\} \\mathrm{\\textrm{ or }}\\left\\{ 2,1\\right\\} )+P(\\{1,3\\}\\textrm{ or }\\{2,2\\}\\textrm{ or }\\{3,1\\}) \\\\ &amp;= \\frac{1}{36}+\\frac{2}{36}+\\frac{3}{36} \\\\ &amp;= \\frac{6}{36} \\\\ &amp;= \\frac{1}{6} \\end{aligned}\\] 2.1.1 Venn Diagrams Venn diagrams provide a very useful graphical method for depicting the sample space S and subsets of it. Figure taken from (Cardinal 2019). Figure 2.1: Venn Diagram for two events Venn Diagram for two disjoint events How would you draw this venn diagram? Label sub-regions in a Venn Diagram for three events Using set theory, how would you write out areas a, d, and f ? A &lt;- c(&quot;a&quot;,&quot;b&quot;, &quot;d&quot;,&quot;e&quot;) B &lt;- c(&quot;b&quot;,&quot;c&quot;, &quot;d&quot;,&quot;f&quot;) C &lt;- c(&quot;d&quot;,&quot;e&quot;,&quot;f&quot;,&quot;g&quot;) x &lt;- list(A=A , B=B , C=C) v0 &lt;- venn.diagram( x, filename=NULL, fill = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;), alpha = 0.50, col = &quot;transparent&quot;) overlaps &lt;- calculate.overlap(x) # extract indexes of overlaps from list names indx &lt;- as.numeric(substr(names(overlaps),2,2)) # labels start at position 7 in the list for Venn&#39;s with 3 circles for (i in 1:length(overlaps)){ v0[[6 + indx[i] ]]$label &lt;- paste(overlaps[[i]], collapse = &quot;\\n&quot;) } grid.draw(v0) 2.1.2 Probability Rules General Addition Rule: \\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\) The reason behind this fact is that if there is if \\(A\\) and \\(B\\) are not disjoint, then some area is added twice when we calculate \\(P\\left(A\\right)+P\\left(B\\right)\\). To account for this, we simply subtract off the area that was double counted. Complement Rule: \\(P(A)+P(A^c)=1\\) This rule follows from the partitioning of the set of all events (\\(S\\)) into two disjoint sets, \\(A\\) and \\(A^c\\). We learned above that \\(A \\cup A^c = S\\) and that \\(P(S) = 1\\). Combining those statements, we obtain the complement rule. Completeness Rule: \\(P(A)=P(A\\cap B)+P(A\\cap B^c)\\) This identity is just breaking the event \\(A\\) into two disjoint pieces. Law of total probability (unconditioned version): Let \\(A_1, A_2, \\ldots\\) be events that form a partition of the sample space \\(S\\), Let \\(B\\) be any event. Then, \\[P(B) = P(A_1 \\cap B) + P(A_2 \\cap B) + \\ldots. \\] This law is key in deriving the marginal event probability in Bayes rule. Recall the HIV example from session 1, we have \\(P(T^+) = P(T^+ \\cap D^+)+P(T^+ \\cap D^-)\\). Figure 2.2: Demonstrate Law of total probabiliy using Venn Diagram Conditional Probability: In general we define conditional probability (assuming \\(P(B) \\ne 0\\)) as \\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\] which can also be rearranged to show \\[\\begin{aligned} P(A\\cap B) &amp;= P(A\\,|\\,B)\\,P(B) \\\\ &amp;= P(B\\,|\\,A)\\,P(A) \\end{aligned}\\] Because the order doesnt matter and \\(P\\left(A\\cap B\\right)=P\\left(B\\cap A\\right)\\). Independent: Two events \\(A\\) and \\(B\\) are said to be independent if \\(P(A\\cap B)=P(A)P(B)\\). What independence is saying that knowing the outcome of event \\(A\\) doesnt give you any information about the outcome of event \\(B\\). If \\(A\\) and \\(B\\) are independent events, then \\(P(A|B) = P(A)\\) and \\(P(B|A) = P(B)\\). In simple random sampling, we assume that any two samples are independent. In cluster sampling, we assume that samples within a cluster are not independent, but clusters are independent of each other. Bayes Rule: This arises naturally from the rule on conditional probability. Since the order does not matter in \\(A \\cap B\\), we can rewrite the equation: \\[\\begin{aligned} P(A \\cap B) &amp;= P(B \\cap A) \\\\ P(A\\mid B)P(B) &amp;= P(B\\mid A)P(A) \\\\ P(A\\mid B) &amp;= \\frac{P(B\\mid A)P(A) }{P(B)}\\\\ &amp;= \\frac{P(B\\mid A)P(A) }{P(B\\mid A)P(A) + P(B\\mid A^C)P(A^C)} \\end{aligned}\\] 2.1.3 How to define and assign probabilities in general? Frequency-type (Empirical Probability): based on the idea of frequency or long-run frequency of an event. Observe sequence of coin tosses (trials) and the count number of times of event \\(A=\\{H\\}\\). The relative frequency of \\(A\\) is given as, \\[ P(A) = \\frac{\\text{Number of times A occurs}}{\\text{Total number of trials}}. \\] Trials are independent. Relative frequency is unpredictable in short-term, but in long-run its predicable. Let \\(n\\) be the total number of trials and \\(m\\) be number of heads, then we have \\[ P(A) = \\lim_{n \\rightarrow \\infty} \\frac{m}{n}.\\] Theoretical Probability: Sometimes \\(P(A)\\) can be deduced from mathematical model using uniform probability property on finite spaces. If the sample space S is finite, then one possible probability measure on \\(S\\) is the uniform probability measure, which assigns probability \\(1/|S|\\), equal probability to each outcome. Here |S| is the number of elements in the sample space \\(S\\). By additivity, it then follows that for any event \\(A\\) we have, \\[ P(A) = \\frac{|A|}{|S|} = \\frac{\\text{Number of outcomes in S that satisfy A}}{\\text{Total number of outcomes in S} } \\] Long-term frequency It is natural to think of large sequences of similar events defining frequency-type probability if we allow the number of trials to be indefinitely large. Figure 2.3: Demonstrate law of large number with the coin tossing example Belief-type (Subjective Probability): based on the idea of degree of belief (weight of evidence) about an event where the scale is anchored at certain (=1) and impossible (=0). Subjective Probability Consider these events We will have more than 100cm of snow this winter. An asteroid is responsible for the extinction of the dinosaurs Mammography reduces the rate of death from breast cancer in women over 50 by more than 10% The 70 year old man in your office, just diagnosed with lung cancer, will live at least 5 years Can you think of them in terms of long-run frequencies? We cannot always think in terms of long-run frequencies Also, we may not care about long-run frequencies How are they relevant to this patient, this hypothesis? Even where long-run frequency could apply, often there is no such data available However, opinions are formed and beliefs exist Which probability to use? For inference, the Bayesian approach relies mainly on the belief interpretation of probability. The laws of probability can be derived from some basic axioms that do not rely on long-run frequencies. The Monty Hall Problem Revisit Suppose youre on a game show, and youre given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows whats behind the doors, opens another door, say No. 3, which has a goat. He then says to you, Do you want to pick door No. 2? Is it to your advantage to switch your choice? Before any door is picked, we assume all three doors are equal likely to have a car behind it (prior probability of 1/3). Without loss of generality, let assume we pick door 1 and Monty opens door 2 (goat behind). \\(P( \\text{door 1 has car}) = P( \\text{door 2 has car}) = P( \\text{door 3 has car)} = \\frac{1}{3}\\) \\(P( \\text{Monty opens door 2} \\mid \\text{door 1 has car} ) = \\frac{1}{2}\\) \\(P( \\text{Monty opens door 2} \\mid \\text{door 2 has car} ) = 0\\) \\(P( \\text{Monty opens door 2} \\mid \\text{door 3 has car} ) = 1\\) We want to know these two probabilities: \\(P( \\text{door 1 has car} \\mid \\text{Monty opens door 2} )\\) and \\(P( \\text{door 3 has car} \\mid \\text{Monty opens door 2} )\\). Simplify notations, we have \\[\\begin{aligned} &amp; P( D_1 = \\text{car} \\mid \\text{open } D_2 ) \\\\ &amp; = \\frac{P( \\text{open } D_2 \\mid D_1 = \\text{car} )P(D_1 = \\text{car})}{P(\\text{open } D_2)} \\\\ &amp; = \\frac{P( \\text{open } D_2 \\mid D_1 = \\text{car} )P(D_1 = \\text{car})}{P( \\text{open } D_2 \\mid D_1 = \\text{car} )P(D_1 = \\text{car}) + P(\\text{open } D_2 \\mid D_3 = \\text{car})P(D_3 = \\text{car})} \\\\ &amp; = \\frac{\\frac{1}{2} \\times \\frac{1}{3}}{\\frac{1}{2} \\times \\frac{1}{3} + 1 \\times \\frac{1}{3}} = \\frac{1}{3} \\end{aligned}\\] \\[\\begin{aligned} &amp; P( D_3 = \\text{car} \\mid \\text{open } D_2 ) \\\\ &amp; = \\frac{P( \\text{open } D_2 \\mid D_3 = \\text{car} )P(D_3 = \\text{car})}{P(\\text{open } D_2)} \\\\ &amp; = \\frac{P( \\text{open } D_2 \\mid D_3 = \\text{car} )P(D_3 = \\text{car})}{P( \\text{open } D_2 \\mid D_3 = \\text{car} )P(D_3 = \\text{car}) + P(\\text{open } D_2 \\mid D_1 = \\text{car})P(D_1 = \\text{car})} \\\\ &amp; = \\frac{ 1 \\times \\frac{1}{3}}{ 1 \\times \\frac{1}{3} + \\frac{1}{2} \\times \\frac{1}{3}} = \\frac{2}{3} \\end{aligned}\\] Given we pick door 1 and Monty opens door 2 (goat behind), the probability of the car behind door 1 is 1/3 while the probability of the car behind door 3 is 2/3. Therefore, we should switch to door 3 for a better chance of winning. The ELISA test Problem Revisit The ELISA test for HIV was widely used in the mid-1990s for screening blood donations. As with most medical diagnostic tests, the ELISA test is not perfect. If a person actually carries the HIV virus, experts estimate that this test gives a positive result 97.7% of the time. (This number is called the sensitivity of the test.) If a person does not carry the HIV virus, ELISA gives a negative (correct) result 92.6% of the time (the specificity of the test). Assume the estimated HIV prevalence at the time was 0.5% of (the prior base rate). Suppose a randomly selected individual tested positive, please calculate \\(P(D^+ \\mid T^+)\\) and \\(P(D^- \\mid T^-)\\). 2.2 Random Variables The different types of probability distributions (and therefore your analysis method) can be divided into two general classes: Continuous Random Variables - the variable takes on numerical values and could, in principle, take any of an uncountable number of values. 2.Discrete Random Variables - the variable takes on one of small set of values (or only a countable number of outcomes). 2.3 R Quick Reference We give a brief summary of the distributions used most in this course and the abbreviations used in R. Distribution Stem Parameters Parameter Interpretation Binomial binom size prob Number of Trials, Probability of Success (per Trial) Exponential exp rate Mean of the distribution Normal norm mean=0 sd=1 Center of the distribution, Standard deviation Uniform unif min=0 max=1 Minimum and Maximum of the distribution All the probability distributions available in R are accessed in exactly the same way, using a d-function, p-function, q-function, and r-function. Function Result d-function(x) The height of the probability distribution/density at \\(x\\) p-function(x) \\(P\\left(X\\le x\\right)\\) q-function(q) \\(x\\) such that \\(P\\left(X\\le x\\right) = q\\) r-function(n) \\(n\\) random observations from the distribution The mosaic package has versions of the p and q -functions that also print a out nice picture of the probabilities that you ask for. These functions are named by just adding an x at the beginning of the function. For example mosaic::xpnorm(-1). References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
