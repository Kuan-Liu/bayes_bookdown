[["bayes.html", "Session 3 Introduction to Bayesian inference 3.1 Classical frequentist approach 3.2 Introduction to Bayesian approach 3.3 Summary of Conjugate priors &amp; models 3.4 Frequentist vs Bayesian", " Session 3 Introduction to Bayesian inference Review of frequentist inferential approaches Introduce Bayesian inference Learn two simple Bayesian models (Beta-binomial &amp; normal-normal) Discuss practical advantages and disadvantages of Bayesian approach 3.1 Classical frequentist approach The classical (frequentist) statistical approach takes many forms, but the most wide-ranging is the likelihood-based approach This approach specifies a distributional form for data and considers the parameters of the distributions to be fixed constants to be estimated. The parameters are estimated by finding the values that maximize the likelihood (hence the name) i.e. given the observed data, and assuming they come from specific distributions, what are the parameter values for these distributions that maximize the likelihood of these data? Review of likelihood function Given a statistical model with some parameters (lets call them \\(\\theta\\)), and given a set of observed data of size \\(n\\), \\(D = \\{x_1, x_2, \\ldots, x_n \\}\\), the likelihood function, \\(L(\\theta, D)\\) is a function that for every value of \\(\\theta\\) is equal to the probability (mass or density) of observing \\(D\\) given \\(\\theta\\) i.e. \\(L(\\theta, D) = L_D(\\theta) = P(Data | \\theta)\\) if we assume \\(x_1, x_2, \\ldots, x_n\\) are independent and identically distributed, we can express the likelihood function as \\[ L(\\theta, D) = P(x_1 \\mid \\theta)\\times P(x_2 \\mid \\theta) \\ldots \\times P(x_n \\mid \\theta) = \\prod_{i=1}^n P(x_i\\mid \\theta).\\] Example - Bernoulli trials Suppose we want to estimate the risk of death \\(\\theta\\) after a surgery We assume that every patient has the same risk \\(\\theta\\) We collect data from 10 surgeries and we find that 3 patients died and 7 survived, What is the likelihood function for \\(\\theta\\) in this example? The distribution for each patient is \\(Bernoulli(\\theta)\\) the probability of the number of those who died out of \\(n\\) (here \\(n\\)=10) is \\(Binomial(\\theta, 10)\\) The probability mass function of the binomial is \\[p(x|\\theta, n) = {n \\choose x} \\theta^x (1-\\theta)^{n-x}\\] The likelihood function of the observed data (3 deaths out of 10) given \\(\\theta\\) is \\[ L_D(\\theta) = p(x=3| \\theta) = {10 \\choose 3} \\theta^3 (1-\\theta)^{10-3} \\propto \\theta^3 (1-\\theta)^{10-3}\\] Maximum Likelihood Estimator The value that maximizes the likelihood function is called the maximum likelihood estimator or MLE It is the most likely value for \\(\\theta\\) given the observed data In this example it is equal to \\(\\hat{\\theta}_{mle} = \\frac{x}{n} = \\frac{3}{10}=0.3\\) (the observed proportion of event), which can be obtained by taking the first derivative of the loglikelihood and calculate the value of \\(\\theta\\) that yields \\[\\begin{aligned} LogL(\\theta, D) &amp;= log({10 \\choose 3}) + 3\\ log(\\theta) + (10-3)\\ log(1- \\theta) \\\\ \\frac{\\partial}{\\partial \\theta}LogL(\\theta, D) &amp; = \\frac{3}{\\theta} - \\frac{10-3}{1-p} = 0 \\\\ \\hat{\\theta}_{mle} &amp; = \\frac{3}{10}=0.3 \\end{aligned}\\] It is the most commonly method to estimate a parameter in frequentist statistics #simulating a sequence of probability representing parameter \\theta; #\\theta, probability of success, value between 0 and 1; theta &lt;- seq(0, 1, length=1000) #coding Binomial likelihood given x = 3 and n = 10; L &lt;- choose(10,3)*theta^3*(1-theta)^(10-3) #coding log Binomial likelihood given x = 3 and n = 10; logL &lt;- log(choose(10,3)) + 3*log(theta)+ (10-3)*log((1-theta)) # Ploting likelihood function d &lt;- tibble(theta=theta, L=L) p1&lt;-ggplot(data=d, aes(theta,L)) + geom_line()+ ggtitle(&quot;Binomial likelihood x = 3 and n=10&quot;) + theme_bw() # Ploting likelihood function d2 &lt;- tibble(theta=theta, logL=logL) p2&lt;-ggplot(data=d, aes(theta,logL)) + geom_line()+ ggtitle(&quot;Log Binomial likelihood x = 3 and n=10&quot;) + theme_bw() ggarrange(p1, p2, ncol = 2, nrow = 1) #negative loglikelihood function of binomial; neglogL &lt;- function(theta){-sum(dbinom(x=3, size = 10, theta, log = TRUE))} #optimize: optim(par = 0.5, fn=neglogL, method = &quot;Brent&quot;, lower = 0, upper = 1, hessian = TRUE) ## $par ## [1] 0.3 ## ## $value ## [1] 1.321151 ## ## $counts ## function gradient ## NA NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ## ## $hessian ## [,1] ## [1,] 47.61985 Maximum Likelihood confidence interval MLE satisfies the following two properties called consistency and asymptotic normality. Consistency. We say that an estimate \\(\\hat{\\theta}\\) is consistent if \\(\\hat{\\theta} \\rightarrow \\theta_0\\) as \\(n \\rightarrow \\infty\\), where \\(\\theta_0\\) is the true unknown parameter and \\(n\\) is sample size. Asymptotic normality \\(\\hat{\\theta}\\) is asymptotic normality if \\[ \\sqrt{n} (\\hat{\\theta} - \\theta_0) \\rightarrow^d N(0, \\sigma_{\\theta_0}^2) \\] where \\(\\sigma_{\\theta_0}^2)\\) is the asymptotic variance of the estimate \\(\\hat{\\theta}\\). Asymptotic normality says that the estimator not only converges to the unknown parameter, but it converges fast enough, at a rate \\(1/\\sqrt{n}\\). Given this properties, we can use Fisher information to estimate the variance of MLE and subsequently obtaining confidence intervals. - MLE Asymptotic normality with Fisher information, \\(I(\\theta_0)\\) \\[ \\sqrt{n} (\\hat{\\theta}_{mle} - \\theta_0) \\rightarrow^d N(0, \\frac{1}{I(\\theta_0)}) \\] Fisher information is defined using the second derivative of the loglikelihood. \\[ I(\\theta) = - E[\\frac{\\partial^2}{\\partial \\theta^2} logL(x_1,\\ldots, x_n \\mid \\theta)]\\] e.g., for binomail distribution, \\(I(\\theta)=\\frac{n}{\\theta(1-\\theta)}\\), thus the 95% CI for \\(\\hat{\\theta}_{mle}\\) is \\[ \\hat{\\theta}_{mle} \\pm 1.96 \\sqrt{\\frac{\\hat{\\theta}_{mle}(1-\\hat{\\theta}_{mle})}{n}} \\] which gives us \\(0.3 \\pm 1.96 \\times \\sqrt{\\frac{0.3 \\times 0.7}{10}}\\), [0.016, 0.584]. How to calculate variance of MLE in R? mle&lt;-optim(par = 0.5, fn=neglogL, method = &quot;Brent&quot;, lower = 0, upper = 1, hessian = TRUE) # solve(mle$hessian) # to compute the inverse of hessian which is the approximate the variance of theta; upperbound&lt;-0.3 + 1.96*sqrt(solve(mle$hessian)) lowerbound&lt;-0.3 - 1.96*sqrt(solve(mle$hessian)) print(paste(&quot;95% CI for theta is:&quot;,round(lowerbound,3),&quot;-&quot;, round(upperbound,3))) ## [1] &quot;95% CI for theta is: 0.016 - 0.584&quot; Practice MLE estimation in R (Tutorial Practice) Suppose we want to estimate the risk of death after a surgery and We assume that every patient has the same risk . We collect data from 100 surgeries and we find that 30 patients died and 70 survived, What is the likelihood function for in this example? What is the MLE estimator given the observed data? Can you construct the 95% CI confidence interval of the MLE estimator? What is you conclusion comparing this estimator to the MLE obtain from the smaller dataset (10 surgeries, 3 patients died and 7 survived)? 3.2 Introduction to Bayesian approach 3.2.1 Review from session 1 In the Bayesian approach, everything that is not data is considered as a parameter Uncertainty about these parameters is expressed using probability distributions and probabilistic statements A prior distribution expresses what is known or believed independently of the data This prior is updated as data or new evidence is presented The posterior distribution expresses the updated belief Recall Bayes theorem Let \\(D = \\text{patient has disease}\\) and \\(Y = \\text{patient has a positive diagnostic test}\\), \\[\\begin{aligned} P(D \\mid T) &amp; = \\frac{P(T \\mid D)P(D)}{P(T)} \\\\ &amp; = \\frac{P(T \\mid D)P(D)}{P(T \\mid D)P(D) + P(T \\mid D^c)P(D^c)} \\end{aligned}\\] \\(P(T\\mid D)\\) is the likelihood of the outcome (positive test) given the unknown parameter (disease state) \\(P(D)\\) is pre-test probability (prior probability) of disease \\(P(D\\mid T)\\) is the post-test probability of disease which can be obtained by multiplying the likelihood and the pre-test probabiltiy. Here, to calculate \\(P(D\\mid T)\\) we need \\(P(D)\\)! A very sensitive test (e.g., P(TD) = 0.99) can still result in a small post-test probability if the prior probability of disease, \\(P(D)\\), is low! The Bayesian approach to estimating parameters stems from Bayes theorem for continuous variables: Let \\(\\theta\\) be the parameter of interest and \\(y\\) be the observed data, \\[\\begin{aligned} P(\\theta \\mid y) &amp; = \\frac{P(y \\mid \\theta)P(\\theta)}{P(y)} \\\\ &amp; = \\frac{\\text{likelihood of data given parameter} \\times \\text{prior}}{\\text{marginal distribution of data free of the parameter}} \\\\ &amp; \\propto \\text{likelihood}(y \\mid \\theta ) \\times \\text{prior}(\\theta) \\end{aligned}\\] \\(P(y)\\) is called a normalizing factor, its in place to ensure that \\(\\int P(\\theta \\mid y) d\\theta = 1\\), that is the posterior distribution of \\(\\theta\\) is a proper probability distribution with area under the density curve equals to 1. Its value is not of interest, unless we are comparing between data models. The essence of Bayes theorem only concerns the terms involving the parameter, \\(\\theta\\), hence \\(P(\\theta \\mid y) \\propto P(y\\mid \\theta)P(\\theta)\\). Estimating a Proportion Suppose you have observed 6 patients in a Phase I RCT on a given dose of drug, - 0 out of 6 patients have had an adverse event - decision to escalate to a higher dose if its unlikely that the current dosing results in a true proportion of adverse events above 20% (i.e., given the current data, is there sufficient evidence to infer the true proportion of adverse event is less than 20%, if so we can increase the dose level) This is a classic phase I estimate, under frequentist test (Exact Binomial Test) we have binom.test(x=0, n=6, p = 0.2, alternative = c(&quot;less&quot;), conf.level = 0.95) ## ## Exact binomial test ## ## data: 0 and 6 ## number of successes = 0, number of trials = 6, p-value = 0.2621 ## alternative hypothesis: true probability of success is less than 0.2 ## 95 percent confidence interval: ## 0.0000000 0.3930378 ## sample estimates: ## probability of success ## 0 The observed proportion \\(\\hat{\\theta}=0\\) with 95% CI: 0 - 0.39. How much evidence we have that AE rate is &lt; 20%? #suppose we observe 0 adverse event out of 14 patients; #the test results below suggest we would reject the null hypothesis; #at 0.05 alpha level and conclude the true AE rate is &lt; 20%; binom.test(x=0, n=14, p = 0.2, alternative = c(&quot;less&quot;), conf.level = 0.95) ## ## Exact binomial test ## ## data: 0 and 14 ## number of successes = 0, number of trials = 14, p-value = 0.04398 ## alternative hypothesis: true probability of success is less than 0.2 ## 95 percent confidence interval: ## 0.0000000 0.1926362 ## sample estimates: ## probability of success ## 0 What would a Bayesian do? To make probability statements about \\(\\theta\\) after observing data \\(y\\), we need a probability distribution for \\(\\theta\\) given \\(y\\) (the posterior distribution). 1.First, we need to specify a prior distribution for \\(\\theta\\), \\(P(\\theta)\\). Example 1: We might have no idea about \\(\\theta\\) other than that it lies in the interval [0,1] and thus specify a unif(0,1). Let \\(\\theta \\sim U(0,1)\\), the prior probability distribution (p.d.f) is \\[ P(\\theta) = \\frac{1}{1-0} = 1.\\] Example 2: We might have some knowledge about the range of \\(\\theta\\), say, we are believe \\(0.05&lt;\\theta&lt;0.5\\). We can have \\[ \\theta \\sim U(0.05, 0.5)\\] \\[P(\\theta) = \\frac{1}{0.5-0.05} = 2.22.\\] We assume the \\(P(y \\mid \\theta)\\) follows a binomial distribution, thus the likelihood of the observed data given \\(\\theta\\) is \\[ P(y = 0 \\mid \\theta) = {6 \\choose 0} \\theta^0 (1-\\theta)^6 = (1-\\theta)^6\\] The posterior then becomes (given example prior 1) \\[\\begin{align} P(\\theta \\mid y = 0) &amp;= \\frac{P(y = 0 \\mid \\theta) \\times P(\\theta)}{P(y=0)} \\\\ &amp; = \\frac{(1-\\theta)^6 \\times 1}{P(y=0)} \\\\ &amp; = \\text{Constant} \\times (1-\\theta)^6 \\\\ &amp; \\propto (1-\\theta)^6 \\end{align}\\] d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000), y = 0, n = 6) %&gt;% mutate(prior = dunif(p_grid, 0, 1), likelihood = dbinom(y, n, p_grid)) %&gt;% mutate(posterior = likelihood * prior ) d %&gt;% pivot_longer(prior:posterior) %&gt;% # this line allows us to dictate the order in which the panels will appear mutate(name = factor(name, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;))) %&gt;% ggplot(aes(x = p_grid, y = value, fill = name)) + geom_area(show.legend = FALSE) + scale_fill_manual(values = c(&quot;grey&quot;, &quot;red&quot;, &quot;blue&quot;)) + facet_wrap(~ name, scales = &quot;free&quot;)+ theme_bw() Figure 3.1: Approximate posterior distribution obtained using Bayes rule with UNIF(0,1) prior. In this example, the normalizaing term P(y=0) is not considered. Figure 3.2: Approximate posterior distribution obtained using Bayes rule with UNIF(0.05,0.5) prior. In this example, the normalizaing term P(y=0) is not considered. Why is P(y=0) free of \\(\\theta\\)? The law of total probability for discrete parameter values can be used Suppose there are two possible values of parameter \\(\\theta\\), 0.5 and 0.1. Suppose we know the prior distribution of \\(\\theta\\): \\(P(\\theta = 0.5) = 0.8\\) and \\(P(\\theta = 0.1) = 0.2\\) Likelihood values are calculated given a known \\(\\theta\\), so then dont include the parameter \\(\\theta\\). Call these \\(P_{0.5} = P(Y = 0 \\mid \\theta = 0.5)\\) and \\(P_{0.1} = P(Y = 0 \\mid \\theta = 0.1)\\), Putting all components together using law of total probability, \\(P(Y = 0)\\) does not involved the unknown \\(\\theta\\) \\[P(Y = 0) = P(Y = 0 \\mid \\theta = 0.5)P(\\theta = 0.5) + P(Y = 0 \\mid \\theta = 0.1)P(\\theta = 0.1)\\] \\[ P(Y = 0) = P_{0.5} \\times 0.8 + P_{0.1} \\times 0.2 \\] In case of a continuous parameter value, we can obtain \\(P(y=0)\\) by integrating over the space of \\(\\theta\\) as following \\[P(y=0) = \\int_{\\theta=0}^{\\theta=1} P(y=0 \\mid \\theta) P(\\theta) \\ d \\theta\\] Integrating over \\(\\theta\\) is analogous to summing over a set of discrete values of \\(\\theta\\). After integration over \\(\\theta\\), \\(\\theta\\) is no longer featured in \\(P(y=0)\\). In this example, we have \\[P(y=0) = \\int_{\\theta=0}^{\\theta=1} P(y=0 \\mid \\theta) \\times \\frac{1}{1-0} \\ d \\theta \\] \\[P(y=0) = \\int_{\\theta=0}^{\\theta=1} P(y=0 \\mid \\theta) \\times \\frac{1}{0.5-0.05} \\ d \\theta \\] #Integration in R with one variable; #Unif(0,1); integrand &lt;- function(theta) {(1-theta)^6} normalizing_constant&lt;-integrate(integrand, lower = 0, upper = 1) normalizing_constant ## 0.1428571 with absolute error &lt; 0.0000000000000016 #Unif(0.05,0.5); integrand2 &lt;- function(theta) {((1-theta)^6)/(0.5-0.05)} normalizing_constant2&lt;-integrate(integrand2, lower = 0, upper = 1) normalizing_constant2 ## 0.3174603 with absolute error &lt; 0.0000000000000035 d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000), y = 0, n = 6) %&gt;% mutate(prior1 = dunif(p_grid, 0, 1), prior2 = dunif(p_grid, 0.05, 0.5), likelihood = dbinom(y, n, p_grid)) %&gt;% mutate(posterior1 = likelihood * prior1 / normalizing_constant$value, posterior2 = likelihood * prior2 / normalizing_constant2$value) d %&gt;% pivot_longer(posterior1:posterior2) %&gt;% ggplot(aes(x = p_grid, y = value, group = as.factor(name))) + geom_area(aes(fill = name),position=&quot;identity&quot;) + scale_fill_manual(name = &quot;Prior&quot;, labels = c(&quot;UNIF(0,1)&quot;,&quot;UNIF(0.05,0.5)&quot;),values = c(&quot;#d8b365&quot;, &quot;#5ab4ac&quot;)) + geom_vline(xintercept = 0.2, size=1) + annotate(&quot;text&quot;, x=0.3, y=6, label= expression(paste(theta, &quot;=0.2&quot;)), size = 5) + labs(x = expression(theta), y = &quot;posterior probability density&quot;, title=&quot;Posterior distribution given uniform priors and binary data&quot;)+ theme_bw() Figure 3.3: Posterior distribution obtained using Bayes rule with UNIF(0,1) and UNIF(0.05,0.5) priors Practice posterior estimation with brms package in R (Tutorial Practice) we will introduce MCMC in later sessions. dat1&lt;-data.frame(y=rep(0,6)) fit1 &lt;- brm(data = dat1, family = bernoulli(link = &quot;identity&quot;), y ~ 1, prior = c(prior(uniform(0, 1), class = Intercept)), iter = 1000 + 2500, warmup = 1000, chains = 2, cores = 2, seed = 123) fit2 &lt;- brm(data = dat1, family = bernoulli(link = &quot;identity&quot;), y ~ 1, prior = c(prior(uniform(0.05, 0.5), class = Intercept)), iter = 1000 + 2500, warmup = 1000, chains = 2, cores = 2, seed = 123) summary(fit1) summary(fit2) plot(fit1) plot(fit2) mcmc_areas( fit1, pars = c(&quot;b_Intercept&quot;), prob = 0.80, # 80% inner intervals; prob_outer = 0.95, # 99% outter intervals; point_est = &quot;mean&quot; ) mcmc_areas( fit2, pars = c(&quot;b_Intercept&quot;), prob = 0.80, # 80% inner intervals; prob_outer = 0.95, # 99% outter intervals; point_est = &quot;mean&quot; ) hypothesis(fit1, &#39;Intercept &lt; 0.2&#39;) hypothesis(fit2, &#39;Intercept &lt; 0.2&#39;) 3.2.2 The Beta-binomial model With a single sample of \\(n\\) binary outcomes, we have one unknown parameter: \\(\\theta\\) and \\(0 \\leq \\theta \\leq 1\\). We need a prior distribution for \\(\\theta\\) (e.g., proportion, risk, probability of event etc): \\(P(\\theta)\\) To express indifference between all values of \\(\\theta\\), we can use a uniform distribution on \\(\\theta\\), as we did in the previous example To express belief (e.g. based on external evidence) that some values of \\(\\theta\\) are more likely that others, it is convenient to use a beta distribution This has two parameters, often labelled as \\(\\alpha\\) (also written as a) and \\(\\beta\\) (also written as b), which we can choose to represent the strength of the external evidence If a parameter has a Beta(a,b) distribution, then the prior mean is \\[\\frac{a}{a+b}\\] The beta distribution prior for the binomial is useful for illustrating how the Bayesian approach combines prior information and new data Beta-binomial model Recall the likelihood for a binomial outcome of x successes in n trials, \\[ P(x \\mid \\theta) \\propto \\theta^x (1-\\theta)^{n-x}\\] The \\(Beta(\\alpha,\\beta)\\) prior has the same functional form for \\(\\theta\\), \\[ P( \\theta) \\propto \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}\\] We find the posterior as \\[\\begin{align} P(\\theta \\mid x) &amp; \\propto P(x \\mid \\theta) \\times P( \\theta) \\\\ &amp; \\propto \\theta^x (1-\\theta)^{n-x} \\times \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} \\\\ &amp; \\propto \\theta^{x+\\alpha-1} (1-\\theta)^{n-x+\\beta-1} \\end{align}\\] Thus, comparing to the \\(Beta(\\alpha,\\beta)\\) prior, the posterior just changes the exponents by adding x and n-x, respectively. Comparing to the prior, make two changes to get the posterior: \\(a \\rightarrow a+x\\), [a + number of events] \\(b \\rightarrow b+(n-x)\\), [b + number of non-events] Quite simply, when \\(x\\) events have been observed in n subjects, the prior \\[ \\theta \\sim Beta(\\alpha, \\beta) \\] gives the posterior \\[ \\theta \\mid x \\sim Beta(\\alpha+x, \\beta+n-x) \\] The prior and posterior are both beta distributions! Interpretation of Beta Prior Suppose we start with a beta prior with small parameters \\[ \\theta \\sim Beta(0.001, 0.001) \\] Observe x events in n trials, the posterior \\[ \\theta \\mid x \\sim Beta(0.001+x, 0.001+n-x) \\approx Beta(x,n-x)\\] Posterior mean of \\(\\theta \\approx \\frac{x}{n}\\), the equivalent to the MLE based only on the data Interpretation of the \\(Beta(\\alpha,\\beta)\\) prior. Like having seen \\(\\alpha\\) events and \\(\\beta\\) non events in a sample size of \\(\\alpha + \\beta\\) Strength of prior information equivalent to prior sample size \\(\\alpha + \\beta\\) Prior mean = \\(\\frac{\\alpha}{\\alpha + \\beta}\\) Consider Beta(3,7)and Beta(12,28) priors Gold line: prior belief that assumes approximately 3 events in 10 subjects Blue line: prior belief that assumes approximately 12 events in 40(=12+28) subjects Plot Beta densities in R (Tutorial Practice) Try plotting Beta(2,8), Beta(8,2), and Beta(8,8). Example R code provided below. a&lt;-2 b&lt;-2 d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 101)) %&gt;% mutate(priorDensity = dbeta(theta, shape1 = a,shape2 = b)) ggplot(d, aes(theta, priorDensity))+ geom_line()+ xlab(expression(theta))+ ylab(expression(p(theta)))+ ggtitle(paste0(&quot;Beta distribution with &quot;, &quot;a = &quot;,a,&quot;; b = &quot;,b))+ theme_bw() Summarizing Posterior Distribution Since we know the form of the posterior distribution, we can easily calculate functions such as: Posterior mean \\(E(\\theta) = \\frac{\\alpha+x}{\\alpha + \\beta+n}\\) 95% Credible intervals (we will talk more about them next week) \\(P(\\theta &lt; 0.2)\\), \\(P(\\theta &lt; 0.5)\\), \\(P(0.4&lt; \\theta &lt; 0.6)\\), etc, which can be directly used to make probabilistic statement about the \\(\\theta\\). E.g., the probability of the posterior adverse event rate &lt; 0.2 is about 0.95. Generate informative plots for assessing priors and posteriors. All this can easily be done using R. Beta densities posterior summary in R (Tutorial Practice) Suppose we observe 1 adverse events among 10 patients and we assume the prior distribution of adverse event proportion \\(\\theta \\sim Beta(1,1)\\). What is the posterior Beta distribution of \\(\\theta\\)? What is the posterior mean Beta distribution of \\(\\theta\\)? What is the posterior probability \\(P(\\theta&lt;0.2)\\)? Data overwhelming the prior The posterior for the beta-binomial model after seeing x events in n trials is \\(\\theta \\mid x ~ Beta(\\alpha + x, \\beta + n - x)\\) with posterior mean as \\[E(\\theta \\mid x) = \\frac{\\alpha + x}{\\alpha + \\beta + n}\\] In \\(n \\gg \\alpha + \\beta\\) and \\(x \\gg \\alpha\\) (sample size and number of events is large), recall when using prior \\(Beta(0.001, 0.001)\\), \\[ E(\\theta \\mid x) = \\frac{\\alpha + x}{\\alpha + \\beta + n} \\approx \\frac{x}{n}\\] Here, prior is of little importance! A Beta(1,1) is Unif(0,1) When \\(\\alpha = \\beta = 1\\), \\(P(\\theta) \\propto \\theta^{1-1}(1-\\theta)^{1-1} = 1\\), which is the probability density of a unif(0,1). Posterior mean as a weighted average The posterior mean is \\[\\begin{align} E(\\theta \\mid x) &amp; = \\frac{\\alpha + x}{\\alpha + \\beta + n} \\\\ &amp; = \\frac{\\alpha }{\\alpha + \\beta + n} + \\frac{x }{\\alpha + \\beta + n}\\\\ &amp; = \\Big( \\frac{\\alpha+ \\beta }{\\alpha + \\beta + n} \\Big) \\times \\Big( \\frac{\\alpha }{\\alpha + \\beta} \\Big) + \\Big( \\frac{n }{\\alpha + \\beta + n} \\Big) \\times \\Big( \\frac{x }{n} \\Big) \\\\ &amp; = w \\times \\Big( \\frac{\\alpha }{\\alpha + \\beta} \\Big) + (1-w) \\times \\Big( \\frac{x }{n} \\Big) \\\\ &amp; = w \\times (\\text{prior mean}) + (1-w) \\times (\\text{sample estimate}) \\end{align}\\] Where \\(w\\) is the ratio of the prior sample size to the total sample size \\[ w = \\frac{\\alpha+ \\beta }{\\alpha + \\beta + n}.\\] This is a common theme in Bayesian models with actual prior information The posterior distribution will lie between the prior and likelihood The posterior mean is a weighted average of the prior mean and data-based estimate As the sample size increases, the contribution of the prior diminishes Revisit the adverse event example, now using the beta-binomial model Suppose you have observed 0 adverse events in 6 patients in a phase I clinical trial on a given dose of a drug Assume a prior: \\(\\theta \\sim Beta(1,1)\\) ; What is the posterior distribution? How much evidence that the AE rate is &lt; 20%? Find \\(P(\\theta &lt; 0.2)\\) using the posterior distribution? What is the AE rate at a lower dose was 1/6? Can we use this external evidence? New prior for the lower dose, \\(\\theta \\sim Beta(1,5)\\) New Posterior is given 0 adverse events in 6 patients: \\(\\theta \\sim Beta(1,11)\\) Find \\(P(\\theta &lt; 0.2)\\). 3.2.3 The Normal-normal model Parameters with normal priors and likelihoods Continuing with a single parameter model The likelihood of the parameter given some data is that of a normal distribution with known variance \\(\\sigma^2\\) \\[ y \\mid \\theta \\sim N(\\theta, \\sigma^2) \\] suppose the prior for \\(\\theta\\), the mean, follows a normal distribution \\[ \\theta \\sim N(\\mu_0, V_0)\\] We want to make inference about \\(\\theta\\) given the data y, what is the posterior distribution \\(P(\\theta \\mid y)\\)? Normal-normal model 1. The normal model (likelihood) of one data point Consider a single observation \\(y\\) from a normal distribution parameterized by a mean \\(\\theta\\) and variance \\(\\sigma^2\\), we assume that \\(\\sigma^2\\) is known. The normal likelihood for \\(y\\) given \\(\\theta\\) is \\[ P(y\\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y-\\theta)^2}{2\\sigma^2}}, y \\in (-\\infty, \\infty)\\] The \\(E(y) = \\theta\\) and \\(V(y) = \\sigma^2\\) and \\(SD(y) = \\sigma\\). Given \\(y \\mid \\theta \\sim N(\\theta,\\sigma^2 )\\), \\(z = \\frac{x-\\theta}{\\sigma} \\sim N(0,1)\\). Thus, if we rescale \\(y\\) by \\(\\sigma\\), we have 95%CI of y: \\(\\theta \\pm 1.96 \\times \\sigma\\) 2. Prior We can often use a normal prior to represent \\(\\theta \\sim N(\\mu_0, V_0)\\) \\(\\mu_0\\) is the prior mean \\(V_0\\) is the prior variance, which expresses the uncertainty around the mean \\(\\mu_0\\) \\(\\tau_0 = \\frac{1}{V_0}\\) is called precision, it can also be used to express the uncertainty around the mean, e.g. \\(\\theta \\sim N(\\mu_0, \\tau_0)\\) High precision = low variance, in other words, the distribution of the normal random variable \\(\\theta\\) is closely centered around its mean. 3. Posterior With a normal prior and a normal likelihood of the data given parameter \\(\\theta\\), (here we use precision to express the normal distributions) \\[ \\theta \\sim N(\\mu_0,\\tau_0 ) \\text{ and } y \\mid \\theta \\sim N(\\theta,\\tau_y )\\] where \\(\\tau_y = \\frac{1}{\\sigma^2}\\) is the precision of the observed data It can be proven mathematically using the Bayes rule that the posterior distribution given a normal prior and normal likelihood is also normal, \\[ \\theta \\mid y \\sim N(\\mu_1, \\tau_1)\\] The posterior mean \\(\\mu_1\\) is \\[ \\frac{\\tau_0 \\times \\mu_0 + \\tau_y \\times y}{\\tau_0+ \\tau_y} \\] The posterior precision is \\[ \\tau_1 = \\tau_0 + \\tau_y\\] Again, the prior and posterior are both normal distributions! Posterior mean as a weighted average of the prior mean and data-based estimate Using the precision parameters \\[ \\tau_y = \\frac{1}{\\sigma^2_y} \\text{ and } \\tau_0 = \\frac{1}{V_0}\\] The posterior distribution of \\(\\theta\\) is then \\[\\theta \\mid y \\sim N(\\mu_1, \\tau_1)\\] where the mean and precision are \\[\\begin{align} &amp; \\mu_1 = \\Big(\\frac{\\tau_0}{\\tau_0 + \\tau_y}\\Big) \\times \\mu_0 + \\Big(\\frac{\\tau_y}{\\tau_0 + \\tau_y}\\Big) \\times y \\\\ &amp; \\tau_1 = \\tau_0 + \\tau_y \\end{align}\\] The posterior mean is a weighted sum of the prior mean and the observed value. The weights are the relative precisions of the prior and the likelihood \\[\\mu_1 = w_0 \\times \\mu_0 + w_y \\times y\\] \\[w_0 = \\frac{\\tau_0}{\\tau_0 + \\tau_y}\\] and \\[w_y = 1-w_0 = \\frac{\\tau_y}{\\tau_0 + \\tau_y}\\] Data overwhelming the prior and vice versa With relatively low prior precision (imprecise prior and substantial data), i.e., \\(\\tau_0 \\ll \\tau_y\\) and \\(0 \\approx w_0 \\ll w_y \\approx 1\\), \\[ \\mu_1 = w_0 \\times \\mu_0 + w_y \\times y \\approx y\\] With relative little data and aprior that is not too diffuse, i.e., \\(\\tau_y \\ll \\tau_0\\) and \\(0 \\approx w_y \\ll w_0 \\approx 1\\), \\[ \\mu_1 = w_0 \\times \\mu_0 + w_y \\times y \\approx \\mu_0\\] In most cases, the posterior mean is a compromise between what we believed before and what we observed in the data. Normal model with multiple observations Consider a set of \\(n\\) observations \\(y = (y_1, \\ldots, y_n)\\) sample from a \\(N(\\theta, \\sigma^2)\\), where \\(y_1,\\ldots, y_n\\) are identically, independently distributed and \\(y_i \\sim N(\\theta, \\sigma^2)\\), \\(i = 1, \\ldots, n\\). The normal likelihood of the joint distribution \\(y = (y_1, \\ldots, y_n)\\) is \\[\\begin{align} P((y_1, \\ldots, y_n \\mid \\theta) &amp;= \\prod_{i=1}^n P(y_i \\mid \\theta) \\\\ &amp; = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y_i-\\theta)^2}{2\\sigma^2}}, y \\in (-\\infty, \\infty) \\end{align}\\] Given prior \\(\\theta \\sim N(\\mu_0, \\tau_0)\\), the posterior of \\(\\theta \\mid y\\) is \\[P(\\theta \\mid y_1, \\ldots, y_n) = N(\\mu_n, \\tau_n)\\] where the posterior mean \\(\\mu_n\\) is \\[\\mu_n = \\frac{\\tau_0 \\times \\mu_0 + n\\tau_y \\times \\bar{y}}{\\tau_0+ n\\tau_y} = \\Big( \\frac{\\tau_0}{\\tau_0+ n\\tau_y} \\Big) \\times \\mu_0 + \\Big( \\frac{n\\tau_y}{\\tau_0+ n\\tau_y} \\Big) \\times \\bar{y}\\] and the posterior precision is \\[ \\tau_n = \\tau_0 + n\\tau_y\\] \\(\\bar{y} = \\frac{\\sum_{i=1}^2y_i}{n}\\) is the sample mean and \\(\\tau_y = \\frac{1}{\\sigma^2}\\) Again, the posterior mean is a weighted sum between prior and data. As sample size \\(n \\rightarrow \\infty\\), \\(\\Big( \\frac{\\tau_0}{\\tau_0+ n\\tau_y} \\Big) \\rightarrow 0\\) and \\(\\Big( \\frac{n\\tau_y}{\\tau_0+ n\\tau_y} \\Big) \\rightarrow 1\\), data will overwhelm the prior and dominate the posterior distribution. The use of normal-normal model in clinical applications Consider estimating a parameter \\(\\theta\\), e.g. mean, log-hazard ratio, log-odds-ratio, log-relative-risk Suppose we have an estimate of \\(\\theta\\); We will call it \\(y\\) and let \\(\\hat{\\sigma}^2_y\\) be the estimated variance This estimate \\(y\\) can be viewed as a single data observation of \\(\\theta\\)! In large samples, it is approximately true that \\(y \\sim N(\\theta, \\sigma^2_y)\\), where \\(\\theta\\) is the true value and \\(\\sigma^2_y\\) is the true variance (sample size dependent) and the 95%CI of \\(y\\) is \\(\\theta \\pm 1.96 \\times \\sigma\\). We can construct a normal likelihood of data for the parameter \\(\\theta\\) as \\[y \\mid N(\\theta, \\hat{\\sigma}^2_y)\\] Treatment effect estimate on the mean For this example, we will simulate data for a small study of n=100 subjects and randomized 50% to treat=0 and 50% to treat=1 (mimicking an RCT). We will then generate the outcome Y from a normal distribution depending on the treatment indicator. Suppose we have the following table of results from a linear regression estimating treatment effect \\(\\theta\\) on the outcome n &lt;- 100 set.seed(1234) treat &lt;- 1*(runif(n, min = 0, max = 1)&lt;0.5) y &lt;- 5*treat+rnorm(n, mean = 0, sd =10) fit&lt;-lm(y~treat) summary(fit) ## ## Call: ## lm(formula = y ~ treat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.8662 -6.7095 -0.5731 5.7503 24.1866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.303 1.423 0.916 0.3618 ## treat 4.080 1.918 2.127 0.0359 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.543 on 98 degrees of freedom ## Multiple R-squared: 0.04414, Adjusted R-squared: 0.03438 ## F-statistic: 4.525 on 1 and 98 DF, p-value: 0.03591 round(confint.default(fit, &#39;treat&#39;, level=0.95),3) # based on asymptotic normality ## 2.5 % 97.5 % ## treat 0.321 7.84 We are interested in using this published results to make Bayesian inference on \\(\\theta\\) Our observed data (estimate) of \\(\\theta\\), denoted as \\(y\\) is 4.08 with standard error 1.918. Here, \\(y\\) plays the role of data! We call \\(y\\) the observed datum. Relying on the large sample approximation, we assume that the estimate y arose from a normal distribution with true mean \\(\\theta\\), We treat the observed standard error \\(\\hat{\\sigma}_y = 1.918\\) as the true standard deviation of \\(y\\) Recall, the standard deviation of the sampling distribution of a parameter estimate is called its standard error This gives us the likelihood \\[y \\mid \\theta \\sim N(\\theta, 1.918^2)\\] The width of the 95% CI is approximately equal to \\(2 \\times 1.96 \\times \\sigma_y\\) Suppose we are not provided with \\(\\sigma_y\\) from the table of results, but know the 95% CI of y is (1.568,0.064), we can calculate \\(\\sigma_y\\) with \\[\\sigma_y = \\frac{7.84- 0.321}{2\\times 1.96}=1.918\\] The normal likelihood is this example is \\(y \\sim N(4.08, 1.918^2)\\), if we assume a prior \\(\\theta \\sim N(0, 2 \\times 1.918^2)\\), we have a posterior \\[\\begin{align} &amp; \\theta \\mid y \\sim N(\\mu_1, \\tau_1) \\\\ \\mu_1 &amp;= \\frac{ \\frac{1}{prior.sd^2} }{\\frac{1}{prior.sd^2} + \\frac{1}{se.obs^2} } \\times prior.mean + \\frac{\\frac{1}{se.obs^2}}{\\frac{1}{prior.sd^2} + \\frac{1}{se.obs^2}} \\times y.obs \\\\ &amp; = \\frac{ \\frac{1}{2 \\times 1.918^2} }{\\frac{1}{2 \\times 1.918^2} + \\frac{1}{1.918^2} } \\times 0 + \\frac{\\frac{1}{1.918^2}}{\\frac{1}{2 \\times 1.918^2} + \\frac{1}{1.918^2}} \\times 4.08 = 2.72 \\\\ \\tau_1 &amp; = \\tau_0 + \\tau_y \\\\ &amp; = \\frac{1}{prior.sd^2} + \\frac{1}{se.obs^2} \\\\ &amp; = \\frac{1}{2 \\times 1.918^2} + \\frac{1}{1.918^2} = 0.408 \\end{align}\\] d &lt;- tibble(theta = seq(from = -10, to = 15, length.out = 200), y.obs=4.08, se.obs=1.918, prior.mean=0, prior.sd=sqrt(2)*1.918) %&gt;% mutate(priorDensity = dnorm(theta, prior.mean, prior.sd)) %&gt;% mutate(likelihood.0 = dnorm(theta, y.obs, se.obs)) %&gt;% mutate(w = (1/se.obs^2)/(1/se.obs^2 + 1/prior.sd^2)) %&gt;% mutate(post.prec = 1/se.obs^2 + 1/prior.sd^2) %&gt;% mutate(posteriorDensity = dnorm(theta, w*y.obs+(1-w)*prior.mean, sqrt(1/post.prec))) %&gt;% mutate(likelihood = likelihood.0/sum(diff(theta)*likelihood.0[-1])) %&gt;% pivot_longer(cols=c(priorDensity,likelihood,posteriorDensity),names_to=&quot;Function&quot;,values_to=&quot;p&quot;) d$Function &lt;- factor(d$Function, levels=c(&quot;priorDensity&quot;,&quot;likelihood&quot;,&quot;posteriorDensity&quot;), labels=c(&quot;Prior&quot;,&quot;Likelihood&quot;,&quot;Posterior&quot;)) p&lt;-ggplot(d, aes(theta, p,colour=Function))+ geom_line(size=1)+ xlab(expression(theta))+ ylab(expression(p(theta)))+ theme(legend.position = &quot;bottom&quot;)+ scale_colour_manual(values=c(&quot;goldenrod&quot;,&quot;steelblue&quot;,&quot;black&quot;)) +theme_bw() p Hazard Ratio on time-to-event from Cox proportional hazard model In memory of Sir David Cox, 1925-2022 Effect of a Resuscitation Strategy Targeting Peripheral Perfusion Status vs Serum Lactate Levels on 28-Day Mortality Among Patients With Septic Shock The ANDROMEDA-SHOCK Randomized Clinical Trial (Hernández et al. 2019) Open-access link: https://jamanetwork.com/journals/jama/fullarticle/2724361 We are interested to make inference on the hazard ratio of peripheral perfusion versus Lactate level on 28-Day mortality among patients with septic shock. We obtain a point estimate of HR and its 95%CI from Table 2 of (Hernández et al. 2019). Figure 3.4: HR estimates of the effectiveness of peripheral perfusion on primary outcome - Death within 28 days 1. The normal likelihood on log(HR) The estimated adjusted (HR) on 28-day mortality, 0.75 (0.55 to 1.02), is called the observed datum (viewed as a single data observation on \\(\\theta\\), denoted as \\(y\\)) Its known that log of the parameter estimate, such as HR, RR and OR, follow a normal distribution around its true log value. Thus, We need to use the reported output information in order to specify the likelihood for on the log scale. Let \\(\\theta\\) denote the random variable representing log(HR) Let \\(y\\) denote an observed data point of \\(\\theta\\), we have \\(y \\sim N(\\theta, \\sigma_y^2)\\), where \\(\\theta\\) is log(HR) and \\(sigma_y\\) is the standard error of the log(HR) In order to get \\(\\sigma_y^2\\) from the result table, we will use the reported 95% CI of the log of HR! The limits of the CI for the log value are obtained by taking the logs of the reported CI, i.e., 95% CI on the log(HR) in this example is [log(0.55)=-0.598, log(1.02)=0.02]. Take the width of the 95% CI and divided by \\(2\\times 1.96\\), we have \\[\\hat{\\sigma}_y = \\frac{log(1.02) - log(0.55)}{2\\times 1.96} = 0.158\\] We use \\(\\hat{\\sigma}_y\\) to approximate \\(\\sigma_y\\), therefore, the data likelihood \\(y \\mid \\theta \\sim (\\theta,0.158^2)\\) where we have observed \\(y=log(0.75)=-0.287\\) 2. The prior for log(HR) Suppose we consider a neutral prior for log(HR), such that 95% of the prior for probability for the HR is in the range 0.8 to 1.25 and the median of HR is 1 (indicating the intervention is neither beneficial or harmful) The prior of \\(log(HR) \\sim N(log(1), \\sigma_0^2)\\), where \\[ \\sigma_0 = \\frac{\\log(1.25) - \\log(0.8)}{2 \\times 1.96} = 0.11\\] We can express the prior normal distribution on log(HR) as \\[\\theta \\sim N(\\mu_0 = 1, \\sigma_0^2=0.11^2)\\] We specify normal prior for log(HR)!. In probability theory, a lognormal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. In this case, HR follows a lognormal distribution! 3. The posterior of log(HR) Given prior \\(\\theta \\sim N(1, 0.11^2)\\) and likelihood \\(y \\mid \\theta \\sim (\\theta,0.158^2)\\) where \\(y=log(0.75)=-0.287\\) Prior precision is \\(\\frac{1}{0.11^2} = 82.6\\) and the likelihood precision is \\(\\frac{1}{0.158^2} = 40.1\\), we can see that the prior has more information than the likelihood! The posterior precision on log(HR) is \\(\\tau_1 = 82.6 + 40.1=122.7\\) The posterior mean is \\[ \\frac{82.6}{82.6 + 40.1} \\times log(1) + \\frac{40.1}{82.6 + 40.1} \\times log(0.75) = -0.094\\] The posterior variance of \\(\\log(HR) = \\frac{1}{122.7}\\) and the posterior standard deviation is \\(\\sqrt{\\frac{1}{122.7}} = 0.09\\), thus \\[\\theta \\mid y \\sim N(\\mu_1 = -0.094, \\sigma_1^2 = 0.09^2\\] The posterior median HR = exp(-0.094) = 0.91 and the posterior 95% Credible Interval for HR is \\[\\exp(-0.094 \\pm 1.96\\times 0.09) \\rightarrow (0.763, 1.086) \\] #prior on log(HR); mu0&lt;- 0 sd0&lt;- 0.11 prec0 &lt;- 1/sd0^2 #likelihood on log(HR); ybar&lt;- -0.287 sd.ybar&lt;- 0.158 prec.y &lt;- 1/sd.ybar^2 #posterior on log(HR) prec1 &lt;- prec0 + prec.y mu1 &lt;- (prec0 * mu0 + prec.y*ybar)/prec1 sigma1 &lt;- 1/sqrt(prec1) mu.plot &lt;- seq(ybar-1,ybar+1, length.out=200) lik &lt;- dnorm(ybar, mu.plot, sd.ybar) prior &lt;- dnorm(mu.plot, mu0, sd0) posterior&lt;- dnorm(mu.plot, mu1,sigma1) d&lt;-data.frame(mu.plot=rep(mu.plot, 3), p =c(lik,prior,posterior), Function=rep(c(&quot;likelihood&quot;,&quot;prior&quot;,&quot;posterior&quot;),each=length(mu.plot))) p1&lt;- ggplot(d, aes(mu.plot, p,colour=Function))+ geom_line(size = 1)+ xlab(&quot;log(HR)&quot;)+ ylab(&quot;Probability Density&quot;)+ scale_colour_manual(values=c(&quot;goldenrod&quot;,&quot;steelblue&quot;,&quot;black&quot;))+ theme_bw()+ ggtitle(&quot;Neural prior on log(HR): plot on log(HR) scale&quot;) p2&lt;- ggplot(d, aes(exp(mu.plot), p,colour=Function))+ geom_line(size = 1)+ xlab(&quot;HR&quot;)+ ylab(&quot;Probability Density&quot;)+ scale_colour_manual(values=c(&quot;goldenrod&quot;,&quot;steelblue&quot;,&quot;black&quot;))+ theme_bw()+ ggtitle(&quot;Neural prior on log(HR): plot on HR scale&quot;) ggarrange(p1,p2, nrow=2) Posterior summary statistics from R thresholds&lt;-c(1,0.8) res &lt;-exp( qnorm(c(0.5, 0.025, 0.975), mu1, sigma1)) res &lt;- c(res, pnorm(log(thresholds), mu1, sigma1)) names(res) &lt;- c(&quot;median&quot;,&quot;q2.5&quot;,&quot;q97.5&quot;,paste0(&quot;Pr(OR &lt; &quot;,thresholds,&quot;)&quot;)) res ## median q2.5 q97.5 Pr(OR &lt; 1) Pr(OR &lt; 0.8) ## 0.9105607 0.7628965 1.0868065 0.8503338 0.0757977 3.3 Summary of Conjugate priors &amp; models There are a small number of prior-likelihood pairs where the prior and the posterior are in the same family (e.g., both beta, both normal): these are called conjugate models These posterior distributions can be computed without specialized software These examples are useful for illustrating how Bayesian methods combine prior information with data They have only limited practical usefulness - limits on types priors, limits on number of parameters They are useful teaching tools, soon we will see how we can extend beyond these simple models 3.4 Frequentist vs Bayesian R Session information ## R version 4.0.5 (2021-03-31) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Matrix products: ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 ## [3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.8.1 ggmcmc_1.5.1.1 brms_2.16.3 Rcpp_1.0.7 ## [5] ggpubr_0.4.0 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.7 ## [9] purrr_0.3.4 readr_2.1.1 tidyr_1.1.4 tibble_3.1.6 ## [13] ggplot2_3.3.5 tidyverse_1.3.1 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
