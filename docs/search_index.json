[["index.html", "HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research University of Toronto Statement of Acknowledgment of Traditional Land", " HAD5314H - Applied Bayesian Methods in Clinical Epidemiology and Health Care Research Kuan Liu Institute of Health Policy, Management and Evaluation University of Toronto 2022-01-05 University of Toronto Statement of Acknowledgment of Traditional Land We wish to acknowledge this land on which the University of Toronto operates. For thousands of years it has been the traditional land of the Huron-Wendat, the Seneca, and the Mississaugas of the Credit. Today, this meeting place is still the home to many Indigenous people from across Turtle Island and we are grateful to have the opportunity to work on this land. Health Sciences Building "],["course-syllabus.html", "Course Syllabus Course Info Course Description Course Textbook and Structure Calendar and Outline Accessibility and Accommodations Academic Integrity Key Resources and Supports for DSLPH Graduate Students License", " Course Syllabus Course Info Course title: Applied Bayesian Methods in Clinical Epidemiology and Health Care Research Semester: Winter 2022 Class hour and location: Friday, 12pm to 3pm (Two parts: Lecture &amp; Tutorial, with 15 mins break in between). Please see Quercus for other course details. Instructors Email Office Hour Kuan Liu (Kuan) mailto:kuan.liu@utoronto.ca Thursday, 12pm to 1pm Juan Pablo Díaz Martínez (Juan Pablo) mailto:juan.diaz.martinez@mail.utoronto.ca Course Description This course will introduce students to Bayesian data analysis. After a thorough review of fundamental concepts in statistics and probability, an introduction will be given to the fundamentals of the Bayesian approach. Students will learn how to use the brms package in the R statistical software environment to carry out Bayesian analyses of data commonly seen in health sciences. Bayesian methods will be covered for binary, continuous and count outcomes in one and two samples, for logistic, linear and Poisson regression, and for meta-analysis. By the end of this course, students will: Understand what is meant by a Bayesian Analysis and how it differs from a typical analysis under the frequentist framework Understand how modern Bayesian models are fitted Understand the role of prior(s) in Bayesian analysis and how to use prior(s) Be able to fit Bayesian models to common types of study outcomes know what aspects of the Bayesian analysis are an essential part of a statistical report Have developed expertise in using the brms program within the R environment Pre-requisites HAD5316H  Biostatistics II: Advanced Techniques in Applied Regression Methods (or CHL5202H Biostatistics II) and basic programming knowledge in R or SAS. HAD5316H or CHL5202H may be taken concurrently with this course. Evaluation Class participation worth 10% and three individual assignments each worth 30% Course Textbook and Structure 2nd edition of the book by Richard McElreath: https://xcelab.net/rm/statistical-rethinking/ (McElreath 2018) Bayesian Approaches to Clincial Trials and Health-Care Evaluation by David Spiegelhalter, free access via UT library (Spiegelhalter, Abrams, and Myles 2003) The lecture slides will be one main resource for this course. McElreath has a series of lectures on YouTube that are well worth watching: https://www.youtube.com/playlist?list=UUNJK6_DZvcMqNSzQdEkzvzA (Links to an external site.). Additionally, this material, https://bookdown.org/content/4857/ (Links to an external site.) provide R code using brms and tidyverse, two R packages to recreate figures and tables in McElreath (2018). Calendar and Outline Date Topics Assignment Jan 7 Course introduction; Why Bayesian; Bayesian versus Frequentist; Bayesian analysis used in clinical research Jan 14 Review of probability and other statistics concepts Jan 21 Introduction to Bayesian approach Assignment 1: hand out Jan 28 Bayesian Inference Feb 4 Priors (guest lecturer: Dr. Sindu Johnson ) Assignment 1: due Feb 8 Feb 11 Bayesian estimation Feb 18 Normal Models and Linear Regression Assignment 2: hand out Feb 25 Reading week - no lecture Mar 4 Hierarchical models and convergence Mar 11 Models for Binary Data Assignment 2: due Mar 15 Mar 18 Intro to Bayesian Meta-analysis and Bayesian Network Meta-analysis (guest lecturer: Juan Pablo) Assignment 3: hand out Mar 25 Models for Count Data Assignment 3: due Apr 5 Accessibility and Accommodations The University provides academic accommodations for students with disabilities in accordance with the terms of the Ontario Human Rights Code. This occurs through a collaborative process that acknowledges a collective obligation to develop an accessible learning environment that both meets the needs of students and preserves the essential academic requirements of the Universitys courses and programs. For more information, or to register with Accessibility Services, please visit: http://studentlife.utoronto.ca/as. Academic Integrity Academic integrity is essential to the pursuit of learning and scholarship in a university, and to ensuring that a degree from the University of Toronto is a strong signal of each students individual academic achievement. As a result, the University treats cases of cheating and plagiarism very seriously. Help and information is available on the Academic Integrity website. The University of Torontos Code of Behaviour on Academic Matters (http://www.governingcouncil.utoronto.ca/policies/behaveac.htm) outlines the behaviours that constitute academic dishonesty and the processes for addressing academic offences. Key Resources and Supports for DSLPH Graduate Students U of T Student Mental Health Resources U of T Graduate Student Union DLSPH Covid Information DLSPH Student Resources(Policies, Financial Aid, Health and Wellness, etc.) DLSPH Student Handbook License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],["into.html", "Session 1 Introduction to the course 1.1 About me 1.2 Syllabus 1.3 Some history 1.4 Thinking like a Bayesian using the concept of probability", " Session 1 Introduction to the course Students and instructor introduction Going over course syllabus A brief history of Bayesian inference The Bayesian way of thinking (statistical rethinking?) 1.1 About me I am an Assistant Professor in Health Services Research (outcomes and evaluation method emphasis) at IHPME. I also hold a cross-appointment at the Division of Biostatistics. I received my PhD in Biostatistics from U of T under the supervision of Dr. Eleanor Pullenayegum. My primary research focuses on developing methodology for statistical inference with complex longitudinal data in comparative effectiveness research. My areas of methodological interest include causal inference, Bayesian statistics, longitudinal data analysis, bias analysis, and semi-parametric/parametric joint modelling. If you are interested in working with me for your research/thesis projects, free feel to reach me. Besides academic work and creating fun data visualizations in R (Covid Dashboard; 3D Christmas Tree), I love watercolour painting, taking contemporary dance classes in Toronto, and playing piano as an adult beginner (though I am still waiting for my piano being shipped from Japan). My work for Dr. Burnss systematic review paper on the trustworthiness of published Clinical Practice Guidelines for Pharmacologic Treatments of Hospitalized Patients With COVID-19. Please introduce yourselves You can share your program, research interest, learning goals, and hobby. 1.2 Syllabus Detailed course syllabus is posted on Quercus. Some important notes Course schedule LECTURE: 12:05pm - 1:45pm on Fridays TUTORIAL: 2:00pm - 3:00pm on Fridays Ratio may change substantially some weeks 15 mins break between lecture and tutorial Zoom office hour Thursdays between 12:00pm - 1:00pm Begins on January 13, 2021, ends on March 31, 2021 I am happy to arrange for additional virtual office hours Course materials The lecture notes will be one main resource for this course. Online course note is open-access (GitHub Repository). Its licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Please do not use this material for commercial purposes. Course notes for latter sessions might not be available for view. I aim to publish the day before the scheduled lecture. Additional reading materials (e.g., articles) will be posted on Quercus Sample R scripts, data and brms code will be uploaded to Quercus. R and RStudio We will be using R via RStudio for this course. Key packages including tidyverse and brms. You are encouraged to write you assignment using Rmarkdown. Grading Participation under remote learning: accommodation and written explanations Assignments will be handed out and submitted on Quercus Can I work with peers? Auditing the course Welcome but with caveats (past experience from George) seriously committed (try not to miss classes) expected to have the same level of participation Quercus discussion board (Encourage!) 1.3 Some history 1.3.1 Bayesian history Concepts of Bayesian approach first appeared in the famous paper of Reverent Thomas Bayes (1702-1761), An essay towards solving a problem in the doctrine of chances (Bayes 1763). Thomas Bayes was a Presbyterian Minister who is thought to have been born in 1701 and died 1761. Bayesian approach were never really practiced in Bayes lifetime. His work was developed and popularized by Pierre-Simon Laplace (17491827) in the early 19th century. Though Bayes developed his philosophy during the 1740s, it wasnt until the late twentieth century that this work reached a broad audience. During the more than two centuries in between, the frequentist philosophy dominated statistical research and practice. Frequentist ideas came to predominate in the 20th century as a result of the work of Fisher (1890-1962), Neyman (1894-1981) and Egon Pearson (1984-1980). Reasons why the Bayesian approach did not catch on until relatively recently: debate of Bayesian philosophy and computational power. Bayes rule by Bayes He essentially used probabilistic terms to express arguments about the uncertainty for the parameter of a binomial model Given the number of times in which an unknown event has happened and failed: Required the chance that the probability of its happening in a single trial lies somewhere between any two degrees of probability that can be named. unknown event = e.g., Bernoulli trial probability of its happening in a single trial \\(=p\\) We may know ahead of time, or not (e.g. \\(p=0.5\\) for a fair coin) The Monty Hall Problem and the Bayesian solution by Marilyn vos Savant Suppose youre on a game show, and youre given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows whats behind the doors, opens another door, say No. 3, which has a goat. He then says to you, Do you want to pick door No. 2? Is it to your advantage to switch your choice? From wikipedia, Vos Savants response was that the contestant should switch to the other door. Many readers of vos Savants column refused to believe switching is beneficial and rejected her explanation. After the problem appeared in Parade, approximately 10,000 readers, including nearly 1,000 with PhDs, wrote to the magazine, most of them calling vos Savant wrong (Tierney 1991). We will revisit this problem in session 2 on Probability. I provided a short R simulation below to demonstrate why the player should choose to switch door - higher chance of winning the game. Monty Hall simulation in R set.seed(890123) monty &lt;- function(switch=F) { doors &lt;- sample(c(&#39;car&#39;, &#39;goat&#39;, &#39;goat&#39;), size=3, replace = FALSE) pick &lt;- sample(1:3, size=1, replace = FALSE) #randomly pick 1 door out of the three doors; # Using logic true or false to determine a win; if (switch==T) { # if you chose to switch, you win if the car is not behind the picked door; win &lt;- ifelse(doors[pick] != &#39;car&#39;, 1, 0) } else { # if you chose not to switch, you win if the car is behind the picked door; win &lt;- ifelse(doors[pick] == &#39;car&#39;, 1, 0) } return(win) #return the winning outcome for each game; } # run the game for 1000 iterations; # and record a vector of winning status for not switch and switch; no_switch_win &lt;- sapply(1:1000, FUN=function(i) monty(switch=F)) switch_win &lt;- sapply(1:1000, FUN=function(i) monty(switch=T)) print(paste(&quot;Chance of winning if we always not switch: &quot;, sum(no_switch_win)/1000)) ## [1] &quot;Chance of winning if we always not switch: 0.37&quot; print(paste(&quot;Chance of winning if we always switch: &quot;, sum(switch_win)/1000)) ## [1] &quot;Chance of winning if we always switch: 0.668&quot; The recent rise in popularity Advances in computing Re-evaluation of subjectivity of Bayesian approach due to its use of priors Key discussion on subjectivity by Gelman (Gelman and Hennig 2017) In discussions of the foundations of statistics, objectivity and subjectivity are seen as opposites. Objectivity is typically seen as a good thing; many see it as a major requirement for good science. Bayesian statistics is often presented as being subjective because of the choice of a prior distribution. It has been argued that the subjectiveobjective distinction is meaningless because all statistical methods, Bayesian or otherwise, require subjective choices, but the choice of prior distribution is sometimes held to be particularly subjective because, unlike the data model, it cannot be determined even in the asymptotic limit. 1.3.2 History of this course Prof. George Tomlinson created this course back in 2004 He taught this course from 2004 to 2017 and again from 2020 to 2021. Prof. Nicholas Mitsakakis taught it in 2019. This is my first time teaching this course. We are making history together =) I have borrowed a lot of course materials from George. Thank you George! Dr.Sindu Johnson and Juan Pablo will be our guest lecturers this year! 1.4 Thinking like a Bayesian using the concept of probability 1.4.1 Probability is not unitary Mathematically probability is clearly defined (a quantity between 0 and 1 etc., we will talk about this in session 2) There are a few ways of interpreting probability, most notably, long run relative frequency and subjective probability. 1. Long run relative frequency The classical or frequentist way of interpreting probability is based on the proportion of times an outcome occurs in a large (i.e. approaching infinity) number of repetitions of an experiment. This view has various serious problems, including that it requires the outcome to be observable and the experiment to be indeed repeatable! 2. Subjective probability The alternative way (essentially proposed by Bayes, LaPlace, other later prominent Bayesians) is the subjective probability. It encompasses ones views, opinions or beliefs about an event or statement. This event does not have to be repeatable or the outcome observable. It is based on belief. Using this approach, we can express the uncertainty around parameters using probability distributions (e.g., beta distribution for the risk \\(\\theta\\) of death after an operation) Example from (Johnson, Ott, and Dogucu, n.d.) Different interpretation of probability/uncertainty When flipping a fair coin, we say that the probability of flipping Heads is 0.5. How do you interpret this probability? If I flip this coin over and over, roughly 50% will be Heads. Heads and Tails are equally plausible. Both a and b make sense. An election is coming up and a pollster claims that candidate A has a 0.6 probability of winning. How do you interpret this probability? If we observe the election over and over, candidate A will win roughly 60% of the time. Candidate A is much more likely to win than to lose. The pollsters calculation is wrong. Candidate A will either win or lose, thus their probability of winning can only be 0 or 1. In the coin flip example, a Bayesian would conclude that Heads and Tails are equally likely. In contrast, a frequentist would conclude that if we flip the coin over and over and over, roughly 1/2 of these flips will be Heads. 1.4.2 Bayes Rule Bayes rule specifies how a prior probability \\(P(H)\\) of event \\(H\\) is updated in response to the evidence/data \\(E\\) to obtain the posterior probability \\(P(H|E)\\). \\[ P(H|E) = \\frac{P(E|H)P(H)}{P(E)} \\propto P(E|H)P(H) \\] Event \\(H\\) represents a particular hypothesis1 (or model or parameter) Event \\(E\\) represents observed evidence (or data or information) \\(P(H)\\) is the unconditional or prior probability of \\(H\\) (prior to observing \\(E\\)) \\(P(H|E)\\) is the conditional or posterior probability of \\(H\\) after observing evidence \\(E\\). \\(P(E|H)\\) is the likelihood of evidence \\(E\\) given hypothesis (or model or parameter) \\(H\\) The symbol \\(\\propto\\) proportional to, i.e., the right hand side needs to be divided by a normalizing factor (commonly viewed as a constant under Bayesian framework). The Bayesian approach provides a probability of the hypothesis given the data/evidence, which is something generally highly desirable from a scientific perspective. 1.4.3 The Scientific Method in steps Define the question or problem Assess the currently available information decide whether it is sufficient or not 2.1 If yes: conclusions, decisions, actions 2.2 If no, proceed to step 3. Determine what additional information is needed and design a study or experiment to obtain it Carry out the study designed in step 3. Use the data obtained in step 4 to update what was previously known. Return to step 2. Bayesian Advantages: Bayesian methods are especially good in step 2 and 5 They can be used for representing well existing knowledge and also to explicitly update uncertainty using new evidence. This is a done in a coherent, natural way. The ELISA test for HIV was widely used in the mid-1990s for screening blood donations. As with most medical diagnostic tests, the ELISA test is not perfect. If a person actually carries the HIV virus, experts estimate that this test gives a positive result 97.7% of the time. (This number is called the sensitivity of the test.) If a person does not carry the HIV virus, ELISA gives a negative (correct) result 92.6% of the time (the specificity of the test). Assume the estimated HIV prevalence at the time was 0.5% of (the prior base rate). Given a randomly selected individual tested positive; we are interested in the conditional probability that the person actually carries the virus. Suppose a randomly selected individual tested positive: If we know the likelihood of getting a positive test if HIV is truly present, \\(P(T^+ \\mid D^+) = 0.977\\), and the likelihood of getting a positive test if HIV is not present, \\(P(T^+ \\mid D^-) = 1 - 0.926 = 0.074\\), Using the Bayes Rule, we can obtain the posterior probability of the disease given the positive test result, \\[\\begin{aligned} P(D^+ \\mid T^+) &amp; = \\frac{P(T^+ \\mid D^+)P(D^+)}{P(T^+)} \\\\ &amp; = P(T^+ \\mid D^+) \\frac{P(D^+)}{P(T^+)}\\\\ \\Bigg( \\begin{matrix} \\text{Probability of} \\\\ \\text{HIV given} \\\\ \\text{a positive} \\\\ \\text{test} \\end{matrix} \\Bigg) &amp; = \\Bigg(\\begin{matrix} \\text{Probability of} \\\\ \\text{a positive} \\\\ \\text{test among} \\\\ \\text{HIV patients} \\end{matrix} \\Bigg) \\times \\frac{\\text{Probability of having HIV}}{\\text{Probability of a positive test}} \\\\ &amp; \\propto \\Bigg(\\begin{matrix} \\text{Probability of} \\\\ \\text{a positive} \\\\ \\text{test among} \\\\ \\text{HIV patients} \\end{matrix} \\Bigg) \\times \\text{Probability of having HIV} \\end{aligned}\\] We can see here the posterior probability is an updated belief! To summarize conceptually we have some belief about the state of the world, expressed as a mathematical model (such as the linear model used in regression). The Bayesian approach provides an updated belief as a weighted combination of prior beliefs regarding that state and the currently available evidence, with the possibility of the current evidence overwhelming prior beliefs, or prior beliefs remaining largely intact in the face of scant evidence. \\[ \\text{updated belief} = \\text{current evidence} \\times \\text{prior belief or evidence} \\] References "],["lab1-getting-started-with-r-rstudio.html", "Lab1 Getting started with R &amp; RStudio 1.5 R and RStudio Installation 1.6 R Packages 1.7 Working in RStudio 1.8 Basic R (a crash introduction)", " Lab1 Getting started with R &amp; RStudio Successfully install R and RStudio Install tidyverse and brms Try out some base R code Play with data frame and produce some summary statistics Learn about Rmarkdown with live demo by Juan Pablo 1.5 R and RStudio Installation R is a language and environment for statistical computing and graphics (https://cran.r-project.org/manuals.html). Many users of R like a tool called RStudio (https://www.rstudio.com/). This software is what is called an Integrated Development Environment (IDE) for R. It has several nice features, including docked windows for your console and syntax-highlighting editor that supports direct code execution, as well as tools for plotting and workspace management. 1.5.1 Windows operating system install R, https://cran.r-project.org/bin/windows/base/ install RStudio, https://www.rstudio.com/products/rstudio/download/#download YouTube Instruction 1.5.2 macOS operating system install R, https://cran.r-project.org/bin/macosx/ install RStudio, https://www.rstudio.com/products/rstudio/download/#download (select macOS 10.14+ option) YouTube Instruction 1.6 R Packages Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data.(Wickham 2015) To load the functions in a given package, we first have to install the package. We do this using the install.packages() function. Run the line of code that installs the tidyverse package below by removing the # at the start of the second line to uncomment the code. R will install the package to a default directory on your computer. If any dialogue box prompts you to set up a personal library instead, click yes. Once we have the package installed, we must load the functions from this library so we can use them within R. # install.packages(tidyverse, dependencies = T) #uncomment this line if you haven&#39;t installed this package; library(tidyverse) # load package library The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures (https://www.tidyverse.org/). The core packages are ggplot2 (data visualization), dplyr(dataframe manipulation), tidyr(data reshaping), readr(reading datasets), purrr (function and iterations) and tibble(dataframe). 1.6.1 Bayesian Analysis in R using brms package The course will mainly use the brms package in R(Bürkner 2017), which offers a standard R-modelling type interface to the underlying computing engine Stan. Direct use of Stan is not ideal for teaching Bayesian methods. The brms package automatically writes Stan code that can be viewed and edited, so after learning brms, the enterprising student may want to use this Stan code as a steppingstone toward programming directly in Stan. The brms package can be installed and loaded in the same way as any other R package, in this case by typing the following commands in R: #uncomment this line if you haven&#39;t installed this package; # install.packages(brms) library(brms) 1.7 Working in RStudio 1.7.1 RStudio layout When you open RStudio, your interface is made up of four panes as shown below. These can be organised via menu options View &gt; Panes &gt; RStudio layout We can run code in the console at the prompt where R will evaluate it and print the results. However, the best practice is to write your code in a new script file so it can be saved, edited, and reproduced. To open a new script, we select File &gt; New File &gt; R Script. To run code that was written in the script file, you can highlight the code lines you wish be evaluated and press CTRL-Enter (windows) or Cmd+Return (Mac). Additionally, You can comment or uncomment script lines by pressing Ctrl+Shift+C (windows) or Cmd+Shift+C (Mac). The comment operator in R is #. You can find more RStuio default keyboard shortcuts here. In our first tutorial, we will also introduce Rmarkdown, a R version of the markdown file editor that can write and output document in html, word, or pdf format that contents not only the programming code but also any evaluation outputs and graphs. To read more about Rmarkdown, please visit https://rmarkdown.rstudio.com/lesson-1.html. 1.7.2 Customization You can customize your RStudio session under the Options dialog Tools &gt; Global Options menu (or RStudio &gt; Preferences on a Mac). A list of customization categories can be found here, https://support.rstudio.com/hc/en-us/articles/200549016-Customizing-RStudio. For example, its popular to change RStudio appearance and themes (e.g., mordern, sky, dark, and classic). 1.7.3 Working directory The working directory is the default location where R will look for files you want to load and where it will put any files you save. You can use function getwd() to display your current working directory and use function setwd() to set your workding directory to a new folder on your computer. One of the great things about using RStudio Projects is that when you open a project it will automatically set your working directory to the appropriate location. getwd() #show my current working directory; ## [1] &quot;D:/GitHub/bayes_bookdown&quot; 1.7.4 Getting help with R The help section of R is extremely useful if you need more information about the packages and functions that you are currently loaded. You can initiate R help using the help function help() or ?, the help operator. help(brms) 1.8 Basic R (a crash introduction) A more comprehensive introduction to base R can be found at https://cran.r-project.org/doc/manuals/r-release/R-intro.html. In this subsection, I will briefly outline some common R functions and commands for arithmetic, creating and working with object, vector, matrix, and data. This short introduction is created using the intro to R workshop notes by Prof. Kevin Thorpe as well as multiple open-source materials. Some important notes R is case sensitive. Commands are separated by a newline in the console. The # character can be used to make comments. R doesnt execute the rest of the line after the # symbol - it ignores it. Previous session commands can be accessed via the up and down arrow keys on the keyboard. When naming in R, avoid using spaces and special characters (i.e., !@#$%^&amp;*()_+=;:&lt;&gt;?/) and avoid leading names with numbers. 1.8.1 Arithmetic 2*3 2^3 2 + (2 + 3) * 2 - 5 log(3) exp(3) log(exp(1)) #playing with Euler&#39;s number; sqrt(x) #Logical operators; 5&gt;6 5&lt;=6 5==6 #equal; 6==6 5!=6 #not equal; Rounding Issues in R Try evaluating log(0.01^200) and 200*log(0.01) in R. Note that they are mathematically equivalent. log(0.01^200) ## [1] -Inf 200*log(0.01) ## [1] -921.034 1.8.2 Vectors Operator &lt;- is called the assignment operation, we can create a vector (numeric, characteristic, or mixture) using the assignment operation and the c() function. # a vector of a single element; x &lt;- 3 x # a character vector x &lt;- c(&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;) x length(x) nchar(x) #number of characters for each element; # encode a vector as a factor (or category); y &lt;- factor(c(&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;red&quot;, &quot;green&quot;)) y class(y) as.numeric(y) # we can return factors with numeric labels; # we can also label numeric vector with factor levels; z &lt;- factor(c(1,2,3,1,1,2), levels = c(1,2,3), labels = c(&quot;red&quot;, &quot;green&quot;,&quot;yellow&quot;)) z class(z) mode(z) #we can use this to create dummy variables for regression; contrasts(z) # a numeric vector; x &lt;- c(10.4, 5.6, 3.1, 6.4, 21.7, 53.5, 3.6, 2.6, 6.1, 1.7) x x[2] x[1:3] x[-1] x[-(1:3)] length(x) #return number of elements; # a numeric vector composed of all integers between 1 and 10; y &lt;- 1:10 y # a numeric vector composed of all even number integers between 0 and 10; z &lt;- seq(0,10, by=2) z # simple vector based calculations; x + y x*y x/y # matrix in R; matrix(1:16, nrow=4) matrix(1:16, nrow=4, byrow=TRUE) matrix(1:16, nrow=4) diag(matrix(1:16, nrow=4)) diag(c(1,-3,7)) diag(3) # matrix calculation; X &lt;- matrix(1:16, nrow=4, byrow=T) X t(X) #transpose; Y &lt;- matrix(seq(1,32, by=2), nrow=4, byrow=T) Y Y + X Y - X 3 * X X * Y X %*% Y #inner product; 1.8.3 Data frame - The Titanic dataset Titanic &lt;- read.csv(&#39;data/Titanic.csv&#39;, header = TRUE, na.strings = &quot;NA&quot;) knitr::kable(rbind(head(Titanic), tail(Titanic)), row.names = FALSE) Id Name PClass Age Sex Survived SexCode 1 Allen, Miss Elisabeth Walton 1st 29.00 female 1 1 2 Allison, Miss Helen Loraine 1st 2.00 female 0 1 3 Allison, Mr Hudson Joshua Creighton 1st 30.00 male 0 0 4 Allison, Mrs Hudson JC (Bessie Waldo Daniels) 1st 25.00 female 0 1 5 Allison, Master Hudson Trevor 1st 0.92 male 1 0 6 Anderson, Mr Harry 1st 47.00 male 1 0 1308 Zabour, Miss Tamini 3rd NA female 0 1 1309 Zakarian, Mr Artun 3rd 27.00 male 0 0 1310 Zakarian, Mr Maprieder 3rd 26.00 male 0 0 1311 Zenni, Mr Philip 3rd 22.00 male 0 0 1312 Lievens, Mr Rene 3rd 24.00 male 0 0 1313 Zimmerman, Leo 3rd 29.00 male 0 0 Reading data create a local folder, HAD5314H_Bayesian_2022, in your PC where you will store the Titan.csv file downloaded from Quercus set your working directory to this folder import the csv data to your session dim(Titanic) str(Titanic) names(Titanic) Titanic$Age Titanic[,c(&quot;Age&quot;)] Titanic[2:3, 2:3] # some quick dplyr data manipulation; Titanic %&gt;% filter(PClass == &quot;1st&quot;) %&gt;% # filter on 1st class; passengers; select(Id, Name, Age, Sex, Survived) # select these; columns only; # looking at the distinct values; Titanic %&gt;% distinct(PClass) table(Titanic$PClass, useNA = &quot;always&quot;) # counting missing values; Titanic %&gt;% summarise(count = sum(is.na(PClass))) # quick summary; mean(Titanic$Age) sum(is.na(Titanic$Age)) # counting missing values; mean(Titanic$Age, na.rm = TRUE) median(Titanic$Age, na.rm = TRUE) quantile(Titanic$Age, probs =c(0.25,0.75), na.rm = TRUE) Titanic %&gt;% summarise(mean = mean(Age, na.rm = TRUE), na = sum(is.na(Age)), med = median(Age, na.rm = TRUE)) # summary by group; Titanic %&gt;% group_by(PClass) %&gt;% summarise(mean = mean(Age, na.rm = TRUE), na = sum(is.na(Age)), med = median(Age, na.rm = TRUE)) Summarize age by survival status Using the example code above, please calculate the mean and median age by survival status. Can you figure out how to get IQR with the pipe operator %&gt;% in dlypr? 1.8.4 Simple plots boxplot(Age~PClass, data=Titanic) p &lt;- ggplot(Titanic, aes(x=as.factor(PClass), y=Age)) + geom_boxplot() p ggplot(Titanic, aes(x=as.factor(PClass), y=Age, fill = as.factor(PClass))) + geom_boxplot() + geom_jitter(shape=16, position=position_jitter(0.2)) + labs(title=&quot;Plot of age by passenger class&quot;,x=&quot;Passenger Class&quot;, y = &quot;Age&quot;, fill = &quot;Passenger Class&quot;)+ theme_classic() Plot age distribution by survival status Using the example code above, please generate a boxplot of age by survival status. R Session information ## R version 4.0.5 (2021-03-31) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Matrix products: ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 ## [3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.16.3 Rcpp_1.0.7 forcats_0.5.1 stringr_1.4.0 ## [5] dplyr_1.0.7 purrr_0.3.4 readr_2.1.1 tidyr_1.1.4 ## [9] tibble_3.1.6 ggplot2_3.3.5 tidyverse_1.3.1 References "],["prob.html", "Session 2 Probability, random variables and distributions 2.1 Probability 2.2 Probability Distributions", " Session 2 Probability, random variables and distributions Review of probability terminologies, probability rules, and Venn Diagrams Review conditional probability, independence, and Bayes theorem. Review on random variables and common probability distributions Probability Terminology 2.1 Probability Probability has a central place in Bayesian analysis we put a prior probability distribution on the unknowns (parameters), we model the observed data with a probability distribution (likelihood), and we combine the two into a posterior probability distribution Probability Terminology(Evans and Rosenthal 2004) Sample space This set of all possible outcomes of an experiment/trial is known as the sample space of the experiment/trial and is denoted by \\(S\\) or \\(\\Omega\\). Experiment/Trial: each occasion we observe a random phenomenon that we know what outcomes can occur, but we do not know which outcome will occur Event: Any subset of the sample space \\(S\\) is known as an event, denoted by \\(A\\). Note that, \\(A\\) is also a collection of one or more outcomes. Probability defined on events: For each event \\(A\\) of the sample space \\(S\\), \\(P(A)\\), the probability of the event \\(A\\), satisfies the following three conditions: \\(0 \\leq P(A) \\leq 1\\) \\(P(S) = 1\\) and \\(P(\\emptyset) = 0\\); \\(\\emptyset\\) denotes the empty set \\(P\\) is (countably) additive, meaning that if \\(A_1, A_2, \\ldots\\) is a finite or countable sequence of disjoint (also known as mutually exclusive events), then \\[P(A_1 \\cup A_2 \\cup \\ldots ) = P(A_1)+P(A_2)+\\ldots \\] Union: Denote the event that either \\(A\\) or \\(B\\) occurs as \\(A\\cup B\\). Intersection: Denote the event that both \\(A\\) and \\(B\\) occur as \\(A\\cap B\\) Complement: Denote the event that \\(A\\) does not occur as \\(\\bar{A}\\) or \\(A^{C}\\) or \\(A^\\prime\\) (different people use different notations) Disjoint (or mutually exclusive): Two events \\(A\\) and \\(B\\) are said to be disjoint if the occurrence of one event precludes the occurrence of the other. If two events are mutually exclusive, then \\(P(A\\cup B)=P(A)+P(B)\\) Sample Space If the experiment consists of the flipping of a coin, then \\[ S = \\{H, T\\}\\] If the experiment consists of rolling a die, then the sample space is \\[ S = \\{1, 2, 3, 4, 5, 6\\}\\] If the experiments consists of flipping two coins, then the sample space consists of the following four points: \\[ S = \\{(H,H), (H,T), (T,H), (T,T)\\}\\] Event In Example (1), if E = {H}, then E is the event that a head appears on the flip of the coin. Similarly, if \\(E = \\{T \\}\\), then \\(E\\) would be the event that a tail appears In Example (2), if \\(E = \\{1\\}\\), then \\(E\\) is the event that one appears on the roll of the die. If \\(E = \\{2, 4, 6\\}\\), then \\(E\\) would be the event that an even number appears on the roll. In Example (3), if \\(E = \\{(H, H), (H, T )\\}\\), then \\(E\\) is the event that a head appears on the first coin. Probability of an event. Let \\(R\\) be the sum of two standard dice. Suppose we are interested in \\(P(R \\le 4)\\). Notice that the pair of dice can fall 36 different ways (6 ways for the first die and six for the second results in 6x6 possible outcomes, and each way has equal probability \\(1/36\\). \\[\\begin{aligned} P(R \\le 4 ) &amp;= P(R=2)+P(R=3)+P(R=4) \\\\ &amp;= P(\\left\\{ 1,1\\right\\} )+P(\\left\\{ 1,2\\right\\} \\mathrm{\\textrm{ or }}\\left\\{ 2,1\\right\\} )+P(\\{1,3\\}\\textrm{ or }\\{2,2\\}\\textrm{ or }\\{3,1\\}) \\\\ &amp;= \\frac{1}{36}+\\frac{2}{36}+\\frac{3}{36} \\\\ &amp;= \\frac{6}{36} \\\\ &amp;= \\frac{1}{6} \\end{aligned}\\] 2.1.1 Venn Diagrams Venn diagrams provide a very useful graphical method for depicting the sample space S and subsets of it. Figure taken from (Cardinal 2019). Figure 2.1: Venn Diagram for two events Venn Diagram for two disjoint events How would you draw this venn diagram? Label sub-regions in a Venn Diagram for three events Using set theory, how would you write out areas a, d, and f ? A &lt;- c(&quot;a&quot;,&quot;b&quot;, &quot;d&quot;,&quot;e&quot;) B &lt;- c(&quot;b&quot;,&quot;c&quot;, &quot;d&quot;,&quot;f&quot;) C &lt;- c(&quot;d&quot;,&quot;e&quot;,&quot;f&quot;,&quot;g&quot;) x &lt;- list(A=A , B=B , C=C) v0 &lt;- venn.diagram( x, filename=NULL, fill = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;), alpha = 0.50, col = &quot;transparent&quot;) overlaps &lt;- calculate.overlap(x) # extract indexes of overlaps from list names indx &lt;- as.numeric(substr(names(overlaps),2,2)) # labels start at position 7 in the list for Venn&#39;s with 3 circles for (i in 1:length(overlaps)){ v0[[6 + indx[i] ]]$label &lt;- paste(overlaps[[i]], collapse = &quot;\\n&quot;) } grid.draw(v0) 2.1.2 Probability Rules General Addition Rule: \\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\) The reason behind this fact is that if there is if \\(A\\) and \\(B\\) are not disjoint, then some area is added twice when we calculate \\(P\\left(A\\right)+P\\left(B\\right)\\). To account for this, we simply subtract off the area that was double counted. If \\(A\\) and \\(B\\) are disjoint, \\(P(A\\cup B)=P(A)+P(B)\\). Complement Rule: \\(P(A)+P(A^c)=1\\) This rule follows from the partitioning of the set of all events (\\(S\\)) into two disjoint sets, \\(A\\) and \\(A^c\\). We learned above that \\(A \\cup A^c = S\\) and that \\(P(S) = 1\\). Combining those statements, we obtain the complement rule. Completeness Rule: \\(P(A)=P(A\\cap B)+P(A\\cap B^c)\\) This identity is just breaking the event \\(A\\) into two disjoint pieces. Law of total probability (unconditioned version): Let \\(A_1, A_2, \\ldots\\) be events that form a partition of the sample space \\(S\\), Let \\(B\\) be any event. Then, \\[P(B) = P(A_1 \\cap B) + P(A_2 \\cap B) + \\ldots. \\] This law is key in deriving the marginal event probability in Bayes rule. Recall the HIV example from session 1, we have \\(P(T^+) = P(T^+ \\cap D^+)+P(T^+ \\cap D^-)\\). Figure 2.2: Demonstrate Law of total probabiliy using Venn Diagram Law of Total Probability suppose I know that whenever it rains there is 10% chance of being late at work, while the chance is only 1% if it does not rain. Suppose that there is 30% chance of raining tomorrow; what is the chance of being late at work? Denote with event \\(A\\) as I will be late to work tomorrow and event \\(B\\) as It is going to rain tomorrow \\[\\begin{aligned} P(A) &amp;= P(A\\mid B)P(B) + P(A \\mid B^c) P(B^c) \\\\ &amp;= 0.1\\times 0.3+0.01\\times 0.7=0.037 \\end{aligned}\\] Conditional Probability: The probability of even \\(A\\) occurring under the restriction that \\(B\\) is true is called the conditional probability of \\(A\\) given \\(B\\). Denoted as \\(P(A|B)\\). In general we define conditional probability (assuming \\(P(B) \\ne 0\\)) as \\[P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\] which can also be rearranged to show \\[\\begin{aligned} P(A\\cap B) &amp;= P(A\\,|\\,B)\\,P(B) \\\\ &amp;= P(B\\,|\\,A)\\,P(A) \\end{aligned}\\] Because the order doesnt matter and \\(P\\left(A\\cap B\\right)=P\\left(B\\cap A\\right)\\). \\(P(A|B) = 1\\) means that the event \\(B\\) implies the event \\(A\\). \\(P(A|B) = 0\\) means that the event \\(B\\) excludes the possibility of event \\(A\\). Independent: Two events \\(A\\) and \\(B\\) are said to be independent if \\(P(A\\cap B)=P(A)P(B)\\). What independence is saying that knowing the outcome of event \\(A\\) doesnt give you any information about the outcome of event \\(B\\). If \\(A\\) and \\(B\\) are independent events, then \\(P(A|B) = P(A)\\) and \\(P(B|A) = P(B)\\). In simple random sampling, we assume that any two samples are independent. In cluster sampling, we assume that samples within a cluster are not independent, but clusters are independent of each other. Assumptions of independence and non-independence in statistical modelling are important. In linear regression, for example, correct standard error estimates rely on independence amongst observations. In analysis of clustered data, non-independence means that standard regression techniques are problematic. Bayes Rule: This arises naturally from the rule on conditional probability. Since the order does not matter in \\(A \\cap B\\), we can rewrite the equation: \\[\\begin{aligned} P(A \\cap B) &amp;= P(B \\cap A) \\\\ P(A\\mid B)P(B) &amp;= P(B\\mid A)P(A) \\\\ P(A\\mid B) &amp;= \\frac{P(B\\mid A)P(A) }{P(B)}\\\\ &amp;= \\frac{P(B\\mid A)P(A) }{P(B\\mid A)P(A) + P(B\\mid A^C)P(A^C)} \\end{aligned}\\] 2.1.3 How to define and assign probabilities in general? Frequency-type (Empirical Probability): based on the idea of frequency or long-run frequency of an event. Observe sequence of coin tosses (trials) and the count number of times of event \\(A=\\{H\\}\\). The relative frequency of \\(A\\) is given as, \\[ P(A) = \\frac{\\text{Number of times A occurs}}{\\text{Total number of trials}}. \\] Trials are independent. Relative frequency is unpredictable in short-term, but in long-run its predicable. Let \\(n\\) be the total number of trials and \\(m\\) be number of heads, then we have \\[ P(A) = \\lim_{n \\rightarrow \\infty} \\frac{m}{n}.\\] Theoretical Probability: Sometimes \\(P(A)\\) can be deduced from mathematical model using uniform probability property on finite spaces. If the sample space S is finite, then one possible probability measure on \\(S\\) is the uniform probability measure, which assigns probability \\(1/|S|\\), equal probability to each outcome. Here |S| is the number of elements in the sample space \\(S\\). By additivity, it then follows that for any event \\(A\\) we have, \\[ P(A) = \\frac{|A|}{|S|} = \\frac{\\text{Number of outcomes in S that satisfy A}}{\\text{Total number of outcomes in S} } \\] Long-term frequency It is natural to think of large sequences of similar events defining frequency-type probability if we allow the number of trials to be indefinitely large. n = 1000 pHeads =0.5 set.seed(123) flipSequence = sample( x=c(0,1),prob = c(1-pHeads,pHeads),size=n,replace=T) r = cumsum(flipSequence) n= 1:n runProp = r/n flip_data &lt;- data.frame(run=1:1000,prop=runProp) ggplot(flip_data,aes(x=run,y=prop,frame=run)) + geom_path()+ xlim(1,1000)+ylim(0.0,1.0)+ geom_hline(yintercept = 0.5)+ ylab(&quot;Proportion of Heads&quot;)+ xlab(&quot;Number of Flips&quot;)+ theme_bw() Figure 2.3: Demonstrate law of large number with the coin tossing example Belief-type (Subjective Probability): based on the idea of degree of belief (weight of evidence) about an event where the scale is anchored at certain (=1) and impossible (=0). Subjective Probability Consider these events We will have more than 100cm of snow this winter. An asteroid is responsible for the extinction of the dinosaurs Mammography reduces the rate of death from breast cancer in women over 50 by more than 10% The 70 year old man in your office, just diagnosed with lung cancer, will live at least 5 years Can you think of them in terms of long-run frequencies? We cannot always think in terms of long-run frequencies Also, we may not care about long-run frequencies How are they relevant to this patient, this hypothesis? Even where long-run frequency could apply, often there is no such data available However, opinions are formed and beliefs exist Which probability to use? For inference, the Bayesian approach relies mainly on the belief interpretation of probability. The laws of probability can be derived from some basic axioms that do not rely on long-run frequencies. The Monty Hall Problem Revisit Suppose youre on a game show, and youre given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows whats behind the doors, opens another door, say No. 3, which has a goat. He then says to you, Do you want to pick door No. 2? Is it to your advantage to switch your choice? Before any door is picked, we assume all three doors are equal likely to have a car behind it (prior probability of 1/3). Without loss of generality, let assume we pick door 1 and Monty opens door 2 (goat behind). \\(P( \\text{door 1 has car}) = P( \\text{door 2 has car}) = P( \\text{door 3 has car)} = \\frac{1}{3}\\) \\(P( \\text{Monty opens door 2} \\mid \\text{door 1 has car} ) = \\frac{1}{2}\\) \\(P( \\text{Monty opens door 2} \\mid \\text{door 2 has car} ) = 0\\) \\(P( \\text{Monty opens door 2} \\mid \\text{door 3 has car} ) = 1\\) We want to know these two probabilities: \\(P( \\text{door 1 has car} \\mid \\text{Monty opens door 2} )\\) and \\(P( \\text{door 3 has car} \\mid \\text{Monty opens door 2} )\\). Simplify notations, we have \\[\\begin{aligned} &amp; P( D_1 = \\text{car} \\mid \\text{open } D_2 ) \\\\ &amp; = \\frac{P( \\text{open } D_2 \\mid D_1 = \\text{car} )P(D_1 = \\text{car})}{P(\\text{open } D_2)} \\\\ &amp; = \\frac{P( \\text{open } D_2 \\mid D_1 = \\text{car} )P(D_1 = \\text{car})}{P( \\text{open } D_2 \\mid D_1 = \\text{car} )P(D_1 = \\text{car}) + P(\\text{open } D_2 \\mid D_3 = \\text{car})P(D_3 = \\text{car})} \\\\ &amp; = \\frac{\\frac{1}{2} \\times \\frac{1}{3}}{\\frac{1}{2} \\times \\frac{1}{3} + 1 \\times \\frac{1}{3}} = \\frac{1}{3} \\end{aligned}\\] \\[\\begin{aligned} &amp; P( D_3 = \\text{car} \\mid \\text{open } D_2 ) \\\\ &amp; = \\frac{P( \\text{open } D_2 \\mid D_3 = \\text{car} )P(D_3 = \\text{car})}{P(\\text{open } D_2)} \\\\ &amp; = \\frac{P( \\text{open } D_2 \\mid D_3 = \\text{car} )P(D_3 = \\text{car})}{P( \\text{open } D_2 \\mid D_3 = \\text{car} )P(D_3 = \\text{car}) + P(\\text{open } D_2 \\mid D_1 = \\text{car})P(D_1 = \\text{car})} \\\\ &amp; = \\frac{ 1 \\times \\frac{1}{3}}{ 1 \\times \\frac{1}{3} + \\frac{1}{2} \\times \\frac{1}{3}} = \\frac{2}{3} \\end{aligned}\\] Given we pick door 1 and Monty opens door 2 (goat behind), the probability of the car behind door 1 is 1/3 while the probability of the car behind door 3 is 2/3. Therefore, we should switch to door 3 for a better chance of winning. The ELISA test Problem Revisit The ELISA test for HIV was widely used in the mid-1990s for screening blood donations. As with most medical diagnostic tests, the ELISA test is not perfect. If a person actually carries the HIV virus, experts estimate that this test gives a positive result 97.7% of the time. (This number is called the sensitivity of the test.) If a person does not carry the HIV virus, ELISA gives a negative (correct) result 92.6% of the time (the specificity of the test). Assume the estimated HIV prevalence at the time was 0.5% of (the prior base rate). Suppose a randomly selected individual tested positive, please calculate \\(P(D^+ \\mid T^+)\\) and \\(P(D^- \\mid T^-)\\). \\[\\begin{aligned} P(D^+ \\mid T^+) &amp; = \\frac{P(D^+ \\text{and } T^+)}{P(T^+)}\\\\ &amp; = \\frac{\\text{Chance of HIV positive and test positive}}{\\text{Chance of test positive}}\\\\ &amp; = \\frac{P(T^+ \\mid D^+)P(D^+)}{P(T^+)} \\\\ &amp; = \\frac{P(T^+ \\mid D^+)P(D^+)}{P(T^+ \\mid D^+)P(D^+) + P(T^+ \\mid D^-)P(D^-)} \\\\ &amp; = \\frac{0.977 \\times 0.005}{0.977 \\times 0.005 + 0.074 \\times 0.995} = \\frac{0.0049}{0.0785} = 0.0622 \\end{aligned}\\] Demonstrate calculation in R hypothesis = c(&quot;Carries HIV&quot;, &quot;Does not carry HIV&quot;) prior = c(0.005, 1 - 0.005) likelihood = c(0.977, 1 - 0.926) # given positive test product = prior * likelihood posterior = product / sum(product) bayes_table = data.frame(hypothesis, prior, likelihood, product, posterior) %&gt;% add_row(hypothesis = &quot;Sum&quot;, prior = sum(prior), likelihood = NA, product = sum(product), posterior = sum(posterior)) knitr::kable(bayes_table, digits = 4, align = &#39;r&#39;) hypothesis prior likelihood product posterior Carries HIV 0.005 0.977 0.0049 0.0622 Does not carry HIV 0.995 0.074 0.0736 0.9378 Sum 1.000 NA 0.0785 1.0000 2.2 Probability Distributions Random variables are used to describe events and their probability. The formal definition just means that a random variable describes how numbers are attached to each possible elementary outcome. Random variables can be divided into two types: Continuous Random Variables: the variable takes on numerical values and could, in principle, take any of an uncountable number of values. These typically have values that cannot be counted, e.g. age, height, blood pressure, etc. Discrete Random Variables: the variable takes on one of small set of values (or only a countable number of outcomes). The value of a particular measurement can be thought of as being determined by chance, even though there may be no true randomness a patients survival time after surgery for prostate cancer distance an elderly person walks in 6 minutes known factors may give an expected value and variance unknown (or unknowable factors) lead to uncertainty given this mean the component that appears random to the analyst is modelled as being random. Probability distributions The true distribution of most random variables (outcomes) is not known. It is convenient to approximate the distribution by a function (probability distribution) with a small number of parameters. We focus our attention on these parameters, rather than the whole distribution. Statistical modelling that is based on estimating the parameters of distributions is called parametric modelling. All the Bayesian models we will look at are parametric models. All models are wrong but some are useful. Distance &lt;- 0:600 makePlot &lt;- function(frac=0.3){ df &lt;- data.frame(Distance=Distance, f1 = dnorm(Distance,240,80), f2 = dnorm(Distance,450,70), p = rep(frac, length(Distance))) df$f &lt;- df$p*df$f1 + (1-df$p)*df$f2 maxH &lt;- max(df$f)*1.05 p0 &lt;- ggplot(df, aes(Distance,f))+ geom_line(col=&quot;red&quot;,lwd=1)+ ylab(&quot;p(Distance)&quot;)+ xlim(c(0,600))+ theme_bw() p1 &lt;- p0+annotate(&quot;text&quot;, x=200, y=maxH, label=&quot;D ~ p(theta)&quot;, col=&quot;red&quot;, size=3.5) p2 &lt;- p0 + geom_line(data=df, aes(Distance,p*f1),col=&quot;blue&quot;,lwd=1)+ geom_line(data=df, aes(Distance,(1-p)*f2),col=&quot;orange&quot;)+ annotate(&quot;text&quot;,x=50,y=maxH*1.05,hjust=0, label=paste0(round(100*frac), &quot;%: D | Hospitalized ~ N(mu1, V1)&quot;), col=&quot;blue&quot;,size=3.5)+ annotate(&quot;text&quot;,x=50,y=maxH*1.1,hjust=0, label=paste0(round(100-100*frac), &quot;%: D | Not Hospitalized ~ N(mu1+delta, V2)&quot;), col=&quot;orange&quot;,size=3.5) ggarrange(p1, p2, ncol = 1, nrow = 2) } makePlot(frac = 0.3) 2.2.1 Probability density functions This is a function with two important properties The heights of the function at two different x-values indicate relatively how likely those x-values are. The area under the curve between any two x-values is the probability that a randomly sampled value will fall in that range The area under the whole curve is equal to the total probability, i.e. equal to 1 From histograms to PDFs With a large enough sample, and narrow enough intervals, histogram starts to look smooth. The relative heights of the histogram at each blood pressure value tell us how likely these values are. For example, near 120 mm Hg there are about 800 people in each 5 mm interval. As the width of the interval approaches zero, the probability of being in that interval approaches zero. e.g. The event that BP is exactly 129 has zero probability, with \\[P(BP = 129) \\approx P(128.99999 &lt; BP &lt; 129.00001) \\approx 0.\\] For a continuous random variable, we describe the shape of the distribution with a smooth curve (pdf) instead of a histogram. The relative height of the curve at any two points indicates the relative likelihood of the two values. The height is not a probability. The area under the curve between any two points is the probability that a randomly sampled value will fall in that range. e.g., the shaded area represent = 0.5. In fact, the probability of a BP from this population being between 112 and 145 is 0.5. Useful Probability Distributions Discrete random variables and distributions means that the random variable can take on only a countable number of values. We will describe these distributions using probability mass function. Binomial Multinomial Poisson Negative binomial Continuous random variables and distributions: We will describe these distributions using probability density function. Uniform Normal Beta Gamma The expectation for discrete distributions \\[E[X]= \\sum_{x=0}^{n}x\\,P(X=x) \\] and the variance for discrete distributions \\[V[X]=\\sum_{x=0}^{n}\\left(x-E\\left[X\\right]\\right)^{2}\\,P(X=x) \\] The expectation for continuous distributions \\[E[X]= \\int_{a}^{b}x\\,f(x) dx\\] and the variance for continuous distributions \\[V[X]= E[(X-E(X))^2] = E(X^2) - E(X)^2\\] and \\[E[X^2]= \\int_{a}^{b}x^2\\,f(x) dx\\] 2.2.2 Discrete Distributions 2.2.2.1 Bernoulli Distribution A single binary outcome Y can be represented as taking on the values 0 or 1. Of course, this could be success/failure, return/did not return, etc. There is a single parameter - the probability that the variable takes on the value 1 (probability of success), denoted as \\(p\\). \\(P(Y=1) = p\\) and \\(P(Y=0) = 1 - p\\) The probability mass function is \\[P(Y=y) = p^y (1-p)^{1-y}, y \\in \\{0,1 \\}\\] Daniel Bernoulli FRS (1700-1782) was a Swiss mathematician and physicist and was one of the many prominent mathematicians in the Bernoulli family from Basel. 2.2.2.2 Binomial Distribution One of the most important discrete distribution used in statistics is the binomial distribution. This is the distribution which counts the number of heads in \\(n\\) independent coin tosses where each individual coin toss has the probability \\(p\\) of being a head. The same distribution is useful when not just in tossing coins, for example, when taking random samples from a disease cohort and interested in the number of patients in this sample that received surgery. A binomial experiment is one that: Experiment consists of \\(n\\) identical trials. Each trial results in one of two outcomes (Heads/Tails, presence/absence). One will be labeled a success and the other a failure. The probability of success on a single trial is equal to \\(\\p\\) and remains the same from trial to trial. The trials are independent (this is implied from property 3). The random variable \\(X\\) is the number of successes observed during \\(n\\) trials. The Binomial Probability Mass Function The probability mass function for the binomial distribution is \\[ P(X=x) = \\binom{n}{x} p^x(1-p)^{n-x}, \\ \\text{for } x=0,\\ldots,n \\] where \\(n\\) is a positive integer and \\(p\\) is a probability between 0 and 1. This probability mass function uses the expression \\(\\binom{n}{x}\\), called a binomial coefficient \\[ \\binom{n}{x} = \\frac{n!}{x!(n-x)!} \\] which counts the number of ways to choose \\(x\\) things from \\(n\\). The mean of the binomial distribution is \\(E(X) = np\\) and the variance is \\(Var(X) = np(1-p)\\). dist &lt;- data.frame( x=0:10 ) %&gt;% mutate(probability = dbinom(x, size=10, prob=0.5)) ggplot(dist, aes(x=x)) + geom_point(aes(y=probability)) + geom_linerange(aes(ymax=probability, ymin=0)) + scale_x_continuous(breaks=seq(0,10,1))+ ggtitle(&#39;Binomial distribution: n=10, p=0.5&#39;) + theme_bw() We can use R to calculate binomial probabilities. In general, for any distribution, the d-function gives the distribution function (pmf or pdf). \\(P( x = 5 | n=10, p=0.8 ) = 0.02642412\\) from dbinom(5, size=10, prob=0.8) and \\(P( x = 8 | n=10, p=0.8 )=0.3019899\\) from dbinom(8, size=10, prob=0.8). 2.2.2.3 Multinomial Distribution If a random variable \\(Y\\) can take on one of \\(K\\) categories and the probability of each category \\(k\\) is given by the set of probabilities \\(p_1, p_2, \\ldots, p_K\\) and \\(\\sum_{k=1}^K p_k = 1\\). The probability that the single observation \\(Y\\) is in category \\(k\\) is given by \\(P(Y=k) = p_k\\). With \\(n\\) observations, we are interested in the number that fall into each of the \\(k\\) categories. The random variable \\(Y\\), thus follows a multinomial distribution. The multinomial Probability Mass Function The probability mass function for the multinomial distribution is \\[P(y_1, y_2, \\ldots, y_K) = \\frac{n! }{y_1!, y_2!, \\ldots, y_K!} p_1^{y_1} p_2^{y_2} \\cdots p_K^{y_K} \\] This is a useful distribution to use as a starting point for modelling categorical data with a relatively small number (&gt; 2) of possible values. Example Suppose our outcome (random variable) is severity level This can take on the values (1) none/mild ; (2) moderate; and (3) severe. Let the probabilities for the three categories be \\(p_1\\),\\(p_2\\), and \\(p_3\\). \\[P(Y = 1) = p_1; \\ P(Y = 2) = p_2;\\ P(Y = 3) = p_3 = 1 - (p_1+p_2);\\] if we observe 100 subjects, with 20, 65 and 15 in the three categories, \\[ P(20,65,15) = \\frac{100! }{20! 65!15!} p_1^{20} p_2^{65} p_3^{16}\\] if we assume \\(p_1=0.2\\), \\(p_2 = 0.6\\), \\(p_3 = 0.2\\), the above probability is 0.004644407 from R with dmultinom(x=c(20,65,15),prob=c(0.2,0.6,0.2)). 2.2.2.4 Poisson Distribution A commonly used distribution for count data is the Poisson. e.g., number of patients arriving at ER over a 12 hr interval. The following conditions apply: Two or more events do not occur at precisely the same time or in the same space The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a non overlapping period or region. The expected number of events during one period , \\(\\lambda\\), is the same in all periods or regions of the same size. Assuming that these conditions hold for some count variable \\(Y\\), the the probability mass function is given by \\[P(Y=y)=\\frac{\\lambda^{y}e^{-\\lambda}}{y!}\\] where \\(\\lambda\\) is the expected number of events over 1 unit of time or space and \\(e\\) is the eulers number \\(2.718281828\\dots\\). \\[E[Y] = \\lambda\\] \\[Var[Y] = \\lambda\\] Example: Suppose we are interested in the population size of small mammals in a region. Let \\(Y\\) be the number of small mammals caught in a large trap over a 12 hour period. Finally, suppose that \\(Y\\sim Poisson(\\lambda=2.3)\\). What is the probability of finding exactly 4 critters in our trap? \\[P(Y=4) = \\frac{2.3^{4}\\,e^{-2.3}}{4!} = 0.1169\\] What about the probability of finding at most 4? \\[\\begin{aligned} P(Y\\le4) &amp;= P(Y=0)+P(Y=1)+P(Y=2)+P(Y=3)+P(Y=4) \\\\ &amp;= 0.1003+0.2306+0.2652+0.2033+0.1169 \\\\ &amp;= 0.9163 \\end{aligned}\\] What about the probability of finding 5 or more? \\[P(Y\\ge5) = 1-P(Y\\le4) = 1-0.9163 = 0.0837\\] These calculations can be done using the distribution function (d-function) for the Poisson and the cumulative distribution function (p-function). e.g. \\(P(Y=4\\mid \\lambda = 2.3\\) can be obtained using dpois(4, lambda=2.3) and \\(P(Y \\leq 4\\mid \\lambda = 2.3\\) can be obtained using ppois(4, lambda=2.3). 2.2.3 Continous Distributions 2.2.3.1 Uniform Distributions Suppose you wish to draw a random number number between 0 and 1 and any two intervals of equal size should have the same probability of the value being in them. This random variable is said to have a Uniform(0,1) distribution. The uniform distribution has two parameter, a lower bound (a) and an upper bound (b) The pdf is a constant - no value is any more likely than any other \\[p(x \\mid a, b) = \\frac{1}{b-a}, b&gt;a\\] Mean = \\(\\frac{a+b}{2}\\) and variance = \\(\\frac{(b-a)^2}{12}\\) 2.2.3.2 Normal Distributions Undoubtedly the most important distribution in statistics is the normal distribution. Normal Distributions has two parameters: mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Its probability density function is given by \\[f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right]\\] where \\(\\exp[y]\\) is the exponential function \\(e^{y}\\). We could slightly rearrange the function to \\[f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right]\\] and see this distribution is defined by its expectation \\(E[X]=\\mu\\) and its variance \\(Var[X]=\\sigma^{2}\\). Example: It is known that the heights of adult males in the US is approximately normal with a mean of 5 feet 10 inches (\\(\\mu=70\\) inches) and a standard deviation of \\(\\sigma=3\\) inches. I am 5 feet 4 inches (64 inches). What proportion of the adult male population is shorter than me? Using R you can easily find this probability is 0.02275013 using pnorm(64, mean=70, sd=3). 2.2.3.3 Beta Distributions Beta distribution is denoted as \\(Beta(\\alpha, \\beta)\\), with \\(\\alpha&gt;0\\) (shape parameter 1) and \\(\\beta&gt;0\\) (shape parameter 2) We can interpret \\(\\alpha\\) as the number of pseudo events and \\(\\beta\\) as the number of pseudo non-events, so that \\(\\alpha + \\beta\\) is the pseudo sample size If the random variable follows a beta distribution, its value is bounded between 0 and 1, making this distribution a candidate distribution to model proportion. e.g. proportion of patients received surgery. mean = \\(\\frac{\\alpha}{\\alpha+\\beta} = \\frac{\\text(events)}{\\text(sample size)} = p\\) variance = \\[ \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} = \\frac{\\alpha}{\\alpha+\\beta} \\times \\frac{\\beta}{\\alpha+\\beta} \\times \\frac{1}{\\alpha+\\beta+1} = p(1-p)\\frac{1}{\\text(sample size)+1} \\] 2.2.3.4 Gamma Distributions Gamma distribution is denoted as \\(Gamma(\\alpha, \\beta)\\), with \\(\\alpha&gt;0\\) (shape parameter) and \\(\\beta&gt;0\\) (rate parameter) If the random variable follows a Gamma distribution, it can have values larger than 0 mean = \\(\\frac{\\alpha}{\\beta}\\) and variance = \\(\\frac{\\alpha}{\\beta^2}\\) Role of these distributions All of them are potentially distributions for observed outcomes. Similar choices as in non-Bayesian modelling A select group of continuous distributions are frequently used to represent prior distributions for parameters Normal - for regression parameters Beta - for proportions, other bounded parameters Uniform - for proportions, standard deviations Gamma - for rates, variances R Session information ## R version 4.0.5 (2021-03-31) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Matrix products: ## ## locale: ## [1] LC_COLLATE=English_Canada.1252 LC_CTYPE=English_Canada.1252 ## [3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Canada.1252 ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] truncnorm_1.0-8 ggpubr_0.4.0 tweenr_1.0.2 ## [4] gganimate_1.0.7 VennDiagram_1.7.1 futile.logger_1.4.3 ## [7] brms_2.16.3 Rcpp_1.0.7 forcats_0.5.1 ## [10] stringr_1.4.0 dplyr_1.0.7 purrr_0.3.4 ## [13] readr_2.1.1 tidyr_1.1.4 tibble_3.1.6 ## [16] ggplot2_3.3.5 tidyverse_1.3.1 References "],["bayes.html", "Session 3 Bayesian inference 3.1 Priors 3.2 Posterior 3.3 Prediction 3.4 Decision theory", " Session 3 Bayesian inference 3.1 Priors 3.2 Posterior 3.3 Prediction 3.4 Decision theory "],["Prior.html", "Session 4 Considering Prior Distributions", " Session 4 Considering Prior Distributions "],["BayesReg.html", "Session 5 Bayesian Regression 5.1 Normal Models and Linear Regression 5.2 Hierarchical models and convergence 5.3 Models for Binary Data 5.4 Models for Count Data", " Session 5 Bayesian Regression 5.1 Normal Models and Linear Regression 5.2 Hierarchical models and convergence 5.3 Models for Binary Data 5.4 Models for Count Data "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
