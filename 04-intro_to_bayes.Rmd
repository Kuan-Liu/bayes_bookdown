# Introduction to Bayesian inference {#bayes}

::: {.chapterintro data-latex=""}

- Review of frequentist inferential approaches
- Introduce Bayesian inference
- Learn two simple Bayesian models (Beta-binomial & normal-normal)
- Discuss practical advantages and disadvantages of Bayesian approach

:::

<script src="hideOutput.js"></script>

```{r, echo = FALSE, message=FALSE, warning=FALSE}
# Every chapter, we will load all the library we will use at the beginning
# of the chapter. 
library(tidyverse)
library(ggpubr)
library(brms)
library(ggmcmc)
library(bayesplot)
options(scipen = 999)
```

</br>

## Classical frequentist approach

- The classical (frequentist) statistical approach takes many forms, but the most wide-ranging is the likelihood-based approach

- This approach specifies a distributional form for data and considers the
parameters of the distributions to be fixed constants to be estimated.

- The parameters are estimated by finding the values that maximize the
likelihood (hence the name)

> i.e. given the observed data, and assuming they come from specific distributions, what are the parameter values for these distributions that maximize the likelihood of these data?


::: {.important data-latex=""}
**Review of likelihood function**

- Given a statistical model with some parameters (let's call them $\theta$), and given a set of observed data of size $n$, $D = \{x_1, x_2, \ldots, x_n \}$, the likelihood function, $L(\theta, D)$ is a \textbf{mathematical} function that for every value of $\theta$ is equal to the probability (mass or density) of observing $D$ given $\theta$
- i.e. $L(\theta, D) = L_D(\theta) = P(Data | \theta)$
- if we assume $x_1, x_2, \ldots, x_n$ are independent and identically distributed, we can express the likelihood function as

\[ L(\theta, D) = P(x_1 \mid \theta)\times P(x_2 \mid \theta) \ldots \times P(x_n \mid \theta) = \prod_{i=1}^n P(x_i\mid \theta).\]


**Example - Bernoulli trials** Suppose we want to estimate the risk of death $\theta$ after a surgery

- We assume that every patient has the same risk $\theta$
- We collect data from 10 surgeries and we find that 3 patients died and 7 survived,
- What is the likelihood function for $\theta$ in this example?


The distribution for each patient is $Bernoulli(\theta)$ \textbf{Since they are independent} the probability of the number of those who died out of $n$ (here $n$=10) is $Binomial(\theta, 10)$

The probability mass function of the binomial is \[p(x|\theta, n) = {n \choose x} \theta^x (1-\theta)^{n-x}\]

The likelihood function of the observed data (3 deaths out of 10) given $\theta$ is \[ L_D(\theta) = p(x=3| \theta) = {10 \choose 3} \theta^3 (1-\theta)^{10-3} \propto \theta^3 (1-\theta)^{10-3}\]

**Maximum Likelihood Estimator**

- The value that maximizes the likelihood function is called the maximum likelihood estimator or MLE
- It is the "most likely" value for $\theta$ given the observed data
- In this example it is equal to $\hat{\theta}_{mle} = \frac{x}{n} = \frac{3}{10}=0.3$ (the observed proportion of event), which can be obtained by taking the first derivative of the loglikelihood and calculate the value of $\theta$ that yields

$$\begin{aligned}
LogL(\theta, D) &= log({10 \choose 3}) + 3\ log(\theta) + (10-3)\ log(1- \theta) \\
\frac{\partial}{\partial \theta}LogL(\theta, D) & = \frac{3}{\theta} - \frac{10-3}{1-p} = 0 \\
\hat{\theta}_{mle} & = \frac{3}{10}=0.3
\end{aligned}$$

- It is the most commonly method to estimate a parameter in frequentist statistics

:::


```{r echo=T, fig.width=8, fig.align = 'center', eval=TRUE}
#simulating a sequence of probability representing parameter \theta;
#\theta, probability of success, value between 0 and 1;
theta <- seq(0, 1, length=1000) 
#coding Binomial likelihood given x = 3 and n = 10;
L <- choose(10,3)*theta^3*(1-theta)^(10-3)
#coding log Binomial likelihood given x = 3 and n = 10;
logL <- log(choose(10,3)) + 3*log(theta)+ (10-3)*log((1-theta))
# Ploting likelihood function
d <- tibble(theta=theta, L=L)
p1<-ggplot(data=d, aes(theta,L)) +
    geom_line()+
    ggtitle("Binomial likelihood x = 3 and n=10") + 
    theme_bw()
# Ploting likelihood function
d2 <- tibble(theta=theta, logL=logL)
p2<-ggplot(data=d, aes(theta,logL)) +
    geom_line()+
    ggtitle("Log Binomial likelihood x = 3 and n=10") + 
    theme_bw()

ggarrange(p1, p2,  ncol = 2, nrow = 1)

```



```{r echo=T, eval=T}
#negative loglikelihood function of binomial;
neglogL <- function(theta){-sum(dbinom(x=3, size = 10, theta, log = TRUE))}
#optimize:
optim(par = 0.5, fn=neglogL, method = "Brent", lower = 0, upper = 1, hessian = TRUE)
```


::: {.important data-latex=""}


**Maximum Likelihood confidence interval**

MLE satisfies the following two properties called **consistency** and **asymptotic normality**.

1. **Consistency.** We say that an estimate $\hat{\theta}$ is consistent if $\hat{\theta} \rightarrow \theta_0$ as $n \rightarrow \infty$, where $\theta_0$ is the true unknown parameter and $n$ is sample size.

2. **Asymptotic normality** $\hat{\theta}$ is asymptotic normality if

\[ \sqrt{n} (\hat{\theta} - \theta_0) \rightarrow^d N(0, \sigma_{\theta_0}^2) \]
where $\sigma_{\theta_0}^2)$ is the asymptotic variance of the estimate $\hat{\theta}$. Asymptotic normality says that the estimator not only converges to the unknown parameter, but it converges fast enough, at a rate $1/\sqrt{n}$.

Given this properties, we can use **Fisher information** to estimate the variance of MLE and subsequently obtaining confidence intervals.
- MLE Asymptotic normality with Fisher information, $I(\theta_0)$

\[ \sqrt{n} (\hat{\theta}_{mle} - \theta_0) \rightarrow^d N(0, \frac{1}{I(\theta_0)}) \]
 
- Fisher information is defined using the second derivative of the loglikelihood. 
\[ I(\theta) = - E[\frac{\partial^2}{\partial \theta^2} logL(x_1,\ldots, x_n \mid \theta)]\]

    - e.g., for binomail distribution, $I(\theta)=\frac{n}{\theta(1-\theta)}$, thus the 95% CI for $\hat{\theta}_{mle}$ is 
    \[ \hat{\theta}_{mle} \pm 1.96 \sqrt{\frac{\hat{\theta}_{mle}(1-\hat{\theta}_{mle})}{n}} \]
    
    which gives us $0.3 \pm 1.96 \times \sqrt{\frac{0.3 \times 0.7}{10}}$, [0.016, 0.584].

:::

How to calculate variance of MLE in R?

```{r echo=T, eval=T}
mle<-optim(par = 0.5, fn=neglogL, method = "Brent", lower = 0, upper = 1, hessian = TRUE)

# solve(mle$hessian) # to compute the inverse of hessian which is the approximate the variance of theta;
upperbound<-0.3 + 1.96*sqrt(solve(mle$hessian))
lowerbound<-0.3 - 1.96*sqrt(solve(mle$hessian))

print(paste("95% CI for theta is:",round(lowerbound,3),"-", round(upperbound,3)))

```


::: {.guidedexercise data-latex=""}

**Practice MLE estimation in R (Tutorial Practice)** 

Suppose we want to estimate the risk of death \theta after a surgery and We assume that every patient has the same risk \theta. We collect data from 100 surgeries and we find that 30 patients died and 70 survived, 

- What is the likelihood function for \theta in this example?
- What is the MLE estimator given the observed data?
- Can you construct the 95% CI confidence interval of the MLE estimator?
- What is you conclusion comparing this estimator to the MLE obtain from the smaller dataset (10 surgeries, 3 patients died and 7 survived)?
  

:::


## Introduction to Bayesian approach

### Review from session 1

- In the Bayesian approach, everything that is not data is considered as a parameter
- Uncertainty about these parameters is expressed using probability distributions
and probabilistic statements
- A prior distribution expresses what is known or believed independently of the data
- This prior is updated as data or new evidence is presented
- The posterior distribution expresses the updated belief


::: {.important data-latex=""}
**Recall Bayes theorem**

Let $D = \text{patient has disease}$ and $Y = \text{patient has a positive diagnostic test}$,

$$\begin{aligned}
P(D \mid T) & = \frac{P(T \mid D)P(D)}{P(T)} \\
& = \frac{P(T \mid D)P(D)}{P(T \mid D)P(D) + P(T \mid D^c)P(D^c)}
\end{aligned}$$

 - $P(T\mid D)$ is the **likelihood** of the outcome (positive test) given the unknown parameter (disease state)
 - $P(D)$ is **pre-test probability** (prior probability) of disease
 - $P(D\mid T)$ is the post-test probability of disease which can be obtained by multiplying the **likelihood** and the **pre-test probabiltiy**.
 - Here, to calculate $P(D\mid T)$ we need $P(D)$!
 - A very sensitive test (e.g., P(T\mid D) = 0.99) can still result in a small post-test probability if the prior probability of disease, $P(D)$, is low!

:::

**The Bayesian approach to estimating parameters stems from Bayes' theorem for continuous variables:**

Let $\theta$ be the parameter of interest and $y$ be the observed data,

$$\begin{aligned}
P(\theta \mid y) & = \frac{P(y \mid \theta)P(\theta)}{P(y)} \\
& = \frac{\text{likelihood of data given parameter} \times \text{prior}}{\text{marginal distribution of data free of the parameter}} \\
& \propto \text{likelihood}(y \mid \theta ) \times \text{prior}(\theta)
\end{aligned}$$

- $P(y)$ is called a normalizing factor, it's in place to ensure that $\int P(\theta \mid y) d\theta = 1$, that is the posterior distribution of $\theta$ is a proper probability distribution with area under the density curve equals to 1.

- Its value is not of interest, unless we are comparing between data models.

- The essence of Bayes theorem only concerns the terms involving the parameter, $\theta$, hence $P(\theta \mid y) \propto P(y\mid \theta)P(\theta)$.


::: {.workedexample data-latex=""}
**Estimating a Proportion**

Suppose you have observed 6 patients in a Phase I RCT on a given dose of drug,
- 0 out of 6 patients have had an adverse event
- decision to escalate to a higher dose if it's unlikely that the current dosing results in a true proportion of adverse events above 20% (i.e., given the current data, is there sufficient evidence to infer the true proportion of adverse event is less than 20%, if so we can increase the dose level)

- This is a classic phase I estimate, under frequentist test (Exact Binomial Test) we have

```{r }
binom.test(x=0, n=6, p = 0.2,
           alternative = c("less"),
           conf.level = 0.95)
```

 - The observed proportion $\hat{\theta}=0$ with 95% CI: 0 - 0.39.
 - How much evidence we have that AE rate is < 20%?
 
```{r }
#suppose we observe 0 adverse event out of 14 patients; 
#the test results below suggest we would reject the null hypothesis;
#at 0.05 alpha level and conclude the true AE rate is < 20%;

binom.test(x=0, n=14, p = 0.2,
           alternative = c("less"),
           conf.level = 0.95)
```


:::


**What would a Bayesian do?**

To make probability statements about $\theta$ after observing data $y$, we need a probability distribution for $\theta$ given $y$ (the posterior distribution).

1.First, we need to specify a prior distribution for $\theta$, $P(\theta)$.

- Example 1: We might have no idea about $\theta$ other than that it lies in the interval [0,1] and thus specify a unif(0,1). Let $\theta \sim U(0,1)$, the prior probability distribution (p.d.f) is
\[ P(\theta) = \frac{1}{1-0} = 1.\]

- Example 2: We might have some knowledge about the range of $\theta$, say, we are believe $0.05<\theta<0.5$. We can have
\[ \theta \sim U(0.05, 0.5)\]
\[P(\theta) = \frac{1}{0.5-0.05} = 2.22.\]


2. We assume the $P(y \mid \theta)$ follows a binomial distribution, thus the likelihood of the observed data given $\theta$ is 
\[ P(y = 0 \mid \theta) = {6 \choose 0} \theta^0 (1-\theta)^6 = (1-\theta)^6\]


3. The posterior then becomes (given example prior 1)

$$\begin{align}
P(\theta \mid y = 0) &= \frac{P(y = 0 \mid \theta) \times P(\theta)}{P(y=0)} \\
& = \frac{(1-\theta)^6 \times 1}{P(y=0)} \\
& = \text{Constant} \times (1-\theta)^6 \\
 & \propto (1-\theta)^6 
\end{align}$$

```{r fig.height=3, fig.cap="Approximate posterior distribution obtained using Bayes' rule with UNIF(0,1) prior. In this example, the normalizaing term P(y=0) is not considered."}
d <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000),
         y      = 0, 
         n      = 6) %>% 
    mutate(prior      = dunif(p_grid, 0, 1),
         likelihood = dbinom(y, n, p_grid)) %>% 
    mutate(posterior = likelihood * prior )

d %>% pivot_longer(prior:posterior) %>% 
  # this line allows us to dictate the order in which the panels will appear
    mutate(name = factor(name, levels = c("prior", "likelihood", "posterior"))) %>% 
    ggplot(aes(x = p_grid, y = value, fill = name)) +
    geom_area(show.legend = FALSE) +
    scale_fill_manual(values = c("grey", "red", "blue")) +
    facet_wrap(~ name, scales = "free")+
    theme_bw()

```

```{r echo=F, fig.height=3, fig.cap="Approximate posterior distribution obtained using Bayes' rule with UNIF(0.05,0.5) prior. In this example, the normalizaing term P(y=0) is not considered."}
d2 <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000),
         y      = 0, 
         n      = 6) %>% 
    mutate(prior      = dunif(p_grid, 0.05, 0.5),
         likelihood = dbinom(y, n, p_grid)) %>% 
    mutate(posterior = likelihood * prior )

d2 %>% pivot_longer(prior:posterior) %>% 
  # this line allows us to dictate the order in which the panels will appear
    mutate(name = factor(name, levels = c("prior", "likelihood", "posterior"))) %>% 
    ggplot(aes(x = p_grid, y = value, fill = name)) +
    geom_area(show.legend = FALSE) +
    scale_fill_manual(values = c("grey", "red", "blue")) +
    facet_wrap(~ name, scales = "free")+
    theme_bw()

```

**Why is P(y=0) free of $\theta$?**

- The **law of total probability** for discrete parameter values can be used
- Suppose there are two possible values of parameter $\theta$, 0.5 and 0.1. 
- Suppose we know the prior distribution of $\theta$: $P(\theta = 0.5) = 0.8$ and
$P(\theta = 0.1) = 0.2$
- Likelihood values are calculated given a known $\theta$, so then don't include the parameter $\theta$.
- Call these $P_{0.5} = P(Y = 0 \mid \theta = 0.5)$ and $P_{0.1} = P(Y = 0 \mid \theta = 0.1)$, 
- Putting all components together using law of total probability, $P(Y = 0)$ does not involved the unknown $\theta$
\[P(Y = 0) = P(Y = 0 \mid \theta = 0.5)P(\theta = 0.5) + P(Y = 0 \mid \theta = 0.1)P(\theta = 0.1)\]

\[ P(Y = 0) = P_{0.5} \times 0.8 + P_{0.1} \times 0.2 \]


- In case of a continuous parameter value, we can obtain $P(y=0)$ by integrating over the space of $\theta$ as following
\[P(y=0) = \int_{\theta=0}^{\theta=1} P(y=0 \mid \theta) P(\theta) \ d \theta\]
- Integrating over $\theta$ is analogous to summing over a set of discrete values of $\theta$.
- After integration over $\theta$, $\theta$ is no longer featured in $P(y=0)$.

- In this example, we have

\[P(y=0) = \int_{\theta=0}^{\theta=1} P(y=0 \mid \theta) \times \frac{1}{1-0} \ d \theta \]
\[P(y=0) = \int_{\theta=0}^{\theta=1} P(y=0 \mid \theta) \times \frac{1}{0.5-0.05} \ d \theta \]


```{r echo=T}
#Integration in R with one variable;
#Unif(0,1);
integrand <- function(theta) {(1-theta)^6}
normalizing_constant<-integrate(integrand, lower = 0, upper = 1)
normalizing_constant

#Unif(0.05,0.5);
integrand2 <- function(theta) {((1-theta)^6)/(0.5-0.05)}
normalizing_constant2<-integrate(integrand2, lower = 0, upper = 1)
normalizing_constant2
```

<div class="fold s">
```{r echo=T, warning=F, fig.height=3, fig.cap="Posterior distribution obtained using Bayes' rule with UNIF(0,1) and UNIF(0.05,0.5) priors"}

d <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000),
         y      = 0, 
         n      = 6) %>% 
    mutate(prior1 = dunif(p_grid, 0, 1),
           prior2 = dunif(p_grid, 0.05, 0.5),
           likelihood = dbinom(y, n, p_grid)) %>% 
    mutate(posterior1 = likelihood * prior1 / normalizing_constant$value,
           posterior2 = likelihood * prior2 / normalizing_constant2$value)

d %>% pivot_longer(posterior1:posterior2) %>% 
    ggplot(aes(x = p_grid, y = value, group = as.factor(name))) +
    geom_area(aes(fill = name),position="identity") +
    scale_fill_manual(name = "Prior", labels = c("UNIF(0,1)","UNIF(0.05,0.5)"),values = c("#d8b365",  "#5ab4ac")) +
    geom_vline(xintercept = 0.2, size=1) +
    annotate("text", x=0.3, y=6, label= expression(paste(theta, "=0.2")), size = 5) + 
    labs(x = expression(theta), y = "posterior probability density", title="Posterior distribution given uniform priors and binary data")+
    theme_bw()

```
<div>

::: {.guidedexercise data-latex=""}

**Practice posterior estimation with brms package in R (Tutorial Practice)**  

> we will introduce MCMC in later sessions.

```{r echo=T, eval=F}
dat1<-data.frame(y=rep(0,6))
fit1 <- brm(data = dat1, 
      family = bernoulli(link = "identity"),
      y ~ 1,
      prior = c(prior(uniform(0, 1), class = Intercept)),
      iter = 1000 + 2500, warmup = 1000, chains = 2, cores = 2,
      seed = 123)

fit2 <- brm(data = dat1, 
      family = bernoulli(link = "identity"),
      y ~ 1,
      prior = c(prior(uniform(0.05, 0.5), class = Intercept)),
      iter = 1000 + 2500, warmup = 1000, chains = 2, cores = 2,
      seed = 123)

summary(fit1)
summary(fit2)
  
plot(fit1)
plot(fit2)

mcmc_areas(
  fit1, 
  pars = c("b_Intercept"),
  prob = 0.80, # 80% inner intervals;
  prob_outer = 0.95, # 99% outter intervals;
  point_est = "mean"
)

mcmc_areas(
  fit2, 
  pars = c("b_Intercept"),
  prob = 0.80, # 80% inner intervals;
  prob_outer = 0.95, # 99% outter intervals;
  point_est = "mean"
)

hypothesis(fit1, 'Intercept < 0.2')
hypothesis(fit2, 'Intercept < 0.2')

```
:::


### The Beta-binomial model


- With a single sample of $n$ binary outcomes, we have one unknown parameter: $\theta$ and $0 \leq \theta \leq 1$.
- We need a prior distribution for $\theta$ (e.g., proportion, risk, probability of event etc): $P(\theta)$
    - To express indifference between all values of $\theta$, we can use a uniform distribution on $\theta$, as we did in the previous example
    - To express belief (e.g. based on external evidence) that some values of $\theta$ are more likely that others, it is convenient to use a **beta distribution**
    - This has two parameters, often labelled as $\alpha$ (also written as a) and $\beta$ (also written as b), which we can choose to represent the strength of the external evidence
    - If a parameter has a Beta(a,b) distribution, then the prior mean is
    \[\frac{a}{a+b}\]
    - The beta distribution prior for the binomial is useful for illustrating how the Bayesian approach combines prior information and new data


::: {.important data-latex=""}
**Beta-binomial model**

- Recall the likelihood for a binomial outcome of x successes in n trials,

\[ P(x \mid \theta) \propto \theta^x (1-\theta)^{n-x}\]

- The $Beta(\alpha,\beta)$ prior has the same functional form for $\theta$,

\[ P( \theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}\]

- We find the posterior as

$$\begin{align}
P(\theta \mid x) & \propto P(x \mid \theta) \times P( \theta) \\
& \propto \theta^x (1-\theta)^{n-x} \times \theta^{\alpha-1} (1-\theta)^{\beta-1} \\
&  \propto \theta^{x+\alpha-1}  (1-\theta)^{n-x+\beta-1}
\end{align}$$

- Thus, comparing to the $Beta(\alpha,\beta)$ prior, the posterior just changes the exponents by adding **x and n-x**, respectively.

    - Comparing to the prior, make two changes to get the posterior:
        1. $a \rightarrow a+x$, [a + number of events]
        2. $b \rightarrow b+(n-x)$, [b + number of non-events]
    - Quite simply, when $x$ events have been observed in n subjects, the prior 
    
    \[ \theta \sim Beta(\alpha, \beta) \]
    
    - gives the posterior
    
    \[ \theta \mid x \sim Beta(\alpha+x, \beta+n-x) \]

- The prior and posterior are both **beta distributions!**

:::


**Interpretation of Beta Prior**

- Suppose we start with a beta prior with small parameters

\[ \theta \sim Beta(0.001, 0.001) \]

- Observe x events in n trials, the posterior

 \[ \theta \mid x \sim Beta(0.001+x, 0.001+n-x) \approx Beta(x,n-x)\]
 
- Posterior mean of $\theta \approx \frac{x}{n}$, the equivalent to the MLE based only on the data

- Interpretation of the $Beta(\alpha,\beta)$ prior.
    - Like having seen $\alpha$ events and $\beta$ non events in a sample size of $\alpha + \beta$
    - Strength of prior information equivalent to prior "sample size" $\alpha + \beta$
    - Prior mean = $\frac{\alpha}{\alpha + \beta}$

::: {.workedexample data-latex=""}

Consider Beta(3,7)and Beta(12,28) priors

- Gold line: prior belief that assumes approximately 3 events in 10 subjects
- Blue line: prior belief that assumes approximately 12 events in 40(=12+28) subjects

```{r echo=F}
d <- tibble(theta = seq(from = 0, to = 1, length.out = 101)) %>% 
  expand(theta, a = c(3, 12), b = c(7,28)) %>% 
  mutate(PriorSet = paste0("Beta(",format(a), "," , format(b),")")) %>%
  mutate(priorDensity = dbeta(theta, shape1 = a,shape2 = b))%>%
  filter(a/(a+b)==0.3)
  

ggplot(d, aes(theta, priorDensity,colour=PriorSet))+
  geom_line(size=1)+
  xlab(expression(theta))+
  ylab(expression(p(theta)))+
  theme(legend.position = "bottom")+
  scale_colour_manual(values=c("goldenrod","steelblue"))+theme_bw()

```
:::

::: {.guidedexercise data-latex=""}

**Plot Beta densities in R (Tutorial Practice)** Try plotting Beta(2,8), Beta(8,2), and Beta(8,8). Example R code provided below.

```{r echo=T, eval=F}
a<-2
b<-2
d <- tibble(theta = seq(from = 0, to = 1, length.out = 101)) %>% 
  mutate(priorDensity = dbeta(theta, shape1 = a,shape2 = b))

ggplot(d, aes(theta, priorDensity))+
  geom_line()+
  xlab(expression(theta))+
  ylab(expression(p(theta)))+
  ggtitle(paste0("Beta distribution with ", "a = ",a,"; b = ",b))+
  theme_bw()
```

:::

**Summarizing Posterior Distribution**

- Since we know the form of the posterior distribution, we can easily calculate functions such as:
    1. Posterior mean $E(\theta) = \frac{\alpha+x}{\alpha + \beta+n}$
    2. 95% Credible intervals (we will talk more about them next week)
    3. $P(\theta < 0.2)$, $P(\theta < 0.5)$, $P(0.4< \theta < 0.6)$, etc, which can be directly used to make probabilistic statement about the $\theta$. E.g., the probability of the posterior adverse event rate < 0.2 is about 0.95.  
    
- Generate informative plots for assessing priors and posteriors. All this can easily be done using R.
    
::: {.guidedexercise data-latex=""}

**Beta densities posterior summary in R (Tutorial Practice)** Suppose we observe 1 adverse events among 10 patients and we assume the prior distribution of adverse event proportion $\theta \sim Beta(1,1)$.

- What is the posterior Beta distribution of $\theta$?
- What is the posterior mean Beta distribution of $\theta$?
- What is the posterior probability $P(\theta<0.2)$?

:::

**Data overwhelming the prior**

- The posterior for the beta-binomial model after seeing x events in n trials is $\theta \mid x ~ Beta(\alpha + x, \beta + n - x)$ with posterior mean as

\[E(\theta \mid x) = \frac{\alpha + x}{\alpha + \beta + n}\]

- In $n \gg \alpha + \beta$ and $x \gg \alpha$ (sample size and number of events is large), recall when using prior $Beta(0.001, 0.001)$,
\[ E(\theta \mid x) = \frac{\alpha + x}{\alpha + \beta + n} \approx \frac{x}{n}\]

- Here, prior is of little importance!

```{r echo=F, fig.height=3, fig.width=8}

L <- 200

data <- tibble(Observed=c(" 3/10","11/20","21/40","40/80"),
               x=c(3, 11, 21, 40),
               n=c(10, 20, 40, 80),
               row=1)
d <-tibble(theta = seq(from = 0, to = 1, length.out = L),
         a=rep(2, L),
         b=rep(6, L),
         row=1) %>%
  inner_join(data, by="row") %>%
  mutate(a2 = a+x, b2=b+n-x) %>%
  mutate(priorDensity = dbeta(theta, shape1 = a,shape2 = b))%>%
  mutate(likelihood.0 = dbinom(x, prob=theta,size=n))%>%
  mutate(posteriorDensity = dbeta(theta, shape1 = a2,shape2 = b2)) %>%
  group_by(Observed) %>%
  mutate(likelihood = likelihood.0/sum(diff(theta)*likelihood.0[-1])) %>%
  pivot_longer(cols=c(priorDensity,likelihood,posteriorDensity),names_to="Function",values_to="p")
  
d$Function <- factor(d$Function, 
                     levels=c("priorDensity","likelihood","posteriorDensity"),
                     labels=c("Prior","Likelihood","Posterior"))

ggplot(d, aes(theta, p,colour=Function))+
  geom_line(size=1)+
  xlab(expression(theta))+
  ylab(expression(p(theta)))+
  theme(legend.position = "bottom")+
  facet_wrap(~Observed, labeller=label_both,nrow=1)+
  scale_colour_manual(values=c("goldenrod","steelblue","black"))+
  theme_bw()

```


**A Beta(1,1) is Unif(0,1)**

- When $\alpha = \beta = 1$, $P(\theta) \propto \theta^{1-1}(1-\theta)^{1-1} = 1$, which is the probability density of a unif(0,1).

**Posterior mean as a weighted average**

- The posterior mean is

$$\begin{align}
E(\theta \mid x) & = \frac{\alpha + x}{\alpha + \beta + n} \\
& = \frac{\alpha }{\alpha + \beta + n} + \frac{x }{\alpha + \beta + n}\\
& = \Big( \frac{\alpha+ \beta }{\alpha + \beta + n} \Big) \times \Big( \frac{\alpha }{\alpha + \beta} \Big) + \Big( \frac{n }{\alpha + \beta + n} \Big) \times \Big( \frac{x }{n} \Big) \\
& = w \times \Big( \frac{\alpha }{\alpha + \beta} \Big) + (1-w) \times \Big( \frac{x }{n} \Big) \\
& = w \times (\text{prior mean}) + (1-w) \times (\text{sample estimate})
\end{align}$$

- Where $w$ is the ratio of the "prior sample size" to the "total sample size" \[ w = \frac{\alpha+ \beta }{\alpha + \beta + n}.\]

- This is a common theme in Bayesian models with actual prior information
- The posterior distribution will lie between the prior and likelihood
- The posterior mean is a weighted average of the prior mean and data-based estimate
- **As the sample size increases, the contribution of the prior diminishes**


::: {.guidedexercise data-latex=""}

**Revisit the adverse event example, now using the beta-binomial model**

- Suppose you have observed 0 adverse events in 6 patients in a phase I clinical trial
on a given dose of a drug
- Assume a prior: $\theta \sim Beta(1,1)$ ; What is the posterior distribution? 
- How much evidence that the AE rate is < 20%? Find $P(\theta < 0.2)$ using the posterior distribution?
- What is the AE rate at a lower dose was 1/6? Can we use this external evidence?
    - New prior for the lower dose, $\theta \sim Beta(1,5)$
    - New Posterior is given 0 adverse events in 6 patients: $\theta \sim Beta(1,11)$
    - Find $P(\theta < 0.2)$.
:::


### The Normal-normal model


**Parameters with normal priors and likelihoods**

- Continuing with a single parameter model

- The likelihood of the parameter given some data is that of a normal distribution with known variance $\sigma^2$

\[ y \mid \theta \sim N(\theta, \sigma^2) \]

- suppose the prior for $\theta$, the mean, follows a normal distribution

\[ \theta \sim N(\mu_0, V_0)\]

- We want to make inference about $\theta$ given the data y, what is the posterior distribution $P(\theta \mid y)$?


::: {.important data-latex=""}
**Normal-normal model**

**1. The normal model (likelihood) of one data point**

- Consider a single observation $y$ from a normal distribution parameterized by a mean $\theta$ and variance $\sigma^2$, we assume that $\sigma^2$ is known.

- The normal likelihood for $y$ given $\theta$ is
\[ P(y\mid \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y-\theta)^2}{2\sigma^2}}, y \in (-\infty, \infty)\]

- The $E(y) = \theta$ and $V(y) = \sigma^2$ and $SD(y) = \sigma$.

- Given $y \mid \theta \sim N(\theta,\sigma^2 )$, $z = \frac{x-\theta}{\sigma} \sim N(0,1)$. Thus, if we rescale $y$ by $\sigma$, we have

    - 95%CI of y: $\theta \pm 1.96 \times \sigma$


**2. Prior**

We can often use a normal prior to represent $\theta \sim N(\mu_0, V_0)$

- $\mu_0$ is the prior mean
    
- $V_0$ is the prior variance, which expresses the uncertainty around the mean $\mu_0$
    
- $\tau_0 = \frac{1}{V_0}$ is called precision, it can also be used to express the uncertainty around the mean, e.g. $\theta \sim N(\mu_0, \tau_0)$
    
- High precision = low variance, in other words, the distribution of the normal random variable $\theta$ is closely centered around its mean.


**3. Posterior**

 - With a normal prior and a normal likelihood of the data given parameter $\theta$, (here we use precision to express the normal distributions)
 
\[ \theta \sim N(\mu_0,\tau_0 ) \text{ and } y \mid \theta \sim N(\theta,\tau_y )\]

- where $\tau_y = \frac{1}{\sigma^2}$ is the precision of the observed data

- It can be proven mathematically using the Bayes' rule that the posterior distribution given a normal prior and normal likelihood is also normal,

\[ \theta \mid y \sim  N(\mu_1, \tau_1)\]

- The posterior mean $\mu_1$ is

\[ \frac{\tau_0 \times \mu_0 + \tau_y \times y}{\tau_0+ \tau_y} \]

- The posterior precision is

\[ \tau_1 = \tau_0 + \tau_y\]

- Again, **the prior and posterior are both normal distributions!**

:::

**Posterior mean as a weighted average of the prior mean and data-based estimate**


- Using the precision parameters

\[ \tau_y = \frac{1}{\sigma^2_y} \text{ and } \tau_0 = \frac{1}{V_0}\]

- The posterior distribution of $\theta$ is then

\[\theta \mid y \sim N(\mu_1, \tau_1)\]

- where the mean and precision are

$$\begin{align}
& \mu_1 = \Big(\frac{\tau_0}{\tau_0 + \tau_y}\Big) \times \mu_0 + \Big(\frac{\tau_y}{\tau_0 + \tau_y}\Big) \times y \\
& \tau_1 = \tau_0 + \tau_y
\end{align}$$

- The posterior mean is a weighted sum of the prior mean and the observed value.
- The weights are the relative precisions of the prior and the likelihood
\[\mu_1 = w_0 \times \mu_0 + w_y \times y\]
\[w_0 = \frac{\tau_0}{\tau_0 + \tau_y}\] 
and

\[w_y = 1-w_0 = \frac{\tau_y}{\tau_0 + \tau_y}\] 



**Data overwhelming the prior and vice versa**

- With relatively low prior precision (imprecise prior and substantial data), i.e., $\tau_0 \ll \tau_y$ and $0 \approx w_0 \ll w_y \approx 1$,

\[ \mu_1 = w_0 \times \mu_0 + w_y \times y \approx y\]

- With relative little data and aprior that is not too diffuse, i.e., $\tau_y \ll \tau_0$ and $0 \approx w_y \ll w_0 \approx 1$,

\[ \mu_1 = w_0 \times \mu_0 + w_y \times y \approx \mu_0\]

- In most cases, the posterior mean is a compromise between what we believed before and what we observed in the data.


**Normal model with multiple observations**

 - Consider a set of $n$ observations $y = (y_1, \ldots, y_n)$ sample from a $N(\theta, \sigma^2)$, where $y_1,\ldots, y_n$ are identically, independently distributed and $y_i \sim N(\theta, \sigma^2)$, $i = 1, \ldots, n$. 
 
 - The normal likelihood of the joint distribution $y = (y_1, \ldots, y_n)$ is
 
$$\begin{align}
P((y_1, \ldots, y_n \mid \theta) &= \prod_{i=1}^n P(y_i \mid \theta) \\
& = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_i-\theta)^2}{2\sigma^2}}, y \in (-\infty, \infty)
\end{align}$$

- Given prior $\theta \sim N(\mu_0, \tau_0)$, the posterior of $\theta \mid y$ is

\[P(\theta \mid y_1, \ldots, y_n) = N(\mu_n, \tau_n)\]

- where the posterior mean $\mu_n$ is

\[\mu_n = \frac{\tau_0 \times \mu_0 + n\tau_y \times \bar{y}}{\tau_0+ n\tau_y} = \Big( \frac{\tau_0}{\tau_0+ n\tau_y} \Big) \times \mu_0 + \Big( \frac{n\tau_y}{\tau_0+ n\tau_y} \Big) \times \bar{y}\]

- and the posterior precision is

\[ \tau_n = \tau_0 + n\tau_y\]

- $\bar{y} = \frac{\sum_{i=1}^2y_i}{n}$ is the sample mean and $\tau_y = \frac{1}{\sigma^2}$

- Again, the posterior mean is a weighted sum between prior and data. As sample size $n \rightarrow \infty$, $\Big( \frac{\tau_0}{\tau_0+ n\tau_y} \Big) \rightarrow 0$ and $\Big( \frac{n\tau_y}{\tau_0+ n\tau_y} \Big) \rightarrow 1$, **data will overwhelm the prior and dominate the posterior distribution**.  



**The use of normal-normal model in clinical applications**

- Consider estimating a parameter $\theta$, e.g. mean, log-hazard ratio, log-odds-ratio, log-relative-risk

- Suppose we have an estimate of $\theta$; We will call it $y$ and let $\hat{\sigma}^2_y$ be the estimated variance

- This estimate $y$ can be viewed as a single data observation of $\theta$! 

- In large samples, it is approximately true that $y \sim N(\theta, \sigma^2_y)$, where $\theta$ is the true value and $\sigma^2_y$ is the true variance (sample size dependent) and the 95%CI of $y$ is $\theta \pm 1.96 \times \sigma$.

-  We can construct a normal likelihood of data for the parameter $\theta$ as 
\[y \mid N(\theta, \hat{\sigma}^2_y)\]


::: {.workedexample data-latex=""}

**Treatment effect estimate on the mean**

- For this example, we will simulate data for a small study of n=100 subjects and randomized 50% to treat=0 and 50% to treat=1 (mimicking an RCT). We will then generate the outcome Y from a normal distribution depending on the treatment indicator.

- Suppose we have the following table of results from a linear regression estimating treatment effect $\theta$ on the outcome

```{r}
n <- 100
set.seed(1234)
treat <- 1*(runif(n, min = 0, max = 1)<0.5)
y <- 5*treat+rnorm(n, mean = 0, sd =10)
fit<-lm(y~treat)
summary(fit)
round(confint.default(fit, 'treat', level=0.95),3) # based on asymptotic normality
```

- We are interested in using this published results to make Bayesian inference on $\theta$

- Our observed data (estimate) of $\theta$, denoted as $y$ is 4.08 with standard error 1.918. Here, $y$ plays the role of data! We call $y$ the observed datum.

- Relying on the large sample approximation, we assume that the estimate y arose from a normal distribution with true mean $\theta$, **We treat the observed standard error $\hat{\sigma}_y = 1.918$ as the true standard deviation of $y$**

    - Recall, the standard deviation of the sampling distribution of a parameter estimate is called its standard error

- This gives us the likelihood 
\[y \mid \theta \sim N(\theta, 1.918^2)\]

- The width of the 95% CI is approximately equal to $2 \times 1.96 \times \sigma_y$

- Suppose we are not provided with $\sigma_y$ from the table of results, but know the 95% CI of y is (1.568,0.064), we can calculate $\sigma_y$ with 

\[\sigma_y = \frac{7.84- 0.321}{2\times 1.96}=1.918\]

- The normal likelihood is this example is $y \sim N(4.08, 1.918^2)$, if we assume a prior $\theta \sim N(0, 2 \times 1.918^2)$, we have a posterior

\begin{align}
& \theta \mid y \sim N(\mu_1, \tau_1) \\
\mu_1  &= \frac{ \frac{1}{prior.sd^2} }{\frac{1}{prior.sd^2} + \frac{1}{se.obs^2} } \times prior.mean + \frac{\frac{1}{se.obs^2}}{\frac{1}{prior.sd^2} + \frac{1}{se.obs^2}} \times y.obs \\
 & = \frac{ \frac{1}{2 \times 1.918^2} }{\frac{1}{2 \times 1.918^2} + \frac{1}{1.918^2} } \times 0 + \frac{\frac{1}{1.918^2}}{\frac{1}{2 \times 1.918^2} + \frac{1}{1.918^2}} \times 4.08 = 2.72 \\
 
 \tau_1 & = \tau_0 + \tau_y \\
        & = \frac{1}{prior.sd^2} + \frac{1}{se.obs^2} \\
        & = \frac{1}{2 \times 1.918^2} + \frac{1}{1.918^2} = 0.408
\end{align}

```{r}
d <- tibble(theta = seq(from = -10, to = 15, length.out = 200),
         y.obs=4.08,
         se.obs=1.918,
         prior.mean=0,
         prior.sd=sqrt(2)*1.918) %>% 
  mutate(priorDensity = dnorm(theta, prior.mean, prior.sd)) %>%
  mutate(likelihood.0 = dnorm(theta, y.obs, se.obs)) %>%
  mutate(w = (1/se.obs^2)/(1/se.obs^2 + 1/prior.sd^2)) %>%
  mutate(post.prec = 1/se.obs^2 + 1/prior.sd^2) %>%
  mutate(posteriorDensity = dnorm(theta, w*y.obs+(1-w)*prior.mean, sqrt(1/post.prec))) %>%
  mutate(likelihood = likelihood.0/sum(diff(theta)*likelihood.0[-1])) %>%
  pivot_longer(cols=c(priorDensity,likelihood,posteriorDensity),names_to="Function",values_to="p")
  
d$Function <- factor(d$Function, 
                     levels=c("priorDensity","likelihood","posteriorDensity"),
                     labels=c("Prior","Likelihood","Posterior"))

p<-ggplot(d, aes(theta, p,colour=Function))+
  geom_line(size=1)+
  xlab(expression(theta))+
  ylab(expression(p(theta)))+
  theme(legend.position = "bottom")+
  scale_colour_manual(values=c("goldenrod","steelblue","black")) +theme_bw()

p
```


:::



::: {.workedexample data-latex=""}

**Hazard Ratio on time-to-event from COX proportional hazard model**

- Effect of a Resuscitation Strategy Targeting Peripheral Perfusion Status vs Serum Lactate Levels on 28-Day Mortality Among Patients With Septic Shock
The ANDROMEDA-SHOCK Randomized Clinical Trial [@hernandez2019effect]

- Open-access link: https://jamanetwork.com/journals/jama/fullarticle/2724361

- We are interested to make inference on the hazard ratio of peripheral perfusion versus Lactate level on 28-Day mortality among patients with septic shock.

- Let $\theta$ be the true value of this HR, we obtain a point estimate of $\theta$ and it's 95%CI from Table 2 of [@hernandez2019effect].

```{r echo=FALSE, fig.cap="HR estimates of the effectiveness of peripheral perfusion on primary outcome - Death within 28 days", fig.align='center'}
knitr::include_graphics("images/ANDROMEDA-SHOCK_table2.png")
```

- The estimated adjusted $\theta$ (HR) on 28-day mortality, 0.75 (0.55 to 1.02), is called the **observed datum** (viewed as a single data observation on $\theta$, denoted as $y$)  

- It's known that **log of the parameter estimate, such as HR, RR and OR, follow a normal distribution around its true log value**. Thus, We need to use the reported output information in order to specify the likelihood
for the log of the parameter, y.

- Let $y$ denote this data point, we have $y \sim N(log(\theta), \sigma_y^2)$, where $\theta$ is log(HR) and $sigma_y$ is the standard error of the log(HR)

- In order to get $\sigma_y^2$ from the result table, we will use the reported 95% CI of the **log of HR**!

- The limits of the CI for the log value are obtained by taking the logs of the reported CI, i.e., 95% CI on the log(HR) in this example is [log(0.55)=-0.598, log(1.02)=0.02].

- Take the width of the 95% CI and divided by $2\times 1.96$, we have

\[\hat{\sigma}_y = \frac{log(1.02) - log(0.55)}{2\times 1.96} = 0.158\]

- We use $\hat{\sigma}_y$ to approximate $\sigma_y$, therefore, the data likelihood $y \mid \theta \sim (\theta,0.158^2)$ where we have observed $y=log(0.75)=-0.287$



:::



### R Session information {-}




```{r echo=F}
getS3method("print","sessionInfo")(sessionInfo()[c(1:7)])
```

