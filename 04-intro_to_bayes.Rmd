# Introduction to Bayesian inference {#bayes}

::: {.chapterintro data-latex=""}

- Review of frequentist inferential approaches
- Introduce Bayesian inference
- Learn two simple Bayesian models (Beta-binomial & normal-normal)
- Discuss practical advantages and disadvantages of Bayesian approach

:::

<script src="hideOutput.js"></script>

```{r, echo = FALSE, message=FALSE, warning=FALSE}
# Every chapter, we will load all the library we will use at the beginning
# of the chapter. 
library(tidyverse)
library(ggpubr)
library(brms)
options(scipen = 999)
```

</br>

## Classical frequentist approach

- The classical (frequentist) statistical approach takes many forms, but the most wide-ranging is the likelihood-based approach

- This approach specifies a distributional form for data and considers the
parameters of the distributions to be fixed constants to be estimated.

- The parameters are estimated by finding the values that maximize the
likelihood (hence the name)

> i.e. given the observed data, and assuming they come from specific distributions, what are the parameter values for these distributions that maximize the likelihood of these data?


::: {.important data-latex=""}
**Review of likelihood function**

- Given a statistical model with some parameters (let's call them $\theta$), and given a set of observed data of size $n$, $D = \{x_1, x_2, \ldots, x_n \}$, the likelihood function, $L(\theta, D)$ is a \textbf{mathematical} function that for every value of $\theta$ is equal to the probability (mass or density) of observing $D$ given $\theta$
- i.e. $L(\theta, D) = L_D(\theta) = P(Data | \theta)$
- if we assume $x_1, x_2, \ldots, x_n$ are independent and identically distributed, we can express the likelihood function as

\[ L(\theta, D) = P(x_1 \mid \theta)\times P(x_2 \mid \theta) \ldots \times P(x_n \mid \theta) = \prod_{i=1}^n P(x_i\mid \theta).\]


**Example - Bernoulli trials** Suppose we want to estimate the risk of death $\theta$ after a surgery

- We assume that every patient has the same risk $\theta$
- We collect data from 10 surgeries and we find that 3 patients died and 7 survived,
- What is the likelihood function for $\theta$ in this example?


The distribution for each patient is $Bernoulli(\theta)$ \textbf{Since they are independent} the probability of the number of those who died out of $n$ (here $n$=10) is $Binomial(\theta, 10)$

The probability mass function of the binomial is \[p(x|\theta, n) = {n \choose x} \theta^x (1-\theta)^{n-x}\]

The likelihood function of the observed data (3 deaths out of 10) given $\theta$ is \[ L_D(\theta) = p(x=3| \theta) = {10 \choose 3} \theta^3 (1-\theta)^{10-3} \propto \theta^3 (1-\theta)^{10-3}\]

**Maximum Likelihood Estimator**

- The value that maximizes the likelihood function is called the maximum likelihood estimator or MLE
- It is the "most likely" value for $\theta$ given the observed data
- In this example it is equal to $\hat{\theta}_{mle} = \frac{x}{n} = \frac{3}{10}=0.3$ (the observed proportion of event), which can be obtained by taking the first derivative of the loglikelihood and calculate the value of $\theta$ that yields

$$\begin{aligned}
LogL(\theta, D) &= log({10 \choose 3}) + 3\ log(\theta) + (10-3)\ log(1- \theta) \\
\frac{\partial}{\partial \theta}LogL(\theta, D) & = \frac{3}{\theta} - \frac{10-3}{1-p} = 0 \\
\hat{\theta}_{mle} & = \frac{3}{10}=0.3
\end{aligned}$$

- It is the most commonly method to estimate a parameter in frequentist statistics

:::


```{r echo=T, fig.width=8, fig.align = 'center', eval=TRUE}
#simulating a sequence of probability representing parameter \theta;
#\theta, probability of success, value between 0 and 1;
theta <- seq(0, 1, length=101) 
#coding Binomial likelihood given x = 3 and n = 10;
L <- choose(10,3)*theta^3*(1-theta)^(10-3)
#coding log Binomial likelihood given x = 3 and n = 10;
logL <- log(choose(10,3)) + 3*log(theta)+ (10-3)*log((1-theta))
# Ploting likelihood function
d <- tibble(theta=theta, L=L)
p1<-ggplot(data=d, aes(theta,L)) +
    geom_line()+
    ggtitle("Binomial likelihood x = 3 and n=10") + 
    theme_bw()
# Ploting likelihood function
d2 <- tibble(theta=theta, logL=logL)
p2<-ggplot(data=d, aes(theta,logL)) +
    geom_line()+
    ggtitle("Log Binomial likelihood x = 3 and n=10") + 
    theme_bw()

ggarrange(p1, p2,  ncol = 2, nrow = 1)

```



```{r echo=T, eval=T}
#negative loglikelihood function of binomial;
neglogL <- function(theta){-sum(dbinom(x=3, size = 10, theta, log = TRUE))}
#optimize:
optim(par = 0.5, fn=neglogL, method = "Brent", lower = 0, upper = 1, hessian = TRUE)
```


::: {.important data-latex=""}


**Maximum Likelihood confidence interval**

MLE satisfies the following two properties called **consistency** and **asymptotic normality**.

1. **Consistency.** We say that an estimate $\hat{\theta}$ is consistent if $\hat{\theta} \rightarrow \theta_0$ as $n \rightarrow \infty$, where $\theta_0$ is the true unknown parameter and $n$ is sample size.

2. **Asymptotic normality** $\hat{\theta}$ is asymptotic normality if

\[ \sqrt{n} (\hat{\theta} - \theta_0) \rightarrow^d N(0, \sigma_{\theta_0}^2) \]
where $\sigma_{\theta_0}^2)$ is the asymptotic variance of the estimate $\hat{\theta}$. Asymptotic normality says that the estimator not only converges to the unknown parameter, but it converges fast enough, at a rate $1/\sqrt{n}$.

Given this properties, we can use **Fisher information** to estimate the variance of MLE and subsequently obtaining confidence intervals.
- MLE Asymptotic normality with Fisher information, $I(\theta_0)$

\[ \sqrt{n} (\hat{\theta}_{mle} - \theta_0) \rightarrow^d N(0, \frac{1}{I(\theta_0)}) \]
 
- Fisher information is defined using the second derivative of the loglikelihood. 
\[ I(\theta) = - E[\frac{\partial^2}{\partial \theta^2} logL(x_1,\ldots, x_n \mid \theta)]\]

    - e.g., for binomail distribution, $I(\theta)=\frac{n}{\theta(1-\theta)}$, thus the 95% CI for $\hat{\theta}_{mle}$ is 
    \[ \hat{\theta}_{mle} \pm 1.96 \sqrt{\frac{\hat{\theta}_{mle}(1-\hat{\theta}_{mle})}{n}} \]
    
    which gives us $0.3 \pm 1.96 \times \sqrt{\frac{0.3 \times 0.7}{10}}$, [0.016, 0.584].

:::

How to calculate variance of MLE in R?

```{r echo=T, eval=T}
mle<-optim(par = 0.5, fn=neglogL, method = "Brent", lower = 0, upper = 1, hessian = TRUE)

# solve(mle$hessian) # to compute the inverse of hessian which is the approximate the variance of theta;
upperbound<-0.3 + 1.96*sqrt(solve(mle$hessian))
lowerbound<-0.3 - 1.96*sqrt(solve(mle$hessian))

print(paste("95% CI for theta is:",round(lowerbound,3),"-", round(upperbound,3)))

```


::: {.guidedexercise data-latex=""}

**Practice MLE estimation in R (Tutorial Practice)** 

Suppose we want to estimate the risk of death \theta after a surgery and We assume that every patient has the same risk \theta. We collect data from 100 surgeries and we find that 30 patients died and 70 survived, 

- What is the likelihood function for \theta in this example?
- What is the MLE estimator given the observed data?
- Can you construct the 95% CI confidence interval of the MLE estimator?
- What is you conclusion comparing this estimator to the MLE obtain from the smaller dataset (10 surgeries, 3 patients died and 7 survived)?
  

:::


## Introduction to Bayesian approach

### Review from session 1

- In the Bayesian approach, everything that is not data is considered as a parameter
- Uncertainty about these parameters is expressed using probability distributions
and probabilistic statements
- A prior distribution expresses what is known or believed independently of the data
- This prior is updated as data or new evidence is presented
- The posterior distribution expresses the updated belief


::: {.important data-latex=""}
**Recall Bayes theorem**

Let $D = \text{patient has disease}$ and $Y = \text{patient has a positive diagnostic test}$,

$$\begin{aligned}
P(D \mid T) & = \frac{P(T \mid D)P(D)}{P(T)} \\
& = \frac{P(T \mid D)P(D)}{P(T \mid D)P(D) + P(T \mid D^c)P(D^c)}
\end{aligned}$$

 - $P(T\mid D)$ is the **likelihood** of the outcome (positive test) given the unknown parameter (disease state)
 - $P(D)$ is **pre-test probability** (prior probability) of disease
 - $P(D\mid T)$ is the post-test probability of disease which can be obtained by multiplying the **likelihood** and the **pre-test probabiltiy**.
 - Here, to calculate $P(D\mid T)$ we need $P(D)$!
 - A very sensitive test (e.g., P(T\mid D) = 0.99) can still result in a small post-test probability if the prior probability of disease, $P(D)$, is low!

:::

**The Bayesian approach to estimating parameters stems from Bayes' theorem for continuous variables:**

Let $\theta$ be the parameter of interest and $y$ be the observed data,

$$\begin{aligned}
P(\theta \mid y) & = \frac{P(y \mid \theta)P(\theta)}{P(y)} \\
& = \frac{\text{likelihood of data given parameter} \times \text{prior}}{\text{marginal distribution of data free of the parameter}} \\
& \propto \text{likelihood}(y \mid \theta ) \times \text{prior}(\theta)
\end{aligned}$$

- $P(y)$ is called a normalizing factor, it's in place to ensure that $\int P(\theta \mid y) d\theta = 1$, that is the posterior distribution of $\theta$ is a proper probability distribution with area under the density curve equals to 1.

- Its value is not of interest, unless we are comparing between data models.

- The essence of Bayes theorem only concerns the terms involving the parameter, $\theta$, hence $P(\theta \mid y) \propto P(y\mid \theta)P(\theta)$.


::: {.workedexample data-latex=""}
**Estimating a Proportion**

Suppose you have observed 6 patients in a Phase I RCT on a given dose of drug,
- 0 out of 6 patients have had an adverse event
- decision to escalate to a higher dose if it's unlikely that the current dosing results in a true proportion of adverse events above 20% (i.e., given the current data, is there sufficient evidence to infer the true proportion of adverse event is less than 20%, if so we can increase the dose level)

- This is a classic phase I estimate, under frequentist test (Exact Binomial Test) we have

```{r }
binom.test(x=0, n=6, p = 0.2,
           alternative = c("less"),
           conf.level = 0.95)
```

 - The observed proportion $\hat{\theta}=0$ with 95% CI: 0 - 0.39.
 - How much evidence we have that AE rate is < 20%?
 
```{r }
#suppose we observe 0 adverse event out of 14 patients; 
#the test results below suggest we would reject the null hypothesis;
#at 0.05 alpha level and conclude the true AE rate is < 20%;

binom.test(x=0, n=14, p = 0.2,
           alternative = c("less"),
           conf.level = 0.95)
```

**What would a Bayesian do?**

To make probability statements about $\theta$ after observing data $y$, we need a probability distribution for $\theta$ given $y$ (the posterior distribution).

1.First, we need to specify a prior distribution for $\theta$, $P(\theta)$.

- Example 1: We might have no idea about $\theta$ other than that it lies in the interval [0,1] and thus specify a unif(0,1). Let $\theta \sim U(0,1)$, the prior probability distribution (p.d.f) is
\[ P(\theta) = \frac{1}{1-0} = 1.\]

- Example 2: We might have some knowledge about the range of $\theta$, say, we are believe $0.05<\theta<0.5$. We can have
\[ \theta \sim U(0.05, 0.5)\]
\[P(\theta) = \frac{1}{0.5-0.05} = 2.22.\]


2. We assume the $P(y \mid \theta)$ follows a binomial distribution, thus the likelihood of the observed data given $\theta$ is 
\[ P(y = 0 \mid \theta) = {6 \choose 0} \theta^0 (1-\theta)^6 = (1-\theta)^6\]


3. The posterior then becomes (given example prior 1)

$$\begin{align}
P(\theta \mid y = 0) &= \frac{P(y = 0 \mid \theta) \times P(\theta)}{P(y=0)} \\
& = \frac{(1-\theta)^6 \times 1}{P(y=0)} \\
& = \text{Constant} \times (1-\theta)^6 \\
 & \propto (1-\theta)^6 
\end{align}$$

```{r fig.height=3, fig.cap="Approximate posterior distribution obtained using Bayes' rule with UNIF(0,1) prior. In this example, the normalizaing term P(y=0) is not considered."}
d <- tibble(p_grid = seq(from = 0, to = 1, length.out = 101),
         y      = 0, 
         n      = 6) %>% 
    mutate(prior      = dunif(p_grid, 0, 1),
         likelihood = dbinom(y, n, p_grid)) %>% 
    mutate(posterior = likelihood * prior )

d %>% pivot_longer(prior:posterior) %>% 
  # this line allows us to dictate the order in which the panels will appear
    mutate(name = factor(name, levels = c("prior", "likelihood", "posterior"))) %>% 
    ggplot(aes(x = p_grid, y = value, fill = name)) +
    geom_area(show.legend = FALSE) +
    scale_fill_manual(values = c("grey", "red", "blue")) +
    facet_wrap(~ name, scales = "free")+
    theme_bw()

```

```{r echo=F, fig.height=3, fig.cap="Approximate posterior distribution obtained using Bayes' rule with UNIF(0.05,0.5) prior. In this example, the normalizaing term P(y=0) is not considered."}
d <- tibble(p_grid = seq(from = 0, to = 1, length.out = 101),
         y      = 0, 
         n      = 6) %>% 
    mutate(prior      = dunif(p_grid, 0.05, 0.5),
         likelihood = dbinom(y, n, p_grid)) %>% 
    mutate(posterior = likelihood * prior )

d %>% pivot_longer(prior:posterior) %>% 
  # this line allows us to dictate the order in which the panels will appear
    mutate(name = factor(name, levels = c("prior", "likelihood", "posterior"))) %>% 
    ggplot(aes(x = p_grid, y = value, fill = name)) +
    geom_area(show.legend = FALSE) +
    scale_fill_manual(values = c("grey", "red", "blue")) +
    facet_wrap(~ name, scales = "free")+
    theme_bw()

```

**Why is P(y=0) free of $\theta$?**

- The **law of total probability** for discrete parameter values can be used
- Suppose there are two possible values of parameter $\theta$, 0.5 and 0.1. 
- Suppose we know the prior distribution of $\theta$: $P(\theta = 0.5) = 0:8$ and
$P(\theta = 0.1) = 0.2$
- Likelihood values are calculated given a known $\theta$, so then don't include the parameter $\theta$.
- Call these $P_{0.5} = P(Y = 0 \mid \theta = 0.5)$ and $P_{0.1} = P(Y = 0 \mid \theta = 0.1)$, 
- Putting all components together using law of total probability, $P(Y = 0)$ does not involved the unknown $\theta$
\[P(Y = 0) = P(Y = 0 \mid \theta = 0.5)P(\theta = 0.5) + P(Y = 0 \mid \theta = 0.1)P(\theta = 0.1)\]

\[ P(Y = 0) = P_{0.5} \times 0.8 + P_{0.1} \times 0.2 \]


- In case of a continuous parameter value, we can obtain $P(y=0)$ by intergrating over the space of $\theta$ as following
\[P(y=0) = \int_{\theta=0}^{\theta=1} P(y=0 \mid \theta) P(\theta) \ d \theta\]
- Intergrating over $\theta$ is analogous to summing over a set of discrete values of $\theta$.
- After integration over $\theta$, $\theta$ is no longer featured in $P(y=0)$.

:::




### The Beta-binomial model


- With a single sample of $n$ binary outcomes, we have one unknown parameter: $\theta$ and $0 \leq \theta \leq 1$.
- We need a prior distribution for $\theta$ (e.g., proportion, risk, probability of event etc): $P(\theta)$
    - To express indifference between all values of $\theta$, we can use a uniform distribution on $\theta$, as we did in the previous example
    - To express belief (e.g. based on external evidence) that some values of $\theta$ are more likely that others, it is convenient to use a **beta distribution**
    - This has two parameters, often labeled as $\alpha$ (also written as a) and $\beta$ (also written as b), which we can choose to represent the strength of the external evidence
    - If a parameter has a Beta(a,b) distribution, then the prior mean is
    \[\frac{a}{a+b}\]
    - The beta distribution prior for the binomial is useful for illustrating how the Bayesian approach combines prior information and new data
    
    
### R Session information {-}




```{r echo=F}
getS3method("print","sessionInfo")(sessionInfo()[c(1:7)])
```

