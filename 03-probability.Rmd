# Probability, random variables and distributions {#prob}

::: {.chapterintro data-latex=""}

- Review of probability terminologies, probability rules, and Venn Diagrams
- Review conditional probability, independence, and Bayes' theorem. 
- Review on random variables and common probability distributions
Probability Terminology
:::

<script src="hideOutput.js"></script>

```{r, echo = FALSE, message=FALSE, warning=FALSE}
# Every chapter, we will load all the library we will use at the beginning
# of the chapter. 
library(tidyverse)
library(VennDiagram)
library(gganimate)
library(tweenr)
```


</br>


## Probability

**Probability has a central place in Bayesian analysis**

- we put a prior probability distribution on the unknowns (parameters),
- we model the observed data with a probability distribution (likelihood),
- and we combine the two into a posterior probability distribution


::: {.important data-latex=""}
**Probability Terminology**[@evans2004probability]

- **Sample space** This set of all possible
outcomes of an experiment/trial is known as the sample space of the experiment/trial and is denoted by $S$ or $\Omega$.

- **Experiment/Trial**: each occasion we observe a random phenomenon that we know what outcomes can occur, but we do not know which outcome will occur

- **Event**: Any subset of the sample space $S$ is known as an event, denoted by $A$. Note that, $A$ is also a collection of one or more outcomes. 

- **Probability defined on events**: For each event $A$ of the sample space $S$, $P(A)$, the probability of the event $A$, satisfies the following three conditions:
    a. $0 \leq P(A) \leq 1$
    b. $P(S) = 1$ and $P(\emptyset) = 0$; $\emptyset$ denotes the empty set
    c. $P$ is (countably) additive, meaning that if $A_1, A_2, \ldots$ is a finite or countable sequence of disjoint (also known as mutually exclusive events), then
    \[P(A_1 \cup A_2 \cup \ldots ) = P(A_1)+P(A_2)+\ldots \]
    
- **Union:** Denote the event that either $A$ or $B$ occurs as $A\cup B$.

- **Intersection:** Denote the event that **both** $A$ and $B$ occur as $A\cap B$

- **Complement:** Denote the event that $A$ does not occur as $\bar{A}$ or $A^{C}$ or $A^\prime$ (different people use different notations)

- **Disjoint** (or **mutually exclusive**): Two events $A$ and $B$ are said to be **disjoint** if the occurrence of one event precludes the occurrence of the other. *If two events are mutually exclusive, then* $P(A\cup B)=P(A)+P(B)$


:::

::: {.workedexample data-latex=""}

**Sample Space**

1. If the experiment consists of the flipping of a coin, then \[ S = \{H, T\}\]
2. If the experiment consists of rolling a die, then the sample space is \[ S = \{1, 2, 3, 4, 5, 6\}\]
3. If the experiments consists of flipping two coins, then the sample space consists of the following four points:
\[ S = \{(H,H), (H,T), (T,H), (T,T)\}\]

**Event**

1. In Example (1), if E = {H}, then E is the event that a head appears on the flip of the coin. Similarly, if $E = \{T \}$, then $E$ would be the event that a tail appears
2. In Example (2), if $E = \{1\}$, then $E$ is the event that one appears on the roll of the die. If $E = \{2, 4, 6\}$, then $E$ would be the event that an even number appears on the roll.
3. In Example (3), if $E = \{(H, H), (H, T )\}$, then $E$ is the event that a head appears on the first coin.


**Probability of an event**. Let $R$ be the sum of two standard dice. Suppose we are interested in $P(R \le 4)$. Notice that the pair of dice can fall 36 different ways (6 ways for the first die and six for the second results in 6x6 possible outcomes, and each way has equal probability $1/36$. 
$$\begin{aligned} P(R \le 4 )	
  &=	P(R=2)+P(R=3)+P(R=4) \\
	&=	P(\left\{ 1,1\right\} )+P(\left\{ 1,2\right\} \mathrm{\textrm{ or }}\left\{ 2,1\right\} )+P(\{1,3\}\textrm{ or }\{2,2\}\textrm{ or }\{3,1\}) \\
	&=	\frac{1}{36}+\frac{2}{36}+\frac{3}{36} \\
	&=	\frac{6}{36} \\
	&=	\frac{1}{6} \end{aligned}$$
	
:::

### Venn Diagrams

**Venn diagrams provide a very useful graphical method for depicting the sample space S and subsets of it.** 

Figure taken from [@cardinal2019sets].

```{r echo=FALSE, fig.cap="Venn Diagram for two events", fig.align='center', out.width = '75%'}
knitr::include_graphics("images/venn.jpg")
```


::: {.guidedexercise data-latex=""}

1. **Venn Diagram for two disjoint events** How would you draw this venn diagram?

2. **Label sub-regions in a Venn Diagram for three events** Using set theory, how would you write out areas *a*, *d*, and *f* ?
:::

<div class="fold s">
```{r, echo=T, fig.height=4, fig.width=4, fig.align = 'center', eval=TRUE}
A <- c("a","b", "d","e")
B <- c("b","c", "d","f")
C <- c("d","e","f","g")
x <- list(A=A , B=B , C=C)
v0 <- venn.diagram( x, filename=NULL, fill = c("red", "blue", "green"),
                    alpha = 0.50, col = "transparent")

overlaps <- calculate.overlap(x)
# extract indexes of overlaps from list names
indx <- as.numeric(substr(names(overlaps),2,2))
# labels start at position 7 in the list for Venn's with 3 circles
for (i in 1:length(overlaps)){
  v0[[6 + indx[i] ]]$label <- paste(overlaps[[i]], collapse = "\n") 
}

grid.draw(v0)
```
<div>

### Probability Rules


**General Addition Rule:** $P(A\cup B)=P(A)+P(B)-P(A\cap B)$

The reason behind this fact is that if there is if $A$ and $B$ are not disjoint, then some area is added twice when we calculate $P\left(A\right)+P\left(B\right)$. To account for this, we simply subtract off the area that was double counted.


**Complement Rule:** $P(A)+P(A^c)=1$

This rule follows from the partitioning of the set of all events ($S$) into two disjoint sets, $A$ and $A^c$.  We learned above that $A \cup A^c = S$ and that $P(S) = 1$.  Combining those statements, we obtain the complement rule.


**Completeness Rule:** $P(A)=P(A\cap B)+P(A\cap B^c)$

This identity is just breaking the event $A$ into two disjoint pieces.

**Law of total probability (unconditioned version):**  Let $A_1, A_2, \ldots$ be events that form a partition of the sample space $S$, Let $B$ be any event. Then,
\[P(B) = P(A_1 \cap B) + P(A_2 \cap B) + \ldots. \]

This law is key in deriving the marginal event probability in Bayes' rule. Recall the HIV example from session 1, we have $P(T^+) = P(T^+ \cap D^+)+P(T^+ \cap D^-)$.

```{r echo=FALSE, fig.cap="Demonstrate Law of total probabiliy using Venn Diagram", fig.align='center', out.width = '60%'}
knitr::include_graphics("images/law_tot_prob.png")
```


**Conditional Probability**: In general we define conditional probability (assuming $P(B) \ne 0$) as 
$$P(A|B)=\frac{P(A\cap B)}{P(B)}$$
which can also be rearranged to show 
$$\begin{aligned}
P(A\cap B)	&=	P(A\,|\,B)\,P(B) \\
	          &=	P(B\,|\,A)\,P(A)
\end{aligned}$$
 Because the order doesn't matter and $P\left(A\cap B\right)=P\left(B\cap A\right)$.
 

**Independent:** Two events $A$ and $B$ are said to be **independent** if $P(A\cap B)=P(A)P(B)$.
 
What independence is saying that knowing the outcome of event $A$ doesn't give you any information about the outcome of event $B$. *If $A$ and $B$ are independent events, then $P(A|B)	=	P(A)$ and $P(B|A) = P(B)$.*

> In simple random sampling, we assume that any two samples are independent. In cluster sampling, we assume that samples within a cluster are not independent, but clusters are independent of each other.

**Bayes' Rule:** This arises naturally from the rule on conditional probability. Since the order does not matter in $A \cap B$, we can rewrite the equation: 

$$\begin{aligned} 
P(A \cap B) &=	P(B \cap A) \\
P(A\mid B)P(B) &=	P(B\mid A)P(A) \\
P(A\mid B) &=	\frac{P(B\mid A)P(A) }{P(B)}\\
&=	\frac{P(B\mid A)P(A) }{P(B\mid A)P(A) + P(B\mid A^C)P(A^C)}
\end{aligned}$$ 


 
### How to define and assign probabilities in general?

1. **Frequency-type (Empirical Probability)**: based on the idea of frequency or long-run frequency of an event.
    - Observe sequence of coin tosses (trials)  and the count number of times of event $A=\{H\}$. The relative frequency of $A$ is given as,
\[ P(A) = \frac{\text{Number of times A occurs}}{\text{Total number of trials}}. \]
    - Trials are independent. Relative frequency is unpredictable in short-term, but in long-run it's predicable. Let $n$ be the total number of trials and $m$ be number of heads, then we have \[ P(A) = \lim_{n \rightarrow \infty} \frac{m}{n}.\]
    - **Theoretical Probability**: Sometimes $P(A)$ can be deduced from mathematical model using uniform probability property on finite spaces.
        - If the sample space S is finite, then one possible probability measure on $S$ is the uniform probability measure, which assigns probability $1/|S|$, **equal probability to each outcome**. Here |S| is the number of elements in the sample space $S$. By additivity, it then follows that for any event $A$ we have,
   
    $$ P(A) = \frac{|A|}{|S|} = \frac{\text{Number of outcomes in S that satisfy A}}{\text{Total number of outcomes in S} } $$
    - **Long-term frequency** It is natural to think of large sequences of similar events defining frequency-type probability if we allow the number of trials to be indefinitely large. 


```{r echo=FALSE, fig.cap="Demonstrate law of large number with the coin tossing example", fig.height=4, fig.width=4, fig.align = 'center', eval=TRUE}
n = 1000
pHeads =0.5
set.seed(123)
flipSequence = sample( x=c(0,1),prob = c(1-pHeads,pHeads),size=n,replace=T)
 
r = cumsum(flipSequence)
n= 1:n
runProp = r/n
 
flip_data <- data.frame(run=1:1000,prop=runProp)

ggplot(flip_data,aes(x=run,y=prop,frame=run)) + 
  geom_path()+
  xlim(1,1000)+ylim(0.0,1.0)+
  geom_hline(yintercept = 0.5)+
  ylab("Proportion of Heads")+
  xlab("Number of Flips")+
  theme_bw()
```

2. **Belief-type (Subjective Probability)**: based on the idea of degree of belief (weight of evidence) about an event where the scale is anchored at certain (=1) and impossible (=0).

::: {.workedexample data-latex=""}
**Subjective Probability**
Consider these events

1. We will have more than 100cm of snow this winter.
2. An asteroid is responsible for the extinction of the dinosaurs
3. Mammography reduces the rate of death from breast cancer in women over 50 by more than 10%
4. The 70 year old man in your office, just diagnosed with lung cancer, will live at least 5 years

Can you think of them in terms of long-run frequencies?

- We cannot always think in terms of long-run frequencies
- Also, we may not care about long-run frequencies
- How are they relevant to this patient, this hypothesis?
- Even where long-run frequency could apply, often there is no such data available
- However, opinions are formed and beliefs exist

:::

3. **Which probability to use?**

- For inference, the Bayesian approach relies mainly on the belief interpretation of probability.

- The laws of probability can be derived from some basic axioms that do not rely on long-run frequencies.


::: {.workedexample data-latex=""}
**The Monty Hall Problem Revisit**

> Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, "Do you want to pick door No. 2?" Is it to your advantage to switch your choice?

<img src="images/Monty_open_door.png" class="center">

Before any door is picked, we assume all three doors are equal likely to have a car behind it (prior probability of 1/3). Without loss of generality, let assume we pick door 1 and Monty opens door 2 (goat behind).

- $P( \text{door 1 has car}) = P( \text{door 2 has car}) = P( \text{door 3 has car)} = \frac{1}{3}$

- $P( \text{Monty opens door 2} \mid \text{door 1 has car} ) = \frac{1}{2}$ 

- $P( \text{Monty opens door 2} \mid \text{door 2 has car} ) = 0$ 

- $P( \text{Monty opens door 2} \mid \text{door 3 has car} ) = 1$ 

We want to know these two probabilities: $P( \text{door 1 has car} \mid \text{Monty opens door 2} )$ and $P( \text{door 3 has car} \mid \text{Monty opens door 2} )$. Simplify notations, we have 

$$\begin{aligned} 
& P( D_1 = \text{car} \mid \text{open } D_2 ) \\
& = \frac{P( \text{open } D_2 \mid D_1 = \text{car} )P(D_1 = \text{car})}{P(\text{open } D_2)} \\
& = \frac{P( \text{open } D_2 \mid D_1 = \text{car} )P(D_1 = \text{car})}{P( \text{open } D_2 \mid D_1 = \text{car}  )P(D_1 = \text{car}) + P(\text{open } D_2 \mid D_3 = \text{car})P(D_3 = \text{car})} \\
& = \frac{\frac{1}{2} \times \frac{1}{3}}{\frac{1}{2} \times \frac{1}{3} + 1 \times \frac{1}{3}} = \frac{1}{3}
\end{aligned}$$ 

$$\begin{aligned} 
& P( D_3 = \text{car} \mid \text{open } D_2 ) \\
& = \frac{P( \text{open } D_2 \mid D_3 = \text{car} )P(D_3 = \text{car})}{P(\text{open } D_2)} \\
& = \frac{P( \text{open } D_2 \mid D_3 = \text{car} )P(D_3 = \text{car})}{P( \text{open } D_2 \mid D_3 = \text{car}  )P(D_3 = \text{car}) + P(\text{open } D_2 \mid D_1 = \text{car})P(D_1 = \text{car})} \\
& = \frac{ 1 \times \frac{1}{3}}{ 1 \times \frac{1}{3} + \frac{1}{2} \times \frac{1}{3}} = \frac{2}{3}
\end{aligned}$$ 

Given we pick door 1 and Monty opens door 2 (goat behind), the probability of the car behind door 1 is 1/3 while the probability of the car behind door 3 is 2/3. Therefore, we should switch to door 3 for a better chance of winning.
:::


::: {.guidedexercise data-latex=""}

**The ELISA test Problem Revisit** 
The ELISA test for HIV was widely used in the mid-1990s for screening blood donations. As with most medical diagnostic tests, the ELISA test is not perfect.

- If a person actually carries the HIV virus, experts estimate that this test gives a positive result 97.7% of the time.  (This number is called the *sensitivity* of the test.)
- If a person does not carry the HIV virus, ELISA gives a negative (correct) result 92.6% of the time (the *specificity* of the test).
- Assume the estimated HIV prevalence at the time was 0.5% of (the *prior base rate*). 

Suppose a randomly selected individual **tested positive**, please calculate $P(D^+ \mid T^+)$ and $P(D^- \mid T^-)$. 
:::


## Random Variables

The different types of probability distributions (and therefore your analysis 
method) can be divided into two general classes:

1. Continuous Random Variables - the variable takes on numerical values and could, 
in principle, take any of an uncountable number of values.

2.Discrete Random Variables - the variable takes on one of small set of values (or only a countable number of outcomes).



## R Quick Reference

We give a brief summary of the distributions used most in this course and the 
abbreviations used in R.

+--------------+-------------+---------------+------------------------------------+
| Distribution |  Stem       | Parameters    | Parameter Interpretation           |
+==============+=============+===============+====================================+
| Binomial     | `binom`     | `size`        | Number of Trials,                  |
|              |             | `prob`        | Probability of Success (per Trial) |
+--------------+-------------+---------------+------------------------------------+
| Exponential  | `exp`       | `rate`        | Mean of the distribution           |
+--------------+-------------+---------------+------------------------------------+
| Normal       | `norm`      | `mean=0`      | Center of the distribution,        |
|              |             | `sd=1`        | Standard deviation                 |
+--------------+-------------+---------------+------------------------------------+
| Uniform      | `unif`      | `min=0`       | Minimum and                        |
|              |             | `max=1`       | Maximum of the distribution        | 
+--------------+-------------+---------------+------------------------------------+


All the probability distributions available in R are accessed in exactly the 
same way, using a d-function, p-function, q-function, and r-function. 

+--------------------+-----------------------------------------------------------+
| Function           |    Result                                                 |
+====================+===========================================================+
| `d`-function(x)    | The height of the probability distribution/density at $x$ |
+--------------------+-----------------------------------------------------------+
| `p`-function(x)    |  $P\left(X\le x\right)$                                   |
+--------------------+-----------------------------------------------------------+
| `q`-function(q)    |  $x$ such that $P\left(X\le x\right) = q$                 |
+--------------------+-----------------------------------------------------------+
| `r`-function(n)    | $n$ random observations from the distribution             |
+--------------------+-----------------------------------------------------------+

The `mosaic` package has versions of the p and q -functions that also print a out 
nice picture of the probabilities that you ask for. These functions are named by 
just adding an 'x' at the beginning of the function. For example `mosaic::xpnorm(-1)`.


